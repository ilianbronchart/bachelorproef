{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CHECKPOINTS_PATH\"] = \"../checkpoints\"\n",
    "os.environ[\"TRACKING_RESULTS_PATH\"] = \"data/processed_tracking_results\"\n",
    "\n",
    "import itertools\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from src.api.models.pydantic import SimRoomClassDTO\n",
    "from src.config import UNKNOWN_CLASS_ID\n",
    "from ultralytics import YOLO\n",
    "\n",
    "from experiment.experiment_utils import (\n",
    "    calculate_metrics,\n",
    "    create_confusion_matrix,\n",
    "    evaluate_predictions,\n",
    "    render_confusion_matrix,\n",
    "    update_confusion_matrix,\n",
    ")\n",
    "from experiment.settings import (\n",
    "    CLASS_ID_TO_NAME,\n",
    "    FINAL_PREDICTIONS_PATH,\n",
    "    FULLY_LABELED_RECORDINGS,\n",
    "    GAZE_SEGMENTATION_RESULTS_PATH,\n",
    "    LABELING_REC_DIFF_BACKGROUND_ID,\n",
    "    LABELING_REC_SAME_BACKGROUND_ID,\n",
    "    OBJECT_DATASETS_PATH,\n",
    "    TRAINING_DATASETS_PATH,\n",
    "    YOLO_MODELS_PATH,\n",
    "    SIMROOM_ID,\n",
    "    RECORDINGS_PATH,\n",
    "    RECORDING_FRAMES_PATH,\n",
    "    MODELS_PATH\n",
    ")\n",
    "from src.api.db import Session, engine\n",
    "from src.api.repositories import simrooms_repo\n",
    "from src.api.services import simrooms_service\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tempfile\n",
    "from src.utils import extract_frames_to_dir\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ef725",
   "metadata": {},
   "source": [
    "# Global Config Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c609dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_CROP_SIZE = 640\n",
    "IMG_CROP_SIZE_HALF = IMG_CROP_SIZE // 2\n",
    "OBJECT_DETECTION_DATASETS_PATH = TRAINING_DATASETS_PATH / \"object_detection\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2837b",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b374a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracking_results_per_class(session: Session, labeling_recording_id: str):\n",
    "    calibration_id = simrooms_repo.get_calibration_recording(\n",
    "        session, simroom_id=SIMROOM_ID, recording_id=labeling_recording_id\n",
    "    ).id\n",
    "    tracked_classes = simrooms_repo.get_tracked_classes(session, calibration_id)\n",
    "\n",
    "    if len(tracked_classes) != 15:\n",
    "        raise ValueError(f\"Expected 15 tracked classes but got {len(tracked_classes)}\")\n",
    "\n",
    "    tracking_results_per_class = {\n",
    "        tracked_class.id: simrooms_repo.get_tracking_result_paths(\n",
    "            session, calibration_id, tracked_class.id\n",
    "        )\n",
    "        for tracked_class in tracked_classes\n",
    "    }\n",
    "\n",
    "    return tracking_results_per_class, tracked_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2576c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_per_class_metadata(\n",
    "    session: Session,\n",
    "    labeling_recording_id: str,\n",
    "):\n",
    "    tracking_results_per_class, tracked_classes = get_tracking_results_per_class(\n",
    "        session, labeling_recording_id\n",
    "    )\n",
    "    per_class_metadata = {}\n",
    "\n",
    "    for tracked_class in tracked_classes:\n",
    "        class_id = tracked_class.id\n",
    "        tracking_results = tracking_results_per_class[class_id]\n",
    "\n",
    "        frame_indexes = []\n",
    "        laplacian_variances = []\n",
    "        mask_areas = []\n",
    "        bboxes = []\n",
    "        for tracking_result in tracking_results:\n",
    "            file = np.load(tracking_result)\n",
    "            if int(tracking_result.stem) != int(file[\"frame_idx\"]):\n",
    "                raise ValueError(\n",
    "                    f\"Frame index mismatch: {tracking_result.stem} != {file['frame_idx']}\"\n",
    "                )\n",
    "\n",
    "            frame_indexes.append(int(tracking_result.stem))\n",
    "            laplacian_variances.append(file[\"laplacian_variance\"])\n",
    "            mask_areas.append(np.sum(file[\"mask\"]))\n",
    "            bboxes.append(file[\"box\"])\n",
    "\n",
    "        per_class_metadata[class_id] = {\n",
    "            \"class_name\": tracked_class.class_name,\n",
    "            \"color\": tracked_class.color,\n",
    "            \"frame_indexes\": frame_indexes,\n",
    "            \"laplacian_variances\": laplacian_variances,\n",
    "            \"mask_areas\": mask_areas,\n",
    "            \"bboxes\": bboxes,\n",
    "        }\n",
    "\n",
    "    return per_class_metadata\n",
    "\n",
    "\n",
    "def select_samples_per_class(per_class_metadata, num_samples_per_class):\n",
    "    selected_samples_per_class = {}\n",
    "\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        if len(metadata[\"frame_indexes\"]) < num_samples_per_class:\n",
    "            print(\n",
    "                f\"Class {CLASS_ID_TO_NAME[class_id]} has only {len(metadata['frame_indexes'])} samples, ignoring it\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        laplacian_variances = metadata[\"laplacian_variances\"]\n",
    "        mask_areas = metadata[\"mask_areas\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        # zip all metadata together\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        # choose num_samples random samples\n",
    "        selected_samples = np.random.choice(\n",
    "            len(sample_list), num_samples_per_class, replace=False\n",
    "        )\n",
    "\n",
    "        selected_samples = [sample_list[i] for i in selected_samples]\n",
    "\n",
    "        # unzip the selected samples\n",
    "        selected_frame_indexes, selected_bboxes = zip(*selected_samples)\n",
    "        selected_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": selected_frame_indexes,\n",
    "            \"bboxes\": selected_bboxes,\n",
    "        }\n",
    "\n",
    "    return selected_samples_per_class\n",
    "\n",
    "\n",
    "def get_samples_per_frame(per_class_metadata):\n",
    "    samples_per_frame = {}\n",
    "\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        # zip all metadata together\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        for frame_index, bbox in sample_list:\n",
    "            if frame_index not in samples_per_frame:\n",
    "                samples_per_frame[frame_index] = []\n",
    "            samples_per_frame[frame_index].append((class_id, bbox))\n",
    "\n",
    "    return samples_per_frame\n",
    "\n",
    "\n",
    "def get_train_val_split(\n",
    "    selected_samples_per_class,\n",
    "    train_ratio=0.8,\n",
    "):\n",
    "    train_samples_per_class = {}\n",
    "    val_samples_per_class = {}\n",
    "\n",
    "    for class_id, metadata in selected_samples_per_class.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        # zip all metadata together\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        # shuffle the samples\n",
    "        np.random.shuffle(sample_list)\n",
    "\n",
    "        # split the samples into train and val\n",
    "        split_index = int(len(sample_list) * train_ratio)\n",
    "        train_samples = sample_list[:split_index]\n",
    "        val_samples = sample_list[split_index:]\n",
    "\n",
    "        # unzip the selected samples\n",
    "        train_frame_indexes, train_bboxes = zip(*train_samples)\n",
    "        val_frame_indexes, val_bboxes = zip(*val_samples)\n",
    "\n",
    "        train_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": train_frame_indexes,\n",
    "            \"bboxes\": train_bboxes,\n",
    "        }\n",
    "        val_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": val_frame_indexes,\n",
    "            \"bboxes\": val_bboxes,\n",
    "        }\n",
    "\n",
    "    return train_samples_per_class, val_samples_per_class\n",
    "\n",
    "\n",
    "def plot_per_class_laplacian_variance(per_class_metadata):\n",
    "    # Plot boxplots of laplacian variances per class in a single graph (one boxplot per class)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        laplacian_variances = metadata[\"laplacian_variances\"]\n",
    "        ax.boxplot(\n",
    "            laplacian_variances,\n",
    "            positions=[class_id],\n",
    "            widths=0.5,\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor=metadata[\"color\"]),\n",
    "        )\n",
    "    ax.set_xticks(list(per_class_metadata.keys()))\n",
    "    ax.set_xticklabels([\n",
    "        metadata[\"class_name\"] for metadata in per_class_metadata.values()\n",
    "    ])\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Laplacian Variance\")\n",
    "    ax.set_title(\"Laplacian Variance per Class\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot histograms of laplacian variances per class in a single figure (one histogram per row)\n",
    "    fig, axs = plt.subplots(\n",
    "        len(per_class_metadata), 1, figsize=(12, 6 * len(per_class_metadata))\n",
    "    )\n",
    "    for i, (class_id, metadata) in enumerate(per_class_metadata.items()):\n",
    "        laplacian_variances = metadata[\"laplacian_variances\"]\n",
    "        axs[i].hist(\n",
    "            laplacian_variances,\n",
    "            bins=50,\n",
    "            color=metadata[\"color\"],\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        axs[i].set_title(f\"Laplacian Variance Histogram - {metadata['class_name']}\")\n",
    "        axs[i].set_xlabel(\"Laplacian Variance\")\n",
    "        axs[i].set_ylabel(\"Frequency\")\n",
    "        axs[i].grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe64285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, tmp_frames_dir = simrooms_service.extract_tmp_frames(\n",
    "    LABELING_REC_SAME_BACKGROUND_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52a841f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(engine) as session:\n",
    "    per_class_metadata = get_per_class_metadata(session, LABELING_REC_SAME_BACKGROUND_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1cc02e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_per_class_laplacian_variance(per_class_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d13cb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(\n",
    "    image_np, bboxes, labels, class_name_map=None, color=(0, 255, 0), thickness=2\n",
    "):\n",
    "    img_res = image_np.copy()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "\n",
    "    if not isinstance(bboxes, (list, np.ndarray)):\n",
    "        print(f\"Warning: bboxes is not a list or ndarray: {type(bboxes)}\")\n",
    "        return img_res\n",
    "    if not isinstance(labels, (list, np.ndarray)):\n",
    "        print(f\"Warning: labels is not a list or ndarray: {type(labels)}\")\n",
    "        # Attempt to proceed if labels seem usable, otherwise return\n",
    "        if len(bboxes) != len(labels):\n",
    "            print(\"Warning: bbox and label length mismatch, cannot draw labels.\")\n",
    "            labels = [\"?\" for _ in bboxes]  # Placeholder\n",
    "        elif not all(isinstance(l, (str, int, float)) for l in labels):\n",
    "            print(\"Warning: labels contain non-primitive types, cannot draw reliably.\")\n",
    "            labels = [\"?\" for _ in bboxes]\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        # Assuming bbox format allows direct conversion to int x_min, y_min, x_max, y_max\n",
    "        # This might need adjustment based on the ACTUAL format in your bboxes list\n",
    "        # Example for pascal_voc or albumentations (after denormalizing)\n",
    "        try:\n",
    "            # Check if bbox has at least 4 elements\n",
    "            if len(bbox) < 4:\n",
    "                print(f\"Warning: Skipping invalid bbox (fewer than 4 coords): {bbox}\")\n",
    "                continue\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox[:4])\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Warning: Could not convert bbox coords to int: {bbox}, Error: {e}\")\n",
    "            continue  # Skip this bbox\n",
    "\n",
    "        cv2.rectangle(img_res, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "\n",
    "        label_name = (\n",
    "            str(label)\n",
    "            if class_name_map is None\n",
    "            else class_name_map.get(label, str(label))\n",
    "        )\n",
    "        # Simple text placement above the box\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            label_name, font, font_scale, font_thickness\n",
    "        )\n",
    "        text_y = (\n",
    "            y_min - baseline if y_min - baseline > text_height else y_min + text_height\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img_res, label_name, (x_min, text_y), font, font_scale, color, font_thickness\n",
    "        )\n",
    "\n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "90b6ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_per_class_to_samples_per_frame(samples_per_class):\n",
    "    samples_per_frame = {}\n",
    "    for class_id, metadata in samples_per_class.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        for frame_index, bbox in sample_list:\n",
    "            if frame_index not in samples_per_frame:\n",
    "                samples_per_frame[frame_index] = []\n",
    "            samples_per_frame[frame_index].append((class_id, bbox))\n",
    "\n",
    "    return samples_per_frame\n",
    "\n",
    "\n",
    "def create_data_files(\n",
    "    labels_path: Path,\n",
    "    images_path: Path,\n",
    "    class_label_to_model_id,\n",
    "    sample_idx,\n",
    "    image,\n",
    "    bboxes,\n",
    "    class_labels,\n",
    "):\n",
    "    padded_sample_idx = str(sample_idx).zfill(10)\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    # Save image\n",
    "    image_path = images_path / f\"{padded_sample_idx}.jpg\"\n",
    "    cv2.imwrite(image_path, image)\n",
    "\n",
    "    # Save labels\n",
    "    labels_path = labels_path / f\"{padded_sample_idx}.txt\"\n",
    "\n",
    "    # transform bboxes to YOLO format\n",
    "    file_rows = []\n",
    "    for bbox, class_label in zip(bboxes, class_labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "\n",
    "        # get xywh format\n",
    "        x_center = (x1 + x2) / 2\n",
    "        y_center = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        # normalize the values\n",
    "        x_center /= image_width\n",
    "        y_center /= image_height\n",
    "        width /= image_width\n",
    "        height /= image_height\n",
    "\n",
    "        model_id = class_label_to_model_id[class_label]\n",
    "        file_rows.append(f\"{model_id} {x_center} {y_center} {width} {height}\")\n",
    "\n",
    "    with open(labels_path, \"w\") as f:\n",
    "        for row in file_rows:\n",
    "            f.write(row + \"\\n\")\n",
    "\n",
    "\n",
    "def create_train_or_val_dataset(\n",
    "    per_class_metadata,\n",
    "    class_label_to_model_id,\n",
    "    train_samples_per_class,\n",
    "    samples_per_frame,\n",
    "    frames,\n",
    "    images_path,\n",
    "    labels_path,\n",
    "    is_validation=False,\n",
    "):\n",
    "    selected_samples_per_frame = samples_per_class_to_samples_per_frame(\n",
    "        train_samples_per_class\n",
    "    )\n",
    "\n",
    "    current_sample_idx = 0\n",
    "    for frame_idx, frame in enumerate(tqdm(frames)):\n",
    "        # check if the frame has any samples\n",
    "        if selected_samples_per_frame.get(frame_idx) is None:\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(str(frame))\n",
    "\n",
    "        # gather boxes and labels for the current frame\n",
    "        class_ids, bboxes = zip(*samples_per_frame[frame_idx])\n",
    "        bboxes = np.array(bboxes)\n",
    "        class_labels = [\n",
    "            per_class_metadata[class_id][\"class_name\"] for class_id in class_ids\n",
    "        ]\n",
    "\n",
    "        # for all selected samples in this frame, create crops\n",
    "        for class_id, box in selected_samples_per_frame[frame_idx]:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "            # create a crop around the center of the box\n",
    "            x_min = max(0, cx - IMG_CROP_SIZE_HALF)\n",
    "            y_min = max(0, cy - IMG_CROP_SIZE_HALF)\n",
    "            x_max = min(image.shape[1], cx + IMG_CROP_SIZE_HALF)\n",
    "            y_max = min(image.shape[0], cy + IMG_CROP_SIZE_HALF)\n",
    "\n",
    "            if not is_validation:\n",
    "                transform_steps = [\n",
    "                    A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max),\n",
    "                    A.PadIfNeeded(min_height=IMG_CROP_SIZE, min_width=IMG_CROP_SIZE),\n",
    "                    # A.HorizontalFlip(p=0.5),\n",
    "                    # A.RandomBrightnessContrast(p=0.2)\n",
    "                ]\n",
    "            else:\n",
    "                transform_steps = [\n",
    "                    A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max),\n",
    "                    A.PadIfNeeded(min_height=IMG_CROP_SIZE, min_width=IMG_CROP_SIZE),\n",
    "                ]\n",
    "\n",
    "            transform = A.Compose(\n",
    "                transform_steps,\n",
    "                bbox_params=A.BboxParams(\n",
    "                    format=\"pascal_voc\", label_fields=[\"class_labels\"], min_visibility=0.7\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Augment the image and boxes\n",
    "            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            transformed_image = augmented[\"image\"]\n",
    "            transformed_bboxes = augmented[\"bboxes\"]\n",
    "            transformed_class_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            # Save the transformed image and labels\n",
    "            create_data_files(\n",
    "                labels_path,\n",
    "                images_path,\n",
    "                class_label_to_model_id,\n",
    "                current_sample_idx,\n",
    "                transformed_image,\n",
    "                transformed_bboxes,\n",
    "                transformed_class_labels,\n",
    "            )\n",
    "\n",
    "            current_sample_idx += 1\n",
    "\n",
    "\n",
    "def create_metadata_yaml(\n",
    "    dataset_path: Path,\n",
    "    per_class_metadata: dict,\n",
    "):\n",
    "    abs_dataset_path = dataset_path.resolve()\n",
    "\n",
    "    metadata_yaml = f\"\"\"\n",
    "path: {abs_dataset_path}\n",
    "train: images/train\n",
    "val: images/val\n",
    "names:\n",
    "\"\"\"\n",
    "\n",
    "    class_label_to_model_id = {}\n",
    "    for i, metadata in enumerate(per_class_metadata.values()):\n",
    "        metadata_yaml += f\"  {i}: {metadata['class_name']}\\n\"\n",
    "        class_label_to_model_id[metadata[\"class_name\"]] = i\n",
    "\n",
    "    metadata_yaml_path = dataset_path / \"data.yaml\"\n",
    "    with open(metadata_yaml_path, \"w\") as f:\n",
    "        f.write(metadata_yaml)\n",
    "\n",
    "    return class_label_to_model_id\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    per_class_metadata: dict,\n",
    "    frames: list[Path],\n",
    "    datasets_path: Path,\n",
    "    num_samples_per_class: int,\n",
    "):\n",
    "    print(f\"Creating dataset with {num_samples_per_class} samples per class\")\n",
    "\n",
    "    dataset_path = datasets_path / f\"{num_samples_per_class}_samples\"\n",
    "    train_images_path = dataset_path / \"images/train\"\n",
    "    train_labels_path = dataset_path / \"labels/train\"\n",
    "    val_images_path = dataset_path / \"images/val\"\n",
    "    val_labels_path = dataset_path / \"labels/val\"\n",
    "    data_yml_path = dataset_path / \"data.yaml\"\n",
    "\n",
    "    train_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    train_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "    val_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    val_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Gather necessary metadata\n",
    "    selected_samples_per_class = select_samples_per_class(\n",
    "        per_class_metadata, num_samples_per_class\n",
    "    )\n",
    "    samples_per_frame = get_samples_per_frame(per_class_metadata)\n",
    "    train_samples_per_class, val_samples_per_class = get_train_val_split(\n",
    "        selected_samples_per_class, train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    # Create the dataset\n",
    "    print(f\"Creating training dataset\")\n",
    "    class_label_to_model_id = create_metadata_yaml(dataset_path, per_class_metadata)\n",
    "    create_train_or_val_dataset(\n",
    "        per_class_metadata,\n",
    "        class_label_to_model_id,\n",
    "        train_samples_per_class,\n",
    "        samples_per_frame,\n",
    "        frames,\n",
    "        train_images_path,\n",
    "        train_labels_path,\n",
    "    )\n",
    "    print(f\"Creating validation dataset\")\n",
    "    create_train_or_val_dataset(\n",
    "        per_class_metadata,\n",
    "        class_label_to_model_id,\n",
    "        val_samples_per_class,\n",
    "        samples_per_frame,\n",
    "        frames,\n",
    "        val_images_path,\n",
    "        val_labels_path,\n",
    "        is_validation=True,\n",
    "    )\n",
    "\n",
    "    return dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2000 samples per class\n",
      "Class infuus has only 786 samples, ignoring it\n",
      "Class ampulevloeistof has only 648 samples, ignoring it\n",
      "Class ampulepoeder has only 1116 samples, ignoring it\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14121/14121 [01:05<00:00, 217.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 14121/14121 [00:18<00:00, 745.95it/s] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PosixPath('data/training_datasets/object_detection/2000_samples')"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if OBJECT_DETECTION_DATASETS_PATH.exists():\n",
    "    shutil.rmtree(OBJECT_DETECTION_DATASETS_PATH)\n",
    "OBJECT_DETECTION_DATASETS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dataset_path = create_dataset(\n",
    "    per_class_metadata=per_class_metadata,\n",
    "    frames=frames,\n",
    "    datasets_path=OBJECT_DETECTION_DATASETS_PATH,\n",
    "    num_samples_per_class=2000,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a8b75",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aced1ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0d5a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.139 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.67 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=data/training_datasets/object_detection/2000_samples/data.yaml, epochs=20, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train11, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/zilian/projects/bachelorproef/runs/detect/train11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1747571238.291967    4858 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1747571238.308900    4858 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=15\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    433597  ultralytics.nn.modules.head.Detect           [15, [64, 128, 256]]          \n",
      "YOLO11n summary: 319 layers, 2,592,765 parameters, 2,592,749 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /home/zilian/projects/bachelorproef/runs/detect/train11', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/zilian/projects/bachelorproef/experiment/data/training_datasets/object_detection/2000_samples/labels/train.cache... 19200 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 19200/19200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/zilian/projects/bachelorproef/experiment/data/training_datasets/object_detection/2000_samples/labels/val.cache... 4800 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4800/4800 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/zilian/projects/bachelorproef/runs/detect/train11/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000526, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added âœ…\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/zilian/projects/bachelorproef/runs/detect/train11\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20       4.7G     0.8714      1.974     0.9322        136        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:18<00:00,  7.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:17<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.869      0.767      0.773      0.622\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      4.82G     0.7078     0.8363     0.8842        163        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:14<00:00,  8.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:17<00:00,  4.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.825      0.802      0.834      0.691\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      4.65G     0.6736     0.6633     0.8773        129        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:09<00:00,  8.66it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:14<00:00,  5.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.806      0.839      0.841      0.701\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      4.58G     0.6569     0.5859     0.8752        164        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:08<00:00,  8.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:13<00:00,  5.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.816      0.839      0.861      0.729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      4.59G     0.6285     0.5314     0.8687        119        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:22<00:00,  7.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:15<00:00,  4.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.796      0.861      0.868      0.735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      4.61G     0.6056     0.4947     0.8638        204        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:17<00:00,  7.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:16<00:00,  4.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.815      0.853      0.853      0.736\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      4.64G     0.5899     0.4736     0.8613        188        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:07<00:00,  8.85it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:13<00:00,  5.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.849      0.882      0.882      0.756\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      4.59G     0.5748     0.4554     0.8571        181        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:08<00:00,  8.73it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:15<00:00,  4.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.837      0.857      0.884      0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      4.64G     0.5606     0.4408     0.8538        140        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:09<00:00,  8.64it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:13<00:00,  5.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.862      0.895      0.903      0.779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      4.66G     0.5505     0.4253     0.8505        145        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:09<00:00,  8.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:12<00:00,  6.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.877      0.856      0.898      0.784\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      4.61G     0.5103     0.3783     0.8234        107        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:06<00:00,  8.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:19<00:00,  3.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.856      0.887      0.907      0.793\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      4.61G     0.4964     0.3644     0.8197        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:04<00:00,  9.29it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:13<00:00,  5.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.896      0.871      0.919      0.803\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      4.61G     0.4863     0.3512     0.8177         98        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:07<00:00,  8.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:13<00:00,  5.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.879      0.901      0.918      0.809\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      4.61G     0.4774     0.3398     0.8157        104        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:04<00:00,  9.35it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:16<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.894      0.913      0.924      0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      4.61G     0.4701     0.3303     0.8137         97        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:05<00:00,  9.20it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:14<00:00,  5.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.874      0.923      0.937      0.823\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      4.61G     0.4585      0.321     0.8103        113        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:07<00:00,  8.91it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:17<00:00,  4.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.893       0.92      0.939       0.83\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      4.61G     0.4507     0.3127      0.809        109        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:04<00:00,  9.30it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:16<00:00,  4.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.905      0.921      0.947      0.833\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      4.61G     0.4425     0.3044      0.808        105        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:05<00:00,  9.11it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:14<00:00,  5.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.916      0.937      0.958      0.845\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      4.61G     0.4335     0.2948     0.8055        105        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:07<00:00,  8.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:17<00:00,  4.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.932      0.915      0.958       0.85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      4.61G      0.428     0.2867     0.8047         87        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 600/600 [01:06<00:00,  9.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:13<00:00,  5.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.926       0.93      0.958      0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.475 hours.\n",
      "Optimizer stripped from /home/zilian/projects/bachelorproef/runs/detect/train11/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /home/zilian/projects/bachelorproef/runs/detect/train11/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /home/zilian/projects/bachelorproef/runs/detect/train11/weights/best.pt...\n",
      "Ultralytics 8.3.67 ðŸš€ Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,585,077 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 75/75 [00:23<00:00,  3.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       4800      16472      0.926       0.93      0.958      0.852\n",
      "        naaldcontainer       1466       1466      0.982      0.993      0.993      0.971\n",
      "                 spuit       1889       1889      0.886      0.862      0.942      0.698\n",
      "             keukenmes       2009       2009      0.963      0.982      0.991      0.931\n",
      "                infuus         30         30      0.643      0.733      0.742      0.597\n",
      "           stethoscoop       1803       1803      0.973      0.997      0.994      0.978\n",
      "               bol wol       1099       1099      0.916      0.987      0.983      0.862\n",
      "                 snoep       1262       1262      0.943      0.953      0.982      0.852\n",
      "               nuchter        760        760      0.957      0.993      0.994      0.978\n",
      "             fotokader       1087       1087      0.987      0.992      0.995      0.979\n",
      "              iced tea       1598       1598      0.989      0.994      0.995      0.884\n",
      "                  bril       1814       1814      0.982      0.994      0.993      0.903\n",
      "                scherm        969        969      0.969      0.992      0.994      0.987\n",
      "              rollator        421        421      0.972      0.993      0.995      0.978\n",
      "       ampulevloeistof         71         71      0.851      0.802      0.901      0.582\n",
      "          ampulepoeder        194        194      0.881      0.689      0.881      0.595\n",
      "        naaldcontainer       1466       1466      0.982      0.993      0.993      0.971\n",
      "                 spuit       1889       1889      0.886      0.862      0.942      0.698\n",
      "             keukenmes       2009       2009      0.963      0.982      0.991      0.931\n",
      "                infuus         30         30      0.643      0.733      0.742      0.597\n",
      "           stethoscoop       1803       1803      0.973      0.997      0.994      0.978\n",
      "               bol wol       1099       1099      0.916      0.987      0.983      0.862\n",
      "                 snoep       1262       1262      0.943      0.953      0.982      0.852\n",
      "               nuchter        760        760      0.957      0.993      0.994      0.978\n",
      "             fotokader       1087       1087      0.987      0.992      0.995      0.979\n",
      "              iced tea       1598       1598      0.989      0.994      0.995      0.884\n",
      "                  bril       1814       1814      0.982      0.994      0.993      0.903\n",
      "                scherm        969        969      0.969      0.992      0.994      0.987\n",
      "              rollator        421        421      0.972      0.993      0.995      0.978\n",
      "       ampulevloeistof         71         71      0.851      0.802      0.901      0.582\n",
      "          ampulepoeder        194        194      0.881      0.689      0.881      0.595\n",
      "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 1.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/zilian/projects/bachelorproef/runs/detect/train11\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model\n",
    "\n",
    "dataset_path = OBJECT_DETECTION_DATASETS_PATH / \"2000_samples/data.yaml\"\n",
    "results = model.train(\n",
    "    data=dataset_path, epochs=20, imgsz=IMG_CROP_SIZE, device=\"cuda\", batch=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "27b130ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"data/models/object_detection\") / \"2000_samples.pt\"\n",
    "model.save(str(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8edaa",
   "metadata": {},
   "source": [
    "## Extract frames for trial recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40e50597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  33%|â–ˆâ–ˆâ–ˆâ–Ž      | 1/3 [00:08<00:17,  8.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 2/3 [00:12<00:06,  6.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.80s/it]\n",
      "Extracting frames: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:17<00:00,  5.80s/it]\n"
     ]
    }
   ],
   "source": [
    "if not RECORDING_FRAMES_PATH.exists():\n",
    "    RECORDING_FRAMES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for recording_id in tqdm(FULLY_LABELED_RECORDINGS, desc=\"Extracting frames\"):\n",
    "    print(f\"Extracting frames for {recording_id}\")\n",
    "    recording_video_path = RECORDINGS_PATH / f\"{recording_id}.mp4\"\n",
    "    recording_frames_path = RECORDING_FRAMES_PATH / recording_id\n",
    "\n",
    "    if recording_frames_path.exists():\n",
    "        shutil.rmtree(recording_frames_path)\n",
    "    recording_frames_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    extract_frames_to_dir(recording_video_path, recording_frames_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelorproef-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
