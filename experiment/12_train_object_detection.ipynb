{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db50a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CHECKPOINTS_PATH\"] = \"../checkpoints\"\n",
    "os.environ[\"TRACKING_RESULTS_PATH\"] = \"data/processed_tracking_results\"\n",
    "\n",
    "import itertools\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from experiment.settings import (\n",
    "    CLASS_ID_TO_NAME,\n",
    "    FULLY_LABELED_RECORDINGS,\n",
    "    LABELING_REC_SAME_BACKGROUND_ID,\n",
    "    TRAINING_DATASETS_PATH,\n",
    "    SIMROOM_ID,\n",
    "    RECORDINGS_PATH,\n",
    "    RECORDING_FRAMES_PATH,\n",
    "    IGNORED_CLASS_IDS\n",
    ")\n",
    "from src.api.db import Session, engine\n",
    "from src.api.repositories import simrooms_repo\n",
    "from src.api.services import simrooms_service\n",
    "import cv2\n",
    "import numpy as np\n",
    "from src.utils import extract_frames_to_dir\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ef725",
   "metadata": {},
   "source": [
    "# Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c609dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECT_DETECTION_DATASETS_PATH = TRAINING_DATASETS_PATH / \"object_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16b92d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracking_results_per_class(session: Session, labeling_recording_id: str):\n",
    "    calibration_id = simrooms_repo.get_calibration_recording(\n",
    "        session, simroom_id=SIMROOM_ID, recording_id=labeling_recording_id\n",
    "    ).id\n",
    "    tracked_classes = simrooms_repo.get_tracked_classes(session, calibration_id)\n",
    "\n",
    "    if len(tracked_classes) != 15:\n",
    "        raise ValueError(f\"Expected 15 tracked classes but got {len(tracked_classes)}\")\n",
    "\n",
    "    tracking_results_per_class = {\n",
    "        tracked_class.id: simrooms_repo.get_tracking_result_paths(\n",
    "            session, calibration_id, tracked_class.id\n",
    "        )\n",
    "        for tracked_class in tracked_classes\n",
    "    }\n",
    "\n",
    "    return tracking_results_per_class, tracked_classes\n",
    "\n",
    "def get_per_class_metadata(\n",
    "    session: Session,\n",
    "    labeling_recording_id: str,\n",
    "    ignore_classes: list = None,\n",
    "):\n",
    "    tracking_results_per_class, tracked_classes = get_tracking_results_per_class(\n",
    "        session, labeling_recording_id\n",
    "    )\n",
    "    per_class_metadata = {}\n",
    "\n",
    "    for tracked_class in tracked_classes:\n",
    "        if ignore_classes and tracked_class.id in ignore_classes:\n",
    "            print(\n",
    "                f\"Class {CLASS_ID_TO_NAME[tracked_class.id]} is ignored, skipping it\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        class_id = tracked_class.id\n",
    "        tracking_results = tracking_results_per_class[class_id]\n",
    "\n",
    "        frame_indexes = []\n",
    "        laplacian_variances = []\n",
    "        mask_areas = []\n",
    "        bboxes = []\n",
    "        for tracking_result in tracking_results:\n",
    "            file = np.load(tracking_result)\n",
    "            if int(tracking_result.stem) != int(file[\"frame_idx\"]):\n",
    "                raise ValueError(\n",
    "                    f\"Frame index mismatch: {tracking_result.stem} != {file['frame_idx']}\"\n",
    "                )\n",
    "\n",
    "            frame_indexes.append(int(tracking_result.stem))\n",
    "            laplacian_variances.append(file[\"laplacian_variance\"])\n",
    "            mask_areas.append(np.sum(file[\"mask\"]))\n",
    "            bboxes.append(file[\"box\"])\n",
    "\n",
    "        per_class_metadata[class_id] = {\n",
    "            \"class_name\": tracked_class.class_name,\n",
    "            \"color\": tracked_class.color,\n",
    "            \"frame_indexes\": frame_indexes,\n",
    "            \"laplacian_variances\": laplacian_variances,\n",
    "            \"mask_areas\": mask_areas,\n",
    "            \"bboxes\": bboxes,\n",
    "        }\n",
    "\n",
    "    return per_class_metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2837b",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2576c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_samples_per_class(per_class_metadata, num_samples_per_class):\n",
    "    selected_samples_per_class = {}\n",
    "\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "        num_available = len(sample_list)\n",
    "                \n",
    "        selected_indices_list = []\n",
    "\n",
    "        if num_available >= num_samples_per_class:\n",
    "            # Enough samples available, sample WITHOUT replacement\n",
    "            selected_indices_list = np.random.choice(\n",
    "                num_available, \n",
    "                size=num_samples_per_class, \n",
    "                replace=False\n",
    "            ).tolist()\n",
    "        else:\n",
    "            # Not enough samples (num_available < num_samples_per_class), so oversample.\n",
    "            # First, include all available samples once.\n",
    "            selected_indices_list.extend(list(range(num_available)))\n",
    "            \n",
    "            num_remaining_to_sample = num_samples_per_class - num_available\n",
    "            \n",
    "            if num_remaining_to_sample > 0:\n",
    "                # Sample the remainder with replacement from the available samples.\n",
    "                oversampled_indices = np.random.choice(\n",
    "                    num_available,\n",
    "                    size=num_remaining_to_sample,\n",
    "                    replace=True\n",
    "                ).tolist()\n",
    "                selected_indices_list.extend(oversampled_indices)\n",
    "            \n",
    "            # Shuffle the combined list to mix original and oversampled instances.\n",
    "            np.random.shuffle(selected_indices_list)\n",
    "\n",
    "        # Retrieve the actual sample data using the selected indices.\n",
    "        final_selected_samples_tuples = [sample_list[i] for i in selected_indices_list]\n",
    "        selected_frame_indexes, selected_bboxes = zip(*final_selected_samples_tuples)\n",
    "        \n",
    "        selected_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": list(selected_frame_indexes),\n",
    "            \"bboxes\": list(selected_bboxes),\n",
    "        }\n",
    "\n",
    "    return selected_samples_per_class\n",
    "    \n",
    "\n",
    "def get_samples_per_frame(per_class_metadata):\n",
    "    samples_per_frame = {}\n",
    "\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        # zip all metadata together\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        for frame_index, bbox in sample_list:\n",
    "            if frame_index not in samples_per_frame:\n",
    "                samples_per_frame[frame_index] = []\n",
    "            samples_per_frame[frame_index].append((class_id, bbox))\n",
    "\n",
    "    return samples_per_frame\n",
    "\n",
    "\n",
    "def get_train_val_split(\n",
    "    selected_samples_per_class,\n",
    "    train_ratio=0.8,\n",
    "):\n",
    "    train_samples_per_class = {}\n",
    "    val_samples_per_class = {}\n",
    "\n",
    "    for class_id, metadata in selected_samples_per_class.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        # zip all metadata together\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        # shuffle the samples\n",
    "        np.random.shuffle(sample_list)\n",
    "\n",
    "        # split the samples into train and val\n",
    "        split_index = int(len(sample_list) * train_ratio)\n",
    "        train_samples = sample_list[:split_index]\n",
    "        val_samples = sample_list[split_index:]\n",
    "\n",
    "        # unzip the selected samples\n",
    "        train_frame_indexes, train_bboxes = zip(*train_samples)\n",
    "        val_frame_indexes, val_bboxes = zip(*val_samples)\n",
    "\n",
    "        train_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": train_frame_indexes,\n",
    "            \"bboxes\": train_bboxes,\n",
    "        }\n",
    "        val_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": val_frame_indexes,\n",
    "            \"bboxes\": val_bboxes,\n",
    "        }\n",
    "\n",
    "    return train_samples_per_class, val_samples_per_class\n",
    "\n",
    "\n",
    "def plot_per_class_laplacian_variance(per_class_metadata):\n",
    "    # Plot boxplots of laplacian variances per class in a single graph (one boxplot per class)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        laplacian_variances = metadata[\"laplacian_variances\"]\n",
    "        ax.boxplot(\n",
    "            laplacian_variances,\n",
    "            positions=[class_id],\n",
    "            widths=0.5,\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor=metadata[\"color\"]),\n",
    "        )\n",
    "    ax.set_xticks(list(per_class_metadata.keys()))\n",
    "    ax.set_xticklabels([\n",
    "        metadata[\"class_name\"] for metadata in per_class_metadata.values()\n",
    "    ])\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Laplacian Variance\")\n",
    "    ax.set_title(\"Laplacian Variance per Class\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot histograms of laplacian variances per class in a single figure (one histogram per row)\n",
    "    fig, axs = plt.subplots(\n",
    "        len(per_class_metadata), 1, figsize=(12, 6 * len(per_class_metadata))\n",
    "    )\n",
    "    for i, (class_id, metadata) in enumerate(per_class_metadata.items()):\n",
    "        laplacian_variances = metadata[\"laplacian_variances\"]\n",
    "        axs[i].hist(\n",
    "            laplacian_variances,\n",
    "            bins=50,\n",
    "            color=metadata[\"color\"],\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        axs[i].set_title(f\"Laplacian Variance Histogram - {metadata['class_name']}\")\n",
    "        axs[i].set_xlabel(\"Laplacian Variance\")\n",
    "        axs[i].set_ylabel(\"Frequency\")\n",
    "        axs[i].grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe64285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, tmp_frames_dir = simrooms_service.extract_tmp_frames(\n",
    "    LABELING_REC_SAME_BACKGROUND_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a841f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ampulepoeder is ignored, skipping it\n"
     ]
    }
   ],
   "source": [
    "with Session(engine) as session:\n",
    "    per_class_metadata = get_per_class_metadata(session, LABELING_REC_SAME_BACKGROUND_ID, IGNORED_CLASS_IDS)\n",
    "\n",
    "plot_per_class_laplacian_variance(per_class_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d13cb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(\n",
    "    image_np, bboxes, labels, class_name_map=None, color=(0, 255, 0), thickness=2\n",
    "):\n",
    "    img_res = image_np.copy()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "\n",
    "    if not isinstance(bboxes, (list, np.ndarray)):\n",
    "        print(f\"Warning: bboxes is not a list or ndarray: {type(bboxes)}\")\n",
    "        return img_res\n",
    "    if not isinstance(labels, (list, np.ndarray)):\n",
    "        print(f\"Warning: labels is not a list or ndarray: {type(labels)}\")\n",
    "        # Attempt to proceed if labels seem usable, otherwise return\n",
    "        if len(bboxes) != len(labels):\n",
    "            print(\"Warning: bbox and label length mismatch, cannot draw labels.\")\n",
    "            labels = [\"?\" for _ in bboxes]  # Placeholder\n",
    "        elif not all(isinstance(l, (str, int, float)) for l in labels):\n",
    "            print(\"Warning: labels contain non-primitive types, cannot draw reliably.\")\n",
    "            labels = [\"?\" for _ in bboxes]\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        # Assuming bbox format allows direct conversion to int x_min, y_min, x_max, y_max\n",
    "        # This might need adjustment based on the ACTUAL format in your bboxes list\n",
    "        # Example for pascal_voc or albumentations (after denormalizing)\n",
    "        try:\n",
    "            # Check if bbox has at least 4 elements\n",
    "            if len(bbox) < 4:\n",
    "                print(f\"Warning: Skipping invalid bbox (fewer than 4 coords): {bbox}\")\n",
    "                continue\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox[:4])\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Warning: Could not convert bbox coords to int: {bbox}, Error: {e}\")\n",
    "            continue  # Skip this bbox\n",
    "\n",
    "        cv2.rectangle(img_res, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "\n",
    "        label_name = (\n",
    "            str(label)\n",
    "            if class_name_map is None\n",
    "            else class_name_map.get(label, str(label))\n",
    "        )\n",
    "        # Simple text placement above the box\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            label_name, font, font_scale, font_thickness\n",
    "        )\n",
    "        text_y = (\n",
    "            y_min - baseline if y_min - baseline > text_height else y_min + text_height\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img_res, label_name, (x_min, text_y), font, font_scale, color, font_thickness\n",
    "        )\n",
    "\n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90b6ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_per_class_to_samples_per_frame(samples_per_class):\n",
    "    samples_per_frame = {}\n",
    "    for class_id, metadata in samples_per_class.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        for frame_index, bbox in sample_list:\n",
    "            if frame_index not in samples_per_frame:\n",
    "                samples_per_frame[frame_index] = []\n",
    "            samples_per_frame[frame_index].append((class_id, bbox))\n",
    "\n",
    "    return samples_per_frame\n",
    "\n",
    "\n",
    "def create_data_files(\n",
    "    labels_path: Path,\n",
    "    images_path: Path,\n",
    "    class_label_to_model_id,\n",
    "    sample_idx,\n",
    "    image,\n",
    "    bboxes,\n",
    "    class_labels,\n",
    "):\n",
    "    padded_sample_idx = str(sample_idx).zfill(10)\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    # Save image\n",
    "    image_path = images_path / f\"{padded_sample_idx}.jpg\"\n",
    "    cv2.imwrite(image_path, image)\n",
    "\n",
    "    # Save labels\n",
    "    labels_path = labels_path / f\"{padded_sample_idx}.txt\"\n",
    "\n",
    "    # transform bboxes to YOLO format\n",
    "    file_rows = []\n",
    "    for bbox, class_label in zip(bboxes, class_labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "\n",
    "        # get xywh format\n",
    "        x_center = (x1 + x2) / 2\n",
    "        y_center = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        # normalize the values\n",
    "        x_center /= image_width\n",
    "        y_center /= image_height\n",
    "        width /= image_width\n",
    "        height /= image_height\n",
    "\n",
    "        model_id = class_label_to_model_id[class_label]\n",
    "        file_rows.append(f\"{model_id} {x_center} {y_center} {width} {height}\")\n",
    "\n",
    "    with open(labels_path, \"w\") as f:\n",
    "        for row in file_rows:\n",
    "            f.write(row + \"\\n\")\n",
    "\n",
    "\n",
    "def create_train_or_val_dataset(\n",
    "    per_class_metadata,\n",
    "    class_label_to_model_id,\n",
    "    train_samples_per_class,\n",
    "    samples_per_frame,\n",
    "    frames,\n",
    "    images_path,\n",
    "    labels_path,\n",
    "    crop_size: int,\n",
    "    is_validation=False,\n",
    "):\n",
    "    selected_samples_per_frame = samples_per_class_to_samples_per_frame(\n",
    "        train_samples_per_class\n",
    "    )\n",
    "\n",
    "    current_sample_idx = 0\n",
    "    for frame_idx, frame in enumerate(tqdm(frames)):\n",
    "        # check if the frame has any samples\n",
    "        if selected_samples_per_frame.get(frame_idx) is None:\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(str(frame))\n",
    "\n",
    "        # gather boxes and labels for the current frame\n",
    "        class_ids, bboxes = zip(*samples_per_frame[frame_idx])\n",
    "        bboxes = np.array(bboxes)\n",
    "        class_labels = [\n",
    "            per_class_metadata[class_id][\"class_name\"] for class_id in class_ids\n",
    "        ]\n",
    "\n",
    "        # for all selected samples in this frame, create crops\n",
    "        for class_id, box in selected_samples_per_frame[frame_idx]:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "            # create a crop around the center of the box\n",
    "            half_crop = crop_size // 2\n",
    "            x_min = max(0, cx - half_crop)\n",
    "            y_min = max(0, cy - half_crop)\n",
    "            x_max = min(image.shape[1], cx + half_crop)\n",
    "            y_max = min(image.shape[0], cy + half_crop)\n",
    "\n",
    "            transform_steps = [\n",
    "                A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max),\n",
    "                A.PadIfNeeded(min_height=crop_size, min_width=crop_size),\n",
    "            ]\n",
    "\n",
    "            if not is_validation:\n",
    "                transform_steps.append(A.HorizontalFlip(p=0.5))\n",
    "                transform_steps.append(A.RandomBrightnessContrast(p=0.2))\n",
    "\n",
    "            transform = A.Compose(\n",
    "                transform_steps,\n",
    "                bbox_params=A.BboxParams(\n",
    "                    format=\"pascal_voc\", label_fields=[\"class_labels\"], min_visibility=0.7\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Augment the image and boxes\n",
    "            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            transformed_image = augmented[\"image\"]\n",
    "            transformed_bboxes = augmented[\"bboxes\"]\n",
    "            transformed_class_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            # Save the transformed image and labels\n",
    "            create_data_files(\n",
    "                labels_path,\n",
    "                images_path,\n",
    "                class_label_to_model_id,\n",
    "                current_sample_idx,\n",
    "                transformed_image,\n",
    "                transformed_bboxes,\n",
    "                transformed_class_labels,\n",
    "            )\n",
    "\n",
    "            current_sample_idx += 1\n",
    "\n",
    "\n",
    "def create_metadata_yaml(\n",
    "    dataset_path: Path,\n",
    "    per_class_metadata: dict,\n",
    "):\n",
    "    abs_dataset_path = dataset_path.resolve()\n",
    "\n",
    "    metadata_yaml = f\"\"\"\n",
    "path: {abs_dataset_path}\n",
    "train: images/train\n",
    "val: images/val\n",
    "names:\n",
    "\"\"\"\n",
    "\n",
    "    class_label_to_model_id = {}\n",
    "    for i, metadata in enumerate(per_class_metadata.values()):\n",
    "        metadata_yaml += f\"  {i}: {metadata['class_name']}\\n\"\n",
    "        class_label_to_model_id[metadata[\"class_name\"]] = i\n",
    "\n",
    "    metadata_yaml_path = dataset_path / \"data.yaml\"\n",
    "    with open(metadata_yaml_path, \"w\") as f:\n",
    "        f.write(metadata_yaml)\n",
    "\n",
    "    return class_label_to_model_id\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    per_class_metadata: dict,\n",
    "    frames: list[Path],\n",
    "    datasets_path: Path,\n",
    "    crop_size: int,\n",
    "    dataset_type: str,\n",
    "    num_samples_per_class: int,\n",
    "):\n",
    "    print(f\"Creating dataset with {num_samples_per_class} samples per class\")\n",
    "\n",
    "    dataset_name = f\"{dataset_type}_{crop_size}_{num_samples_per_class}\"\n",
    "    dataset_path = datasets_path / dataset_name\n",
    "    train_images_path = dataset_path / \"images/train\"\n",
    "    train_labels_path = dataset_path / \"labels/train\"\n",
    "    val_images_path = dataset_path / \"images/val\"\n",
    "    val_labels_path = dataset_path / \"labels/val\"\n",
    "\n",
    "    train_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    train_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "    val_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    val_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Gather necessary metadata\n",
    "    selected_samples_per_class = select_samples_per_class(\n",
    "        per_class_metadata, num_samples_per_class\n",
    "    )\n",
    "    samples_per_frame = get_samples_per_frame(per_class_metadata)\n",
    "    train_samples_per_class, val_samples_per_class = get_train_val_split(\n",
    "        selected_samples_per_class, train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    # Create the dataset\n",
    "    print(f\"Creating training dataset\")\n",
    "    class_label_to_model_id = create_metadata_yaml(dataset_path, per_class_metadata)\n",
    "    create_train_or_val_dataset(\n",
    "        per_class_metadata,\n",
    "        class_label_to_model_id,\n",
    "        train_samples_per_class,\n",
    "        samples_per_frame,\n",
    "        frames,\n",
    "        train_images_path,\n",
    "        train_labels_path,\n",
    "        crop_size=crop_size,\n",
    "    )\n",
    "    print(f\"Creating validation dataset\")\n",
    "    create_train_or_val_dataset(\n",
    "        per_class_metadata,\n",
    "        class_label_to_model_id,\n",
    "        val_samples_per_class,\n",
    "        samples_per_frame,\n",
    "        frames,\n",
    "        val_images_path,\n",
    "        val_labels_path,\n",
    "        crop_size=crop_size,\n",
    "        is_validation=True,\n",
    "    )\n",
    "\n",
    "    return dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ampulepoeder is ignored, skipping it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating datasets:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 1000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:47<00:00, 298.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:12<00:00, 1131.99it/s]\n",
      "Creating datasets:  12%|█▎        | 1/8 [00:59<06:58, 59.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [01:22<00:00, 171.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14121 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "if OBJECT_DETECTION_DATASETS_PATH.exists():\n",
    "    shutil.rmtree(OBJECT_DETECTION_DATASETS_PATH)\n",
    "OBJECT_DETECTION_DATASETS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "crop_size = [480, 640]\n",
    "dataset_type = [\"same_background\"]#, \"different_background\", \"mixed_background\"]\n",
    "num_samples_per_class = [1000, 2000, 3000, 4000]\n",
    "\n",
    "all_combinations = list(\n",
    "    itertools.product(\n",
    "        crop_size,\n",
    "        dataset_type,\n",
    "        num_samples_per_class,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Creating datasets with {len(all_combinations)} combinations\")\n",
    "\n",
    "print(f\"Extracting metadata for recording {LABELING_REC_SAME_BACKGROUND_ID}\")\n",
    "with Session(engine) as session:\n",
    "    per_class_metadata = get_per_class_metadata(session, LABELING_REC_SAME_BACKGROUND_ID, IGNORED_CLASS_IDS)\n",
    "\n",
    "print(f\"Extracting frames for recording {LABELING_REC_SAME_BACKGROUND_ID}\")\n",
    "frames, tmp_frames_dir = simrooms_service.extract_tmp_frames(\n",
    "    LABELING_REC_SAME_BACKGROUND_ID\n",
    ")\n",
    "\n",
    "for combination in tqdm(all_combinations, desc=\"Creating datasets\"):\n",
    "    crop_size, dataset_type, num_samples = combination\n",
    "\n",
    "    dataset_path = create_dataset(\n",
    "        per_class_metadata=per_class_metadata,\n",
    "        frames=frames,\n",
    "        datasets_path=OBJECT_DETECTION_DATASETS_PATH,\n",
    "        crop_size=crop_size,\n",
    "        dataset_type=dataset_type,\n",
    "        num_samples_per_class=num_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a8b75",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aced1ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d5a1f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.143 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.67 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=data/training_datasets/object_detection/2000_samples/data.yaml, epochs=20, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, cache=False, device=cuda, workers=8, project=None, name=train14, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=/home/zilian/projects/bachelorproef/runs/detect/train14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1748010793.456841    2153 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1748010793.473549    2153 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding model.yaml nc=80 with nc=14\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    433402  ultralytics.nn.modules.head.Detect           [14, [64, 128, 256]]          \n",
      "YOLO11n summary: 319 layers, 2,592,570 parameters, 2,592,554 gradients, 6.5 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir /home/zilian/projects/bachelorproef/runs/detect/train14', view at http://localhost:6006/\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/zilian/projects/bachelorproef/experiment/data/training_datasets/object_detection/2000_samples/labels/train.cache... 22400 images, 0 backgrounds, 0 corrupt: 100%|██████████| 22400/22400 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/zilian/projects/bachelorproef/experiment/data/training_datasets/object_detection/2000_samples/labels/val.cache... 5600 images, 0 backgrounds, 0 corrupt: 100%|██████████| 5600/5600 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /home/zilian/projects/bachelorproef/runs/detect/train14/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000556, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added ✅\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/zilian/projects/bachelorproef/runs/detect/train14\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20      4.64G      0.916      1.956     0.9556        163        640: 100%|██████████| 700/700 [01:31<00:00,  7.62it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:16<00:00,  5.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.875        0.9       0.92      0.731\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/20      4.85G     0.7645     0.8427     0.9072        159        640: 100%|██████████| 700/700 [01:23<00:00,  8.37it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:16<00:00,  5.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.909      0.928      0.952      0.783\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/20      4.64G     0.7304     0.6724     0.8978        167        640: 100%|██████████| 700/700 [01:17<00:00,  9.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:17<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.904      0.914      0.944       0.78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/20      4.72G     0.7049     0.5956     0.8895        170        640: 100%|██████████| 700/700 [01:13<00:00,  9.56it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:16<00:00,  5.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.914      0.938      0.961      0.813\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/20      4.79G     0.6692      0.541     0.8804        127        640: 100%|██████████| 700/700 [01:13<00:00,  9.46it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:16<00:00,  5.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.923      0.929      0.959      0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/20      4.65G     0.6452     0.5106     0.8747        145        640: 100%|██████████| 700/700 [01:17<00:00,  9.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.928      0.938      0.968      0.837\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/20      4.66G     0.6276     0.4857     0.8702        157        640: 100%|██████████| 700/700 [01:13<00:00,  9.50it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:17<00:00,  5.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.925      0.943      0.972      0.847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/20      4.65G     0.6141     0.4688     0.8685        182        640: 100%|██████████| 700/700 [01:13<00:00,  9.47it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.931      0.947      0.972      0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/20      4.64G     0.6008     0.4531     0.8632        125        640: 100%|██████████| 700/700 [01:15<00:00,  9.32it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.932      0.951      0.975      0.862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/20      4.64G     0.5864     0.4418     0.8601        146        640: 100%|██████████| 700/700 [01:19<00:00,  8.76it/s] \n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:15<00:00,  5.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.927      0.954      0.975      0.863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/20      4.61G     0.5486     0.3934      0.834         99        640: 100%|██████████| 700/700 [01:14<00:00,  9.43it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:15<00:00,  5.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.943      0.956      0.978       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/20      4.61G     0.5347     0.3753     0.8302         84        640: 100%|██████████| 700/700 [01:11<00:00,  9.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:17<00:00,  5.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.945       0.96      0.979      0.872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/20      4.61G     0.5205     0.3634      0.826         98        640: 100%|██████████| 700/700 [01:11<00:00,  9.75it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:17<00:00,  5.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.947      0.961       0.98      0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/20      4.61G       0.51      0.354     0.8235        102        640: 100%|██████████| 700/700 [01:11<00:00,  9.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.954      0.961      0.982      0.884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/20      4.61G     0.5015     0.3431     0.8208         98        640: 100%|██████████| 700/700 [01:11<00:00,  9.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.952      0.961      0.982      0.886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20      4.61G     0.4931      0.334      0.819         93        640: 100%|██████████| 700/700 [01:11<00:00,  9.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.953      0.967      0.984      0.889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/20      4.61G     0.4817     0.3234     0.8164        109        640: 100%|██████████| 700/700 [01:10<00:00,  9.94it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:17<00:00,  5.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.958      0.966      0.985      0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/20      4.61G     0.4725     0.3148     0.8136        111        640: 100%|██████████| 700/700 [01:11<00:00,  9.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.958      0.967      0.985      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/20      4.61G     0.4657     0.3074     0.8127        119        640: 100%|██████████| 700/700 [01:14<00:00,  9.36it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:17<00:00,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124      0.962      0.964      0.986      0.896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/20      4.61G     0.4555     0.2983       0.81        104        640: 100%|██████████| 700/700 [01:10<00:00,  9.92it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:18<00:00,  4.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124       0.96      0.968      0.986      0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "20 epochs completed in 0.517 hours.\n",
      "Optimizer stripped from /home/zilian/projects/bachelorproef/runs/detect/train14/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from /home/zilian/projects/bachelorproef/runs/detect/train14/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating /home/zilian/projects/bachelorproef/runs/detect/train14/weights/best.pt...\n",
      "Ultralytics 8.3.67 🚀 Python-3.10.12 torch-2.5.1+cu121 CUDA:0 (NVIDIA GeForce RTX 4090, 24564MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,584,882 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|██████████| 88/88 [00:23<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all       5600      19124       0.96      0.969      0.986      0.899\n",
      "        naaldcontainer       1460       1460      0.975      0.998      0.994      0.975\n",
      "                 spuit       1951       1951      0.909       0.83      0.941       0.69\n",
      "             keukenmes       2123       2123      0.979      0.978       0.99       0.93\n",
      "                infuus        479        479      0.895      0.943      0.977      0.867\n",
      "           stethoscoop       2014       2014      0.965      0.995      0.992      0.974\n",
      "               bol wol       1200       1200      0.943      0.974      0.984      0.863\n",
      "                 snoep       1598       1598      0.956      0.964      0.988      0.865\n",
      "               nuchter        756        756      0.955      0.989       0.99      0.974\n",
      "             fotokader       1100       1100      0.986      0.994      0.994       0.98\n",
      "              iced tea       1628       1628       0.99      0.993      0.994      0.887\n",
      "                  bril       1921       1921      0.977      0.981      0.991      0.899\n",
      "               monitor       1009       1009      0.966      0.992      0.994      0.988\n",
      "              rollator        429        429      0.984      0.993      0.995      0.981\n",
      "       ampulevloeistof       1456       1456      0.957      0.935      0.981      0.712\n",
      "        naaldcontainer       1460       1460      0.975      0.998      0.994      0.975\n",
      "                 spuit       1951       1951      0.909       0.83      0.941       0.69\n",
      "             keukenmes       2123       2123      0.979      0.978       0.99       0.93\n",
      "                infuus        479        479      0.895      0.943      0.977      0.867\n",
      "           stethoscoop       2014       2014      0.965      0.995      0.992      0.974\n",
      "               bol wol       1200       1200      0.943      0.974      0.984      0.863\n",
      "                 snoep       1598       1598      0.956      0.964      0.988      0.865\n",
      "               nuchter        756        756      0.955      0.989       0.99      0.974\n",
      "             fotokader       1100       1100      0.986      0.994      0.994       0.98\n",
      "              iced tea       1628       1628       0.99      0.993      0.994      0.887\n",
      "                  bril       1921       1921      0.977      0.981      0.991      0.899\n",
      "               monitor       1009       1009      0.966      0.992      0.994      0.988\n",
      "              rollator        429        429      0.984      0.993      0.995      0.981\n",
      "       ampulevloeistof       1456       1456      0.957      0.935      0.981      0.712\n",
      "Speed: 0.1ms preprocess, 0.4ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/zilian/projects/bachelorproef/runs/detect/train14\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model\n",
    "\n",
    "dataset_path = OBJECT_DETECTION_DATASETS_PATH / \"2000_samples/data.yaml\"\n",
    "results = model.train(\n",
    "    data=dataset_path, epochs=20, imgsz=IMG_CROP_SIZE, device=\"cuda\", batch=32\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b130ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"data/models/object_detection\") / \"2000_samples.pt\"\n",
    "model.save(str(model_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8edaa",
   "metadata": {},
   "source": [
    "## Extract frames for trial recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e50597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  10%|█         | 1/10 [00:07<01:03,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  20%|██        | 2/10 [00:11<00:44,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  30%|███       | 3/10 [00:16<00:37,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  40%|████      | 4/10 [00:21<00:31,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  50%|█████     | 5/10 [00:25<00:24,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 98128cdc-ffeb-40cb-9528-573e25028e87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  60%|██████    | 6/10 [00:29<00:17,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  70%|███████   | 7/10 [00:33<00:12,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  80%|████████  | 8/10 [00:40<00:10,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  90%|█████████ | 9/10 [00:45<00:05,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|██████████| 10/10 [00:50<00:00,  5.02s/it]\n",
      "Extracting frames: 100%|██████████| 10/10 [00:50<00:00,  5.02s/it]\n"
     ]
    }
   ],
   "source": [
    "if not RECORDING_FRAMES_PATH.exists():\n",
    "    RECORDING_FRAMES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for recording_id in tqdm(FULLY_LABELED_RECORDINGS, desc=\"Extracting frames\"):\n",
    "    print(f\"Extracting frames for {recording_id}\")\n",
    "    recording_video_path = RECORDINGS_PATH / f\"{recording_id}.mp4\"\n",
    "    recording_frames_path = RECORDING_FRAMES_PATH / recording_id\n",
    "\n",
    "    if recording_frames_path.exists():\n",
    "        shutil.rmtree(recording_frames_path)\n",
    "    recording_frames_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    extract_frames_to_dir(recording_video_path, recording_frames_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelorproef-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
