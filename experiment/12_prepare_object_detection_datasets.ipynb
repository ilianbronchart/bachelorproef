{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db50a41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CHECKPOINTS_PATH\"] = \"../checkpoints\"\n",
    "os.environ[\"TRACKING_RESULTS_PATH\"] = \"data/processed_tracking_results\"\n",
    "\n",
    "import itertools\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from experiment.settings import (\n",
    "    CLASS_ID_TO_NAME,\n",
    "    FULLY_LABELED_RECORDINGS,\n",
    "    LABELING_REC_SAME_BACKGROUND_ID,\n",
    "    TRAINING_DATASETS_PATH,\n",
    "    SIMROOM_ID,\n",
    "    RECORDINGS_PATH,\n",
    "    RECORDING_FRAMES_PATH,\n",
    "    IGNORED_CLASS_IDS,\n",
    "    OBJECT_DETECTION_MODELS_PATH,\n",
    ")\n",
    "from src.api.db import Session, engine\n",
    "from src.api.repositories import simrooms_repo\n",
    "from src.api.services import simrooms_service\n",
    "import cv2\n",
    "import numpy as np\n",
    "from src.utils import extract_frames_to_dir\n",
    "import albumentations as A\n",
    "from tqdm import tqdm\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0ef725",
   "metadata": {},
   "source": [
    "# Global Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c609dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "OBJECT_DETECTION_DATASETS_PATH = TRAINING_DATASETS_PATH / \"object_detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "16b92d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tracking_results_per_class(session: Session, labeling_recording_id: str):\n",
    "    calibration_id = simrooms_repo.get_calibration_recording(\n",
    "        session, simroom_id=SIMROOM_ID, recording_id=labeling_recording_id\n",
    "    ).id\n",
    "    tracked_classes = simrooms_repo.get_tracked_classes(session, calibration_id)\n",
    "\n",
    "    if len(tracked_classes) != 15:\n",
    "        raise ValueError(f\"Expected 15 tracked classes but got {len(tracked_classes)}\")\n",
    "\n",
    "    tracking_results_per_class = {\n",
    "        tracked_class.id: simrooms_repo.get_tracking_result_paths(\n",
    "            session, calibration_id, tracked_class.id\n",
    "        )\n",
    "        for tracked_class in tracked_classes\n",
    "    }\n",
    "\n",
    "    return tracking_results_per_class, tracked_classes\n",
    "\n",
    "\n",
    "def get_per_class_metadata(\n",
    "    session: Session,\n",
    "    labeling_recording_id: str,\n",
    "    ignore_classes: list = None,\n",
    "):\n",
    "    tracking_results_per_class, tracked_classes = get_tracking_results_per_class(\n",
    "        session, labeling_recording_id\n",
    "    )\n",
    "    per_class_metadata = {}\n",
    "\n",
    "    for tracked_class in tracked_classes:\n",
    "        if ignore_classes and tracked_class.id in ignore_classes:\n",
    "            print(f\"Class {CLASS_ID_TO_NAME[tracked_class.id]} is ignored, skipping it\")\n",
    "            continue\n",
    "\n",
    "        class_id = tracked_class.id\n",
    "        tracking_results = tracking_results_per_class[class_id]\n",
    "\n",
    "        frame_indexes = []\n",
    "        laplacian_variances = []\n",
    "        mask_areas = []\n",
    "        bboxes = []\n",
    "        for tracking_result in tracking_results:\n",
    "            file = np.load(tracking_result)\n",
    "            if int(tracking_result.stem) != int(file[\"frame_idx\"]):\n",
    "                raise ValueError(\n",
    "                    f\"Frame index mismatch: {tracking_result.stem} != {file['frame_idx']}\"\n",
    "                )\n",
    "\n",
    "            frame_indexes.append(int(tracking_result.stem))\n",
    "            laplacian_variances.append(file[\"laplacian_variance\"])\n",
    "            mask_areas.append(np.sum(file[\"mask\"]))\n",
    "            bboxes.append(file[\"box\"])\n",
    "\n",
    "        per_class_metadata[class_id] = {\n",
    "            \"class_name\": tracked_class.class_name,\n",
    "            \"color\": tracked_class.color,\n",
    "            \"frame_indexes\": frame_indexes,\n",
    "            \"laplacian_variances\": laplacian_variances,\n",
    "            \"mask_areas\": mask_areas,\n",
    "            \"bboxes\": bboxes,\n",
    "        }\n",
    "\n",
    "    return per_class_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2837b",
   "metadata": {},
   "source": [
    "# Create Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2576c8db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_samples_per_class(per_class_metadata, num_samples_per_class):\n",
    "    selected_samples_per_class = {}\n",
    "\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "        num_available = len(sample_list)\n",
    "\n",
    "        selected_indices_list = []\n",
    "\n",
    "        if num_available >= num_samples_per_class:\n",
    "            # Enough samples available, sample WITHOUT replacement\n",
    "            selected_indices_list = np.random.choice(\n",
    "                num_available, size=num_samples_per_class, replace=False\n",
    "            ).tolist()\n",
    "        else:\n",
    "            # Not enough samples (num_available < num_samples_per_class), so oversample.\n",
    "            # First, include all available samples once.\n",
    "            selected_indices_list.extend(list(range(num_available)))\n",
    "\n",
    "            num_remaining_to_sample = num_samples_per_class - num_available\n",
    "\n",
    "            if num_remaining_to_sample > 0:\n",
    "                # Sample the remainder with replacement from the available samples.\n",
    "                oversampled_indices = np.random.choice(\n",
    "                    num_available, size=num_remaining_to_sample, replace=True\n",
    "                ).tolist()\n",
    "                selected_indices_list.extend(oversampled_indices)\n",
    "\n",
    "            # Shuffle the combined list to mix original and oversampled instances.\n",
    "            np.random.shuffle(selected_indices_list)\n",
    "\n",
    "        # Retrieve the actual sample data using the selected indices.\n",
    "        final_selected_samples_tuples = [sample_list[i] for i in selected_indices_list]\n",
    "        selected_frame_indexes, selected_bboxes = zip(*final_selected_samples_tuples)\n",
    "\n",
    "        selected_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": list(selected_frame_indexes),\n",
    "            \"bboxes\": list(selected_bboxes),\n",
    "        }\n",
    "\n",
    "    return selected_samples_per_class\n",
    "\n",
    "\n",
    "def get_samples_per_frame(per_class_metadata):\n",
    "    samples_per_frame = {}\n",
    "\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        # zip all metadata together\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        for frame_index, bbox in sample_list:\n",
    "            if frame_index not in samples_per_frame:\n",
    "                samples_per_frame[frame_index] = []\n",
    "            samples_per_frame[frame_index].append((class_id, bbox))\n",
    "\n",
    "    return samples_per_frame\n",
    "\n",
    "\n",
    "def get_train_val_split(\n",
    "    selected_samples_per_class,\n",
    "    train_ratio=0.8,\n",
    "):\n",
    "    train_samples_per_class = {}\n",
    "    val_samples_per_class = {}\n",
    "\n",
    "    for class_id, metadata in selected_samples_per_class.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "\n",
    "        # zip all metadata together\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        # shuffle the samples\n",
    "        np.random.shuffle(sample_list)\n",
    "\n",
    "        # split the samples into train and val\n",
    "        split_index = int(len(sample_list) * train_ratio)\n",
    "        train_samples = sample_list[:split_index]\n",
    "        val_samples = sample_list[split_index:]\n",
    "\n",
    "        # unzip the selected samples\n",
    "        train_frame_indexes, train_bboxes = zip(*train_samples)\n",
    "        val_frame_indexes, val_bboxes = zip(*val_samples)\n",
    "\n",
    "        train_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": train_frame_indexes,\n",
    "            \"bboxes\": train_bboxes,\n",
    "        }\n",
    "        val_samples_per_class[class_id] = {\n",
    "            \"frame_indexes\": val_frame_indexes,\n",
    "            \"bboxes\": val_bboxes,\n",
    "        }\n",
    "\n",
    "    return train_samples_per_class, val_samples_per_class\n",
    "\n",
    "\n",
    "def plot_per_class_laplacian_variance(per_class_metadata):\n",
    "    # Plot boxplots of laplacian variances per class in a single graph (one boxplot per class)\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    for class_id, metadata in per_class_metadata.items():\n",
    "        laplacian_variances = metadata[\"laplacian_variances\"]\n",
    "        ax.boxplot(\n",
    "            laplacian_variances,\n",
    "            positions=[class_id],\n",
    "            widths=0.5,\n",
    "            patch_artist=True,\n",
    "            boxprops=dict(facecolor=metadata[\"color\"]),\n",
    "        )\n",
    "    ax.set_xticks(list(per_class_metadata.keys()))\n",
    "    ax.set_xticklabels([\n",
    "        metadata[\"class_name\"] for metadata in per_class_metadata.values()\n",
    "    ])\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha=\"right\")\n",
    "    ax.set_xlabel(\"Class\")\n",
    "    ax.set_ylabel(\"Laplacian Variance\")\n",
    "    ax.set_title(\"Laplacian Variance per Class\")\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot histograms of laplacian variances per class in a single figure (one histogram per row)\n",
    "    fig, axs = plt.subplots(\n",
    "        len(per_class_metadata), 1, figsize=(12, 6 * len(per_class_metadata))\n",
    "    )\n",
    "    for i, (class_id, metadata) in enumerate(per_class_metadata.items()):\n",
    "        laplacian_variances = metadata[\"laplacian_variances\"]\n",
    "        axs[i].hist(\n",
    "            laplacian_variances,\n",
    "            bins=50,\n",
    "            color=metadata[\"color\"],\n",
    "            alpha=0.7,\n",
    "            edgecolor=\"black\",\n",
    "        )\n",
    "        axs[i].set_title(f\"Laplacian Variance Histogram - {metadata['class_name']}\")\n",
    "        axs[i].set_xlabel(\"Laplacian Variance\")\n",
    "        axs[i].set_ylabel(\"Frequency\")\n",
    "        axs[i].grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe64285b",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames, tmp_frames_dir = simrooms_service.extract_tmp_frames(\n",
    "    LABELING_REC_SAME_BACKGROUND_ID\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a841f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ampulepoeder is ignored, skipping it\n"
     ]
    }
   ],
   "source": [
    "with Session(engine) as session:\n",
    "    per_class_metadata = get_per_class_metadata(\n",
    "        session, LABELING_REC_SAME_BACKGROUND_ID, IGNORED_CLASS_IDS\n",
    "    )\n",
    "\n",
    "plot_per_class_laplacian_variance(per_class_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d13cb6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_bboxes(\n",
    "    image_np, bboxes, labels, class_name_map=None, color=(0, 255, 0), thickness=2\n",
    "):\n",
    "    img_res = image_np.copy()\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 0.5\n",
    "    font_thickness = 1\n",
    "\n",
    "    if not isinstance(bboxes, (list, np.ndarray)):\n",
    "        print(f\"Warning: bboxes is not a list or ndarray: {type(bboxes)}\")\n",
    "        return img_res\n",
    "    if not isinstance(labels, (list, np.ndarray)):\n",
    "        print(f\"Warning: labels is not a list or ndarray: {type(labels)}\")\n",
    "        # Attempt to proceed if labels seem usable, otherwise return\n",
    "        if len(bboxes) != len(labels):\n",
    "            print(\"Warning: bbox and label length mismatch, cannot draw labels.\")\n",
    "            labels = [\"?\" for _ in bboxes]  # Placeholder\n",
    "        elif not all(isinstance(l, (str, int, float)) for l in labels):\n",
    "            print(\"Warning: labels contain non-primitive types, cannot draw reliably.\")\n",
    "            labels = [\"?\" for _ in bboxes]\n",
    "\n",
    "    for bbox, label in zip(bboxes, labels):\n",
    "        # Assuming bbox format allows direct conversion to int x_min, y_min, x_max, y_max\n",
    "        # This might need adjustment based on the ACTUAL format in your bboxes list\n",
    "        # Example for pascal_voc or albumentations (after denormalizing)\n",
    "        try:\n",
    "            # Check if bbox has at least 4 elements\n",
    "            if len(bbox) < 4:\n",
    "                print(f\"Warning: Skipping invalid bbox (fewer than 4 coords): {bbox}\")\n",
    "                continue\n",
    "            x_min, y_min, x_max, y_max = map(int, bbox[:4])\n",
    "        except (ValueError, TypeError) as e:\n",
    "            print(f\"Warning: Could not convert bbox coords to int: {bbox}, Error: {e}\")\n",
    "            continue  # Skip this bbox\n",
    "\n",
    "        cv2.rectangle(img_res, (x_min, y_min), (x_max, y_max), color, thickness)\n",
    "\n",
    "        label_name = (\n",
    "            str(label)\n",
    "            if class_name_map is None\n",
    "            else class_name_map.get(label, str(label))\n",
    "        )\n",
    "        # Simple text placement above the box\n",
    "        (text_width, text_height), baseline = cv2.getTextSize(\n",
    "            label_name, font, font_scale, font_thickness\n",
    "        )\n",
    "        text_y = (\n",
    "            y_min - baseline if y_min - baseline > text_height else y_min + text_height\n",
    "        )\n",
    "        cv2.putText(\n",
    "            img_res, label_name, (x_min, text_y), font, font_scale, color, font_thickness\n",
    "        )\n",
    "\n",
    "    return img_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "90b6ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_per_class_to_samples_per_frame(samples_per_class):\n",
    "    samples_per_frame = {}\n",
    "    for class_id, metadata in samples_per_class.items():\n",
    "        frame_indexes = metadata[\"frame_indexes\"]\n",
    "        bboxes = metadata[\"bboxes\"]\n",
    "        sample_list = list(zip(frame_indexes, bboxes))\n",
    "\n",
    "        for frame_index, bbox in sample_list:\n",
    "            if frame_index not in samples_per_frame:\n",
    "                samples_per_frame[frame_index] = []\n",
    "            samples_per_frame[frame_index].append((class_id, bbox))\n",
    "\n",
    "    return samples_per_frame\n",
    "\n",
    "\n",
    "def create_data_files(\n",
    "    labels_path: Path,\n",
    "    images_path: Path,\n",
    "    class_label_to_model_id,\n",
    "    sample_idx,\n",
    "    image,\n",
    "    bboxes,\n",
    "    class_labels,\n",
    "):\n",
    "    padded_sample_idx = str(sample_idx).zfill(10)\n",
    "    image_width, image_height = image.shape[1], image.shape[0]\n",
    "\n",
    "    # Save image\n",
    "    image_path = images_path / f\"{padded_sample_idx}.jpg\"\n",
    "    cv2.imwrite(image_path, image)\n",
    "\n",
    "    # Save labels\n",
    "    labels_path = labels_path / f\"{padded_sample_idx}.txt\"\n",
    "\n",
    "    # transform bboxes to YOLO format\n",
    "    file_rows = []\n",
    "    for bbox, class_label in zip(bboxes, class_labels):\n",
    "        x1, y1, x2, y2 = bbox\n",
    "\n",
    "        # get xywh format\n",
    "        x_center = (x1 + x2) / 2\n",
    "        y_center = (y1 + y2) / 2\n",
    "        width = x2 - x1\n",
    "        height = y2 - y1\n",
    "\n",
    "        # normalize the values\n",
    "        x_center /= image_width\n",
    "        y_center /= image_height\n",
    "        width /= image_width\n",
    "        height /= image_height\n",
    "\n",
    "        model_id = class_label_to_model_id[class_label]\n",
    "        file_rows.append(f\"{model_id} {x_center} {y_center} {width} {height}\")\n",
    "\n",
    "    with open(labels_path, \"w\") as f:\n",
    "        for row in file_rows:\n",
    "            f.write(row + \"\\n\")\n",
    "\n",
    "\n",
    "def create_train_or_val_dataset(\n",
    "    per_class_metadata,\n",
    "    class_label_to_model_id,\n",
    "    train_samples_per_class,\n",
    "    samples_per_frame,\n",
    "    frames,\n",
    "    images_path,\n",
    "    labels_path,\n",
    "    crop_size: int,\n",
    "    is_validation=False,\n",
    "):\n",
    "    selected_samples_per_frame = samples_per_class_to_samples_per_frame(\n",
    "        train_samples_per_class\n",
    "    )\n",
    "\n",
    "    current_sample_idx = 0\n",
    "    for frame_idx, frame in enumerate(tqdm(frames)):\n",
    "        # check if the frame has any samples\n",
    "        if selected_samples_per_frame.get(frame_idx) is None:\n",
    "            continue\n",
    "\n",
    "        image = cv2.imread(str(frame))\n",
    "\n",
    "        # gather boxes and labels for the current frame\n",
    "        class_ids, bboxes = zip(*samples_per_frame[frame_idx])\n",
    "        bboxes = np.array(bboxes)\n",
    "        class_labels = [\n",
    "            per_class_metadata[class_id][\"class_name\"] for class_id in class_ids\n",
    "        ]\n",
    "\n",
    "        # for all selected samples in this frame, create crops\n",
    "        for class_id, box in selected_samples_per_frame[frame_idx]:\n",
    "            x1, y1, x2, y2 = box\n",
    "            cx, cy = (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "            # create a crop around the center of the box\n",
    "            half_crop = crop_size // 2\n",
    "            x_min = max(0, cx - half_crop)\n",
    "            y_min = max(0, cy - half_crop)\n",
    "            x_max = min(image.shape[1], cx + half_crop)\n",
    "            y_max = min(image.shape[0], cy + half_crop)\n",
    "\n",
    "            transform_steps = [\n",
    "                A.Crop(x_min=x_min, y_min=y_min, x_max=x_max, y_max=y_max),\n",
    "                A.PadIfNeeded(min_height=crop_size, min_width=crop_size),\n",
    "            ]\n",
    "\n",
    "            if not is_validation:\n",
    "                transform_steps.append(A.HorizontalFlip(p=0.5))\n",
    "                transform_steps.append(A.RandomBrightnessContrast(p=0.2))\n",
    "\n",
    "            transform = A.Compose(\n",
    "                transform_steps,\n",
    "                bbox_params=A.BboxParams(\n",
    "                    format=\"pascal_voc\", label_fields=[\"class_labels\"], min_visibility=0.7\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Augment the image and boxes\n",
    "            augmented = transform(image=image, bboxes=bboxes, class_labels=class_labels)\n",
    "            transformed_image = augmented[\"image\"]\n",
    "            transformed_bboxes = augmented[\"bboxes\"]\n",
    "            transformed_class_labels = augmented[\"class_labels\"]\n",
    "\n",
    "            # Save the transformed image and labels\n",
    "            create_data_files(\n",
    "                labels_path,\n",
    "                images_path,\n",
    "                class_label_to_model_id,\n",
    "                current_sample_idx,\n",
    "                transformed_image,\n",
    "                transformed_bboxes,\n",
    "                transformed_class_labels,\n",
    "            )\n",
    "\n",
    "            current_sample_idx += 1\n",
    "\n",
    "\n",
    "def create_metadata_yaml(\n",
    "    dataset_path: Path,\n",
    "    per_class_metadata: dict,\n",
    "):\n",
    "    abs_dataset_path = dataset_path.resolve()\n",
    "\n",
    "    metadata_yaml = f\"\"\"\n",
    "path: {abs_dataset_path}\n",
    "train: images/train\n",
    "val: images/val\n",
    "names:\n",
    "\"\"\"\n",
    "\n",
    "    class_label_to_model_id = {}\n",
    "    for i, metadata in enumerate(per_class_metadata.values()):\n",
    "        metadata_yaml += f\"  {i}: {metadata['class_name']}\\n\"\n",
    "        class_label_to_model_id[metadata[\"class_name\"]] = i\n",
    "\n",
    "    metadata_yaml_path = dataset_path / \"data.yaml\"\n",
    "    with open(metadata_yaml_path, \"w\") as f:\n",
    "        f.write(metadata_yaml)\n",
    "\n",
    "    return class_label_to_model_id\n",
    "\n",
    "\n",
    "def create_dataset(\n",
    "    per_class_metadata: dict,\n",
    "    frames: list[Path],\n",
    "    datasets_path: Path,\n",
    "    crop_size: int,\n",
    "    dataset_type: str,\n",
    "    num_samples_per_class: int,\n",
    "):\n",
    "    print(f\"Creating dataset with {num_samples_per_class} samples per class\")\n",
    "\n",
    "    dataset_name = f\"{dataset_type}_{crop_size}_{num_samples_per_class}\"\n",
    "    dataset_path = datasets_path / dataset_name\n",
    "    train_images_path = dataset_path / \"images/train\"\n",
    "    train_labels_path = dataset_path / \"labels/train\"\n",
    "    val_images_path = dataset_path / \"images/val\"\n",
    "    val_labels_path = dataset_path / \"labels/val\"\n",
    "\n",
    "    train_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    train_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "    val_images_path.mkdir(parents=True, exist_ok=True)\n",
    "    val_labels_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Gather necessary metadata\n",
    "    selected_samples_per_class = select_samples_per_class(\n",
    "        per_class_metadata, num_samples_per_class\n",
    "    )\n",
    "    samples_per_frame = get_samples_per_frame(per_class_metadata)\n",
    "    train_samples_per_class, val_samples_per_class = get_train_val_split(\n",
    "        selected_samples_per_class, train_ratio=0.8\n",
    "    )\n",
    "\n",
    "    # Create the dataset\n",
    "    print(f\"Creating training dataset\")\n",
    "    class_label_to_model_id = create_metadata_yaml(dataset_path, per_class_metadata)\n",
    "    create_train_or_val_dataset(\n",
    "        per_class_metadata,\n",
    "        class_label_to_model_id,\n",
    "        train_samples_per_class,\n",
    "        samples_per_frame,\n",
    "        frames,\n",
    "        train_images_path,\n",
    "        train_labels_path,\n",
    "        crop_size=crop_size,\n",
    "    )\n",
    "    print(f\"Creating validation dataset\")\n",
    "    create_train_or_val_dataset(\n",
    "        per_class_metadata,\n",
    "        class_label_to_model_id,\n",
    "        val_samples_per_class,\n",
    "        samples_per_frame,\n",
    "        frames,\n",
    "        val_images_path,\n",
    "        val_labels_path,\n",
    "        crop_size=crop_size,\n",
    "        is_validation=True,\n",
    "    )\n",
    "\n",
    "    return dataset_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929c307d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class ampulepoeder is ignored, skipping it\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating datasets:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 1000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:47<00:00, 298.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:12<00:00, 1131.99it/s]\n",
      "Creating datasets:  12%|█▎        | 1/8 [00:59<06:58, 59.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [01:22<00:00, 171.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:26<00:00, 524.07it/s]\n",
      "Creating datasets:  25%|██▌       | 2/8 [02:48<08:52, 88.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 3000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [01:43<00:00, 136.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:36<00:00, 391.96it/s]\n",
      "Creating datasets:  38%|███▊      | 3/8 [05:08<09:20, 112.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 4000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [02:11<00:00, 107.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:53<00:00, 265.23it/s]\n",
      "Creating datasets:  50%|█████     | 4/8 [08:13<09:23, 140.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 1000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:49<00:00, 282.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:15<00:00, 925.54it/s]\n",
      "Creating datasets:  62%|██████▎   | 5/8 [09:19<05:41, 113.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 2000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [01:29<00:00, 157.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:25<00:00, 554.98it/s]\n",
      "Creating datasets:  75%|███████▌  | 6/8 [11:14<03:48, 114.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 3000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [01:56<00:00, 121.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:36<00:00, 381.91it/s]\n",
      "Creating datasets:  88%|████████▊ | 7/8 [13:47<02:07, 127.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dataset with 4000 samples per class\n",
      "Creating training dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [02:24<00:00, 97.46it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14121/14121 [00:43<00:00, 325.11it/s]\n",
      "Creating datasets: 100%|██████████| 8/8 [16:56<00:00, 127.03s/it]\n"
     ]
    }
   ],
   "source": [
    "if OBJECT_DETECTION_DATASETS_PATH.exists():\n",
    "    shutil.rmtree(OBJECT_DETECTION_DATASETS_PATH)\n",
    "OBJECT_DETECTION_DATASETS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "crop_size = [480, 640]\n",
    "dataset_type = [\"same_background\"]  # , \"different_background\", \"mixed_background\"]\n",
    "num_samples_per_class = [1000, 2000, 3000, 4000]\n",
    "\n",
    "all_combinations = list(\n",
    "    itertools.product(\n",
    "        dataset_type,\n",
    "        crop_size,\n",
    "        num_samples_per_class,\n",
    "    )\n",
    ")\n",
    "\n",
    "print(f\"Creating datasets with {len(all_combinations)} combinations\")\n",
    "\n",
    "print(f\"Extracting metadata for recording {LABELING_REC_SAME_BACKGROUND_ID}\")\n",
    "with Session(engine) as session:\n",
    "    per_class_metadata = get_per_class_metadata(\n",
    "        session, LABELING_REC_SAME_BACKGROUND_ID, IGNORED_CLASS_IDS\n",
    "    )\n",
    "\n",
    "print(f\"Extracting frames for recording {LABELING_REC_SAME_BACKGROUND_ID}\")\n",
    "frames, tmp_frames_dir = simrooms_service.extract_tmp_frames(\n",
    "    LABELING_REC_SAME_BACKGROUND_ID\n",
    ")\n",
    "\n",
    "for combination in tqdm(all_combinations, desc=\"Creating datasets\"):\n",
    "    dataset_type, crop_size, num_samples = combination\n",
    "\n",
    "    dataset_path = create_dataset(\n",
    "        per_class_metadata=per_class_metadata,\n",
    "        frames=frames,\n",
    "        datasets_path=OBJECT_DETECTION_DATASETS_PATH,\n",
    "        crop_size=crop_size,\n",
    "        dataset_type=dataset_type,\n",
    "        num_samples_per_class=num_samples,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a8b75",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "See `scripts/train_object_detectors.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e8edaa",
   "metadata": {},
   "source": [
    "## Extract frames for trial recordings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e50597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  10%|█         | 1/10 [00:07<01:03,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  20%|██        | 2/10 [00:11<00:44,  5.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  30%|███       | 3/10 [00:16<00:37,  5.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  40%|████      | 4/10 [00:21<00:31,  5.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  50%|█████     | 5/10 [00:25<00:24,  4.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 98128cdc-ffeb-40cb-9528-573e25028e87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  60%|██████    | 6/10 [00:29<00:17,  4.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  70%|███████   | 7/10 [00:33<00:12,  4.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  80%|████████  | 8/10 [00:40<00:10,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames:  90%|█████████ | 9/10 [00:45<00:05,  5.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting frames for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting frames: 100%|██████████| 10/10 [00:50<00:00,  5.02s/it]\n",
      "Extracting frames: 100%|██████████| 10/10 [00:50<00:00,  5.02s/it]\n"
     ]
    }
   ],
   "source": [
    "if not RECORDING_FRAMES_PATH.exists():\n",
    "    RECORDING_FRAMES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "for recording_id in tqdm(FULLY_LABELED_RECORDINGS, desc=\"Extracting frames\"):\n",
    "    print(f\"Extracting frames for {recording_id}\")\n",
    "    recording_video_path = RECORDINGS_PATH / f\"{recording_id}.mp4\"\n",
    "    recording_frames_path = RECORDING_FRAMES_PATH / recording_id\n",
    "\n",
    "    if recording_frames_path.exists():\n",
    "        shutil.rmtree(recording_frames_path)\n",
    "    recording_frames_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    extract_frames_to_dir(recording_video_path, recording_frames_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelorproef-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
