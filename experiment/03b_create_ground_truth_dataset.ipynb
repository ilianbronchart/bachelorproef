{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sqlalchemy.orm import Session\n",
    "from src.api.db import engine\n",
    "from src.api.models.gaze import GazePoint\n",
    "from src.api.models.pydantic import SimRoomClassDTO\n",
    "from src.api.repositories import simrooms_repo\n",
    "from src.api.services import gaze_service, simrooms_service, labeling_service\n",
    "from src.config import TOBII_GLASSES_FPS, VIEWED_RADIUS\n",
    "from src.api.utils import image_utils\n",
    "from src.utils import (\n",
    "    cv2_video_fps,\n",
    "    cv2_video_frame_count,\n",
    "    cv2_video_resolution,\n",
    "    extract_frames_to_dir,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from experiment.settings import (\n",
    "    FULLY_LABELED_RECORDINGS,\n",
    "    GROUND_TRUTH_PATH,\n",
    "    LABELING_VALIDATION_VIDEOS_PATH,\n",
    "    TRIAL_RECORDING_IDS,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0d202",
   "metadata": {},
   "source": [
    "# Create the ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fb15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mask', 'box', 'roi', 'class_id', 'frame_idx']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.load(\n",
    "        \"/home/zilian/projects/bachelorproef/experiments/experiment/data/labeling_results/2/1/0.npz\"\n",
    "    ).files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58422dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viewed_annotations_per_frame(\n",
    "    annotated_classes: list[SimRoomClassDTO],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    video_resolution: tuple[int, int],\n",
    "):\n",
    "    # Gather all annotation paths for each annotated frame\n",
    "    annotations_per_frame: dict[int, list[Path]] = {}\n",
    "    for anno_class in annotated_classes:\n",
    "        annotation_paths = labeling_service.get_class_tracking_results(anno_class.id)\n",
    "        for annotation_path in annotation_paths:\n",
    "            frame_idx = int(annotation_path.stem)\n",
    "\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            mask = annotation_file[\"mask\"]\n",
    "            x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "\n",
    "            # Put the mask in a tensor of the same size as the video frame\n",
    "            mask_full = np.zeros(video_resolution, dtype=np.uint8)\n",
    "            mask_full[y1:y2, x1:x2] = mask\n",
    "            mask_full_torch = torch.from_numpy(mask_full)\n",
    "\n",
    "            gaze_point = gaze_point_per_frame.get(frame_idx)\n",
    "            if gaze_point is None:\n",
    "                continue\n",
    "\n",
    "            if gaze_service.mask_was_viewed(mask_full_torch, gaze_point.position):\n",
    "                if frame_idx not in annotations_per_frame:\n",
    "                    annotations_per_frame[frame_idx] = []\n",
    "\n",
    "                annotations_per_frame[frame_idx].append(annotation_path)\n",
    "\n",
    "    return annotations_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4acf39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_validation_video_frames(\n",
    "    frames: list[Path],\n",
    "    annotations_per_frame: dict[int, list[Path]],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    annotated_classes: list[SimRoomClassDTO],\n",
    "):\n",
    "    class_id_to_annotated_class = {\n",
    "        anno_class.id: anno_class for anno_class in annotated_classes\n",
    "    }\n",
    "\n",
    "    # Iterate over frames and draw the annotations on them if they exist\n",
    "    for frame in tqdm(frames, desc=\"Drawing annotations on frames\"):\n",
    "        frame_idx = int(frame.stem)\n",
    "        frame_img = cv2.imread(str(frame))\n",
    "\n",
    "        if annotations_per_frame.get(frame_idx) is not None:\n",
    "            for annotation_path in annotations_per_frame[frame_idx]:\n",
    "                annotation_file = np.load(annotation_path)\n",
    "                class_id = int(annotation_file[\"class_id\"])\n",
    "                x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "                mask = annotation_file[\"mask\"]\n",
    "\n",
    "                # Squeeze mask if it has an extra dimension\n",
    "                if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "                    mask = mask[0]\n",
    "                if mask.dtype != bool:\n",
    "                    mask = mask.astype(bool)\n",
    "\n",
    "                color = class_id_to_annotated_class[class_id].color\n",
    "                class_name = class_id_to_annotated_class[class_id].class_name\n",
    "                box = (x1, y1, x2, y2)\n",
    "\n",
    "                frame_img = image_utils.draw_mask(frame_img, mask, box)\n",
    "                frame_img = image_utils.draw_labeled_box(\n",
    "                    frame_img, box, class_name, color\n",
    "                )\n",
    "\n",
    "        # Draw the gaze point on the frame\n",
    "        gaze_point = gaze_point_per_frame.get(frame_idx)\n",
    "        if gaze_point is not None:\n",
    "            gaze_x, gaze_y = gaze_point.position\n",
    "            cv2.circle(\n",
    "                frame_img,\n",
    "                (int(gaze_x), int(gaze_y)),\n",
    "                radius=VIEWED_RADIUS,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=2,\n",
    "            )\n",
    "\n",
    "        # Save the modified image back to its original location\n",
    "        cv2.imwrite(str(frame), frame_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f9814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Getting viewed annotations for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Building ground truth for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [01:49<23:37, 109.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Getting viewed annotations for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Building ground truth for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [03:38<21:48, 109.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Getting viewed annotations for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Building ground truth for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [05:02<00:00, 21.58s/it]\n"
     ]
    }
   ],
   "source": [
    "if not LABELING_VALIDATION_VIDEOS_PATH.exists():\n",
    "    os.makedirs(LABELING_VALIDATION_VIDEOS_PATH)\n",
    "else:\n",
    "    for file in LABELING_VALIDATION_VIDEOS_PATH.glob(\"*.mp4\"):\n",
    "        # os.remove(file)\n",
    "        pass\n",
    "\n",
    "if GROUND_TRUTH_PATH.exists():\n",
    "    GROUND_TRUTH_PATH.unlink()\n",
    "\n",
    "CREATE_VALIDATION_VIDEO = False\n",
    "gt_rows = []\n",
    "for recording_id in tqdm(TRIAL_RECORDING_IDS):\n",
    "    if recording_id not in FULLY_LABELED_RECORDINGS:\n",
    "        continue\n",
    "\n",
    "    with Session(engine) as session:\n",
    "        calibration_recording = simrooms_repo.get_calibration_recording(\n",
    "            db=session,\n",
    "            simroom_id=1,\n",
    "            recording_id=recording_id,\n",
    "        )\n",
    "        annotated_classes = simrooms_service.get_tracked_classes(\n",
    "            session=session,\n",
    "            simroom_id=1,\n",
    "            recording_id=recording_id,\n",
    "        )\n",
    "\n",
    "        trial_recording_path = calibration_recording.recording.video_path\n",
    "        gaze_data_path = calibration_recording.recording.gaze_data_path\n",
    "\n",
    "    trial_video_resolution = cv2_video_resolution(trial_recording_path)\n",
    "    trial_video_fps = cv2_video_fps(trial_recording_path)\n",
    "    trial_video_frame_count = cv2_video_frame_count(trial_recording_path)\n",
    "\n",
    "    # Load and preprocess gaze points\n",
    "    print(f\"Loading gaze data for {recording_id}\")\n",
    "    gaze_point_per_frame = gaze_service.get_gaze_point_per_frame(\n",
    "        gaze_data_path=gaze_data_path,\n",
    "        resolution=trial_video_resolution,\n",
    "        frame_count=trial_video_frame_count,\n",
    "        fps=trial_video_fps,\n",
    "    )\n",
    "\n",
    "    # Get all annotations that were viewed\n",
    "    print(f\"Getting viewed annotations for {recording_id}\")\n",
    "    annotations_per_frame = get_viewed_annotations_per_frame(\n",
    "        annotated_classes=annotated_classes,\n",
    "        gaze_point_per_frame=gaze_point_per_frame,\n",
    "        video_resolution=trial_video_resolution,\n",
    "    )\n",
    "\n",
    "    # Build the ground truth DataFrame\n",
    "    # TODO: Might be interesting to add blur metric per frame to the ground truth dataset\n",
    "    print(f\"Building ground truth for {recording_id}\")\n",
    "    for frame_idx, annotation_paths in annotations_per_frame.items():\n",
    "        for annotation_path in annotation_paths:\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            class_id = int(annotation_file[\"class_id\"])\n",
    "            mask_area = np.sum(annotation_file[\"mask\"])\n",
    "            x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "            roi = annotation_file[\"roi\"]\n",
    "\n",
    "            laplacian_variance = cv2.Laplacian(roi, cv2.CV_64F).var()\n",
    "\n",
    "            gt_rows.append({\n",
    "                \"recording_id\": recording_id,\n",
    "                \"frame_idx\": frame_idx,\n",
    "                \"class_id\": class_id,\n",
    "                \"mask_area\": mask_area,\n",
    "                \"laplacian_variance\": laplacian_variance,\n",
    "                \"x1\": x1,\n",
    "                \"y1\": y1,\n",
    "                \"x2\": x2,\n",
    "                \"y2\": y2,\n",
    "            })\n",
    "\n",
    "    if CREATE_VALIDATION_VIDEO:\n",
    "        # Extract frames from the video and save them to a temporary directory\n",
    "        print(f\"Extracting frames for {recording_id}\")\n",
    "        tmp_frames_dir = tempfile.TemporaryDirectory()\n",
    "        tmp_frames_path = Path(tmp_frames_dir.name)\n",
    "        extract_frames_to_dir(\n",
    "            video_path=trial_recording_path,\n",
    "            frames_path=tmp_frames_path,\n",
    "            print_output=False,\n",
    "        )\n",
    "        frames = sorted(list(tmp_frames_path.glob(\"*.jpg\")), key=lambda x: int(x.stem))\n",
    "\n",
    "        print(f\"Drawing annotations for {recording_id}\")\n",
    "        draw_validation_video_frames(\n",
    "            frames=frames,\n",
    "            annotations_per_frame=annotations_per_frame,\n",
    "            gaze_point_per_frame=gaze_point_per_frame,\n",
    "            annotated_classes=annotated_classes,\n",
    "        )\n",
    "\n",
    "        print(f\"Creating video for {recording_id}\")\n",
    "        cmd = f'ffmpeg -hwaccel cuda -y -pattern_type glob -framerate {TOBII_GLASSES_FPS} -i \"{tmp_frames_path!s}/*.jpg\" -c:v libx264 -pix_fmt yuv420p \"{LABELING_VALIDATION_VIDEOS_PATH}/{recording_id}.mp4\"'\n",
    "        subprocess.run(\n",
    "            cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "\n",
    "ground_truth_df = pd.DataFrame(gt_rows)\n",
    "ground_truth_df.to_csv(\n",
    "    GROUND_TRUTH_PATH,\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
