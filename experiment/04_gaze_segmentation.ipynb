{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CHECKPOINTS_PATH\"] = \"../checkpoints\"\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from sqlalchemy.orm import Session\n",
    "from src.api.db import engine\n",
    "from src.api.models.db import Recording\n",
    "from src.api.services import gaze_service\n",
    "from src.config import CHECKPOINTS_PATH, GAZE_FOV, TOBII_FOV_X, TOBII_GLASSES_FPS\n",
    "from src.utils import cv2_video_fps, cv2_video_frame_count, cv2_video_resolution\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "from ultralytics import FastSAM\n",
    "\n",
    "from experiment.settings import (\n",
    "    FULLY_LABELED_RECORDINGS,\n",
    "    GAZE_SEGMENTATION_RESULTS_PATH,\n",
    ")\n",
    "import tempfile\n",
    "from src.utils import extract_frames_to_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Session(engine) as session:\n",
    "    trial_recordings = (\n",
    "        session.query(Recording).filter(Recording.id.in_(FULLY_LABELED_RECORDINGS)).all()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting and Tracking based on Gaze Data, and grounding based on previously built Vector Index\n",
    "\n",
    "There's a few considerations that might be interesting in an experimental context:\n",
    "1. Selection of `k` in top-k results from the database?\n",
    "2. Segmentation quality (IOU?, Confidence?)\n",
    "3. Adding padding to the bounding boxes?\n",
    "4. Indexing, search parameters? (which ones exist)\n",
    "5. Merging of same-frame ROIs or not?\n",
    "6. Importance of metrics (average, min, max, variance, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeSegmentationJob:\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_path: Path,\n",
    "        gaze_data_path: Path,\n",
    "        results_path: Path,\n",
    "        fovea_fov: float = GAZE_FOV,\n",
    "        fov_x: float = TOBII_FOV_X,\n",
    "        checkpoint_path: str = \"checkpoints/FastSAM-x.pt\",\n",
    "        frames_path: Path | None = None,\n",
    "    ):\n",
    "        self.video_path = video_path\n",
    "        self.gaze_data_path = gaze_data_path\n",
    "        self.fovea_fov = fovea_fov\n",
    "        self.fov_x = fov_x\n",
    "\n",
    "        # Set up the results directory.\n",
    "        self.results_path = results_path\n",
    "        if self.results_path.exists():\n",
    "            shutil.rmtree(self.results_path, ignore_errors=True)\n",
    "            self.results_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Extract frames to a temporary directory.\n",
    "        if frames_path is None:\n",
    "            self.frames_path = Path(tempfile.mkdtemp())\n",
    "            extract_frames_to_dir(\n",
    "                video_path=self.video_path, frames_path=self.frames_path\n",
    "            )\n",
    "        else:\n",
    "            self.frames_path = frames_path\n",
    "            if not self.frames_path.exists():\n",
    "                raise FileNotFoundError(f\"Frames path {self.frames_path} does not exist.\")\n",
    "\n",
    "        # Load the FastSAM model.\n",
    "        self.model = FastSAM(checkpoint_path)\n",
    "\n",
    "        # Video properties.\n",
    "        self.resolution = cv2_video_resolution(self.video_path)\n",
    "        self.aspect_ratio = self.resolution[1] / self.resolution[0]  # W / H\n",
    "        self.fps = cv2_video_fps(self.video_path)\n",
    "        self.viewed_radius = int((self.fovea_fov / self.fov_x) * self.resolution[1])\n",
    "        self.frame_count = cv2_video_frame_count(self.video_path)\n",
    "\n",
    "        # Parse gaze data.\n",
    "        self.gaze_data = gaze_service.parse_gazedata_file(self.gaze_data_path)\n",
    "        self.gaze_points = gaze_service.get_gaze_points(self.gaze_data, self.resolution)\n",
    "\n",
    "        # Map frame indexes to gaze points.\n",
    "        self.frame_gaze_mapping = gaze_service.match_frames_to_gaze(\n",
    "            self.frame_count, self.gaze_points, self.fps\n",
    "        )\n",
    "\n",
    "    def get_gaze_position(self, frame_idx: int) -> tuple[int, int] | None:\n",
    "        \"\"\"\n",
    "        Get the gaze position for a frame index.\n",
    "        \"\"\"\n",
    "        gaze_points = self.frame_gaze_mapping[frame_idx]\n",
    "        if len(gaze_points) == 0:\n",
    "            return None\n",
    "        return gaze_points[0].position\n",
    "\n",
    "    def mask_too_large(self, mask: torch.Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the mask area is less than or equal to 30% of the frame area.\n",
    "\n",
    "        Args:\n",
    "            mask: A tensor containing a single mask of shape (H, W)\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the mask's area is less than or equal to 30% of the frame area, False otherwise.\n",
    "        \"\"\"\n",
    "        MAX_MASK_AREA = 0.1\n",
    "        height, width = mask.shape\n",
    "        frame_area = height * width\n",
    "        max_mask_area = MAX_MASK_AREA * frame_area\n",
    "\n",
    "        mask_area = mask.sum()\n",
    "        return mask_area >= max_mask_area\n",
    "\n",
    "    def run(self):\n",
    "        frame_paths = list(self.frames_path.iterdir())\n",
    "        frame_paths.sort(key=lambda x: int(x.stem))\n",
    "\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for frame_path in frame_paths:\n",
    "                frame_idx = int(frame_path.stem)\n",
    "                results = self.model.track(\n",
    "                    source=str(frame_path), imgsz=640, verbose=False, persist=True\n",
    "                )[0]\n",
    "\n",
    "                try:\n",
    "                    gaze_position = self.get_gaze_position(frame_idx)\n",
    "                    if gaze_position is None:\n",
    "                        continue\n",
    "\n",
    "                    boxes = []\n",
    "                    rois = []\n",
    "                    masks = []\n",
    "                    object_ids = []\n",
    "                    confidences = []\n",
    "                    for i in range(len(results.boxes)):\n",
    "                        confidences.append(float(results.boxes[i].conf))\n",
    "\n",
    "                        mask = F.resize(\n",
    "                            results.masks[i].data,\n",
    "                            self.resolution,\n",
    "                            interpolation=InterpolationMode.NEAREST,\n",
    "                        ).squeeze()\n",
    "\n",
    "                        if not self.mask_too_large(mask) and gaze_service.mask_was_viewed(\n",
    "                            mask, gaze_position\n",
    "                        ):\n",
    "                            box = masks_to_boxes(mask.unsqueeze(0)).int().cpu().numpy()[0]\n",
    "                            x1, y1, x2, y2 = box\n",
    "                            roi = results.orig_img[y1:y2, x1:x2, :]\n",
    "                            roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "                            boxes.append(box)\n",
    "                            masks.append(mask.cpu().numpy().astype(np.uint8))\n",
    "                            rois.append(roi)\n",
    "                            object_ids.append(int(results.boxes.id[i]))\n",
    "\n",
    "                    if len(boxes) > 0:\n",
    "                        rois_array = np.empty(len(rois), dtype=object)\n",
    "                        for i, roi in enumerate(rois):\n",
    "                            rois_array[i] = roi\n",
    "\n",
    "                        masks_array = np.empty(len(masks), dtype=object)\n",
    "                        for i, mask in enumerate(masks):\n",
    "                            masks_array[i] = mask\n",
    "\n",
    "                        # Offload saving with thread pool (asynchronously)\n",
    "                        executor.submit(\n",
    "                            np.savez_compressed,\n",
    "                            self.results_path / f\"{frame_idx}.npz\",\n",
    "                            boxes=boxes,\n",
    "                            rois=rois_array,\n",
    "                            masks=masks_array,\n",
    "                            object_ids=object_ids,\n",
    "                            frame_idx=frame_idx,\n",
    "                            gaze_position=gaze_position,\n",
    "                            confidences=confidences,\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "                    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing recordings: 100%|██████████| 3/3 [05:19<00:00, 106.52s/it]\n"
     ]
    }
   ],
   "source": [
    "if GAZE_SEGMENTATION_RESULTS_PATH.exists():\n",
    "    shutil.rmtree(GAZE_SEGMENTATION_RESULTS_PATH, ignore_errors=True)\n",
    "GAZE_SEGMENTATION_RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "FRAMES_PATHS = Path(\"data/recording_frames\")\n",
    "\n",
    "\n",
    "def process_recording(recording: Recording):\n",
    "    \"\"\"\n",
    "    Process a recording for gaze segmentation.\n",
    "    \"\"\"\n",
    "    recording_id = recording.id\n",
    "    video_path = Path(recording.video_path)\n",
    "    gaze_data_path = Path(recording.gaze_data_path)\n",
    "    frames_path = FRAMES_PATHS / recording_id\n",
    "    results_path = GAZE_SEGMENTATION_RESULTS_PATH / recording_id\n",
    "\n",
    "    if results_path.exists():\n",
    "        shutil.rmtree(results_path, ignore_errors=True)\n",
    "    results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    job = GazeSegmentationJob(\n",
    "        video_path=video_path,\n",
    "        gaze_data_path=gaze_data_path,\n",
    "        results_path=results_path,\n",
    "        fovea_fov=GAZE_FOV,\n",
    "        fov_x=TOBII_FOV_X,\n",
    "        checkpoint_path=CHECKPOINTS_PATH / \"FastSAM-x.pt\",\n",
    "        frames_path=frames_path,\n",
    "    )\n",
    "    job.run()\n",
    "\n",
    "\n",
    "for recording in tqdm(trial_recordings, desc=\"Processing recordings\"):\n",
    "    if recording.id in FULLY_LABELED_RECORDINGS:\n",
    "        process_recording(recording)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.api.utils import image_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import tempfile\n",
    "from src.utils import extract_frames_to_dir\n",
    "\n",
    "RECORDING_ID = \"32f02db7-adc0-4556-a2da-ed2ba60a58c9\"\n",
    "SEG_RESULTS_PATH = GAZE_SEGMENTATION_RESULTS_PATH / RECORDING_ID\n",
    "VIDEO_PATH = Path(\"data/recordings\") / f\"{RECORDING_ID}.mp4\"\n",
    "\n",
    "temp_video_frames_path = Path(tempfile.gettempdir()) / f\"{RECORDING_ID}\"\n",
    "if temp_video_frames_path.exists():\n",
    "    shutil.rmtree(temp_video_frames_path, ignore_errors=True)\n",
    "temp_video_frames_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "extract_frames_to_dir(\n",
    "    video_path=VIDEO_PATH,\n",
    "    frames_path=temp_video_frames_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 975/975 [00:23<00:00, 41.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CompletedProcess(args='ffmpeg -hwaccel cuda -y -pattern_type glob -framerate 24.95 -i \"/tmp/32f02db7-adc0-4556-a2da-ed2ba60a58c9/*.jpg\" -c:v libx264 -pix_fmt yuv420p \"test.mp4\"', returncode=0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frames = list(temp_video_frames_path.iterdir())\n",
    "seg_results = list(SEG_RESULTS_PATH.iterdir())\n",
    "seg_results.sort(key=lambda x: int(x.stem))\n",
    "\n",
    "frame_id_to_path = {int(frame.stem): frame for frame in frames}\n",
    "\n",
    "for i, results in enumerate(tqdm(seg_results)):\n",
    "    frame_idx = int(results.stem)\n",
    "    frame_path = frame_id_to_path[frame_idx]\n",
    "\n",
    "    frame = cv2.imread(str(frame_path))\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    results_file = np.load(results, allow_pickle=True)\n",
    "\n",
    "    boxes = results_file[\"boxes\"]\n",
    "    masks = results_file[\"masks\"]\n",
    "    object_ids = results_file[\"object_ids\"]\n",
    "\n",
    "    combined_mask = np.zeros((frame.shape[0], frame.shape[1]), dtype=np.uint8)\n",
    "    for mask in masks:\n",
    "        combined_mask = np.logical_or(combined_mask, mask)\n",
    "\n",
    "    image_utils.draw_mask(\n",
    "        img=frame,\n",
    "        mask=combined_mask,\n",
    "        box=(0, 0, frame.shape[1], frame.shape[0]),\n",
    "    )\n",
    "\n",
    "    for j in range(len(boxes)):\n",
    "        image_utils.draw_labeled_box(\n",
    "            img=frame, box=tuple(boxes[j]), label=f\"ID: {object_ids[j]}\", color=\"#FF0000\"\n",
    "        )\n",
    "\n",
    "    # save back to original path\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    cv2.imwrite(str(frame_path), frame)\n",
    "\n",
    "import subprocess\n",
    "\n",
    "cmd = f'ffmpeg -hwaccel cuda -y -pattern_type glob -framerate {TOBII_GLASSES_FPS} -i \"{temp_video_frames_path!s}/*.jpg\" -c:v libx264 -pix_fmt yuv420p \"test.mp4\"'\n",
    "subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bachelorproef-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
