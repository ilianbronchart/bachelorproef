This markdown file contains a list of papers encountered during the course of my research. 

# Model Papers

### Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection

```bibtex
@misc{liu2024groundingdinomarryingdino,
      title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, 
      author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Qing Jiang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
      year={2024},
      eprint={2303.05499},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2303.05499}, 
}
```

### SAM 2: Segment Anything in Images and Videos

```bibtex
@misc{ravi2024sam2segmentimages,
      title={SAM 2: Segment Anything in Images and Videos}, 
      author={Nikhila Ravi and Valentin Gabeur and Yuan-Ting Hu and Ronghang Hu and Chaitanya Ryali and Tengyu Ma and Haitham Khedr and Roman Rädle and Chloe Rolland and Laura Gustafson and Eric Mintun and Junting Pan and Kalyan Vasudev Alwala and Nicolas Carion and Chao-Yuan Wu and Ross Girshick and Piotr Dollár and Christoph Feichtenhofer},
      year={2024},
      eprint={2408.00714},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2408.00714}, 
}
```

### Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks

```bibtex
@misc{xiao2023florence2advancingunifiedrepresentation,
      title={Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks}, 
      author={Bin Xiao and Haiping Wu and Weijian Xu and Xiyang Dai and Houdong Hu and Yumao Lu and Michael Zeng and Ce Liu and Lu Yuan},
      year={2023},
      eprint={2311.06242},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2311.06242}, 
}
```

### ReferEverything: Towards Segmenting Everything We Can Speak of in Videos

Future models may allow for grounded segmentation and tracking of objects through a single model.
The code for this paper is not yet released, and thus cannot currently be used for this thesis. 

```bibtex
@misc{Bagchi2024,
  title={ReferEverything: Towards Segmenting Everything We Can Speak of in Videos}, 
  author={Anurag Bagchi and Zhipeng Bao and Yu-Xiong Wang and Pavel Tokmakov and Martial Hebert},
  year={2024},
  eprint={2410.23287},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2410.23287}
}
```

### EfficientViT-SAM: Accelerated Segment Anything Model Without Accuracy Loss

```bibtex
@misc{zhang2024efficientvitsamacceleratedsegmentmodel,
      title={EfficientViT-SAM: Accelerated Segment Anything Model Without Accuracy Loss}, 
      author={Zhuoyang Zhang and Han Cai and Song Han},
      year={2024},
      eprint={2402.05008},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={https://arxiv.org/abs/2402.05008}, 
}
```

# Gaze data analysis papers

We want to know if a user is fixating on an object in the video.

#### Strategies for enhancing automatic fixation detection in head-mounted eye tracking

https://link.springer.com/article/10.3758/s13428-024-02360-0


# Other Papers

### Automatic object detection and tracking for eye-tracking analysis

This paper also uses the Tobii Glasses 3, and may provide useful insights into motion deblurring and object detection for eye-tracking data.

```bibtex
@phdthesis{Cederin2023, 
  series={UPTEC IT}, 
  title={Automatic object detection and tracking for eye-tracking analysis}, 
  url={https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-504416}, 
  author={Cederin, Liv and Bremberg, Ulrika}, 
  year={2023}, 
  collection={UPTEC IT} 
}
```

### GazeSAM: What You See is What You Segment

```bibtex
@misc{Wang2023,
  title={GazeSAM: What You See is What You Segment}, 
  author={Bin Wang and Armstrong Aboah and Zheyuan Zhang and Ulas Bagci},
  year={2023},
  eprint={2304.13844},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2304.13844}
}
```

### SAM2 for constant VRAM video inference:
#INCLUDE I should also probably have a GPU/CPU memory optimization part in my thesis
https://github.com/facebookresearch/sam2/issues/196
https://arxiv.org/pdf/2411.18977 


(ctrl+f for release_old_frames())

https://chatgpt.com/share/67b70726-59d8-800c-8690-58d486d1c4cd

### Object tracking using BoostTrack

https://github.com/vukasin-stanojevic/BoostTrack

### Spatially-Varying Blur Detection Based on Multiscale Fused and Sorted Transform Coefficients of Gradient Magnitudes

https://arxiv.org/pdf/1703.07478.pdf