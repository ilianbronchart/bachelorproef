% Encoding: UTF-8

@article{Cho2024,
  author = {Cho, Seung-Wan and Lim, Yeong-Hyun and Seo, Kyung-Min and Kim, Jungin},
  title = {Integration of eye-tracking and object detection in a deep learning system for quality inspection analysis},
  journaltitle = {Journal of Computational Design and Engineering},
  volume = {11},
  number = {3},
  pages = {158--173},
  date = {2024-05},
  issn = {2288-5048},
  doi = {10.1093/jcde/qwae042},
  url = {https://doi.org/10.1093/jcde/qwae042},
  eprint = {https://academic.oup.com/jcde/article-pdf/11/3/158/58697884/qwae042.pdf},
}

@online{Kulyk2023,
  author = {D. Kulyk},
  date = {2023-07},
  title = {Combining object detection and eye tracking to identify points of interest for VR art exhibition visitors},
  url = {http://essay.utwente.nl/95999/},
  urldate = {2025-05-02},
}

@online{Zhu2024,
  title={Medical SAM 2: Segment medical images as video via Segment Anything Model 2}, 
  author={Jiayuan Zhu and Yunli Qi and Junde Wu},
  date={2024-08-01},
  eprint={2408.00874},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2408.00874},
  urldate = {2024-11-15}
}

@online{Bagchi2024,
  title={ReferEverything: Towards Segmenting Everything We Can Speak of in Videos}, 
  author={Anurag Bagchi and Zhipeng Bao and Yu-Xiong Wang and Pavel Tokmakov and Martial Hebert},
  date={2024-10-30},
  eprint={2410.23287},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2410.23287},
  urldate = {2024-11-15}
}

@online{Liu2023,
  title={Grounding dino: Marrying dino with grounded pre-training for open-set object detection},
  author={Liu, Shilong and Zeng, Zhaoyang and Ren, Tianhe and Li, Feng and Zhang, Hao and Yang, Jie and Li, Chunyuan and Yang, Jianwei and Su, Hang and Zhu, Jun and others},
  journal={arXiv preprint arXiv:2303.05499},
  url={https://arxiv.org/abs/2303.05499},
  date={2023-03-09},
  urldate = {2024-11-15}
}

@Comment{jabref-meta: databaseType:biblatex;}
