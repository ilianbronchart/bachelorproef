% Encoding: UTF-8

@article{Pauszek2023,
  title = {An introduction to eye tracking in human factors healthcare research and medical device testing},
  journaltitle = {Human Factors in Healthcare},
  volume = {3},
  date = {2023},
  issn = {2772-5014},
  doi = {10.1016/j.hfh.2022.100031},
  url = {https://www.sciencedirect.com/science/article/pii/S2772501422000288},
  author = {Joseph R. Pauszek},
  keywords = {Eye tracking, Applied, Human factors, Healthcare, Medical devices},
  abstract = {Eye tracking is a powerful and sophisticated tool that provides an objective glimpse into the cognition of healthcare providers, patients, caregivers, and medical device users. Insights gleaned from eye tracking can be harnessed to better understand – and ultimately improve – the dynamics of healthcare, which quite literally has the potential to save lives. Nonetheless, the use of eye tracking within healthcare research and medical device testing remains in its infancy, which at least partly reflects the learning curve that it demands. As such, the central aim of this article is to provide an easily digestible primer for healthcare researchers and practitioners interested in first getting started with eye tracking. The discussion offers a general overview of how it works, device types and notable specifications, a taxonomy of common metrics, and various sensible best practices and recommendations tailored to the use of wearable eye trackers in a high-fidelity simulated use study context.}
}

@article{Clarke2017,
  author = {Alasdair D. F. Clarke and Aoife Mahon and Alex Irvine and Amelia R. Hunt},
  title ={People Are Unable to Recognize or Report on Their Own Eye Movements},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  volume = {70},
  number = {11},
  pages = {2251--2270},
  date = {2017},
  doi = {10.1080/17470218.2016.1231208},
  note ={PMID: 27595318},
  URL = { https://doi.org/10.1080/17470218.2016.1231208},
  eprint = { https://doi.org/10.1080/17470218.2016.1231208},
  abstract = { Eye movements bring new information into our visual system. The selection of each fixation is the result of a complex interplay of image features, task goals, and biases in motor control and perception. To what extent are we aware of the selection of saccades and their consequences? Here we use a converging methods approach to answer this question in three diverse experiments. In Experiment 1, participants were directed to find a target in a scene by a verbal description of it. We then presented the path the eyes took together with those of another participant. Participants could only identify their own path when the comparison scanpath was searching for a different target. In Experiment 2, participants viewed a scene for three seconds and then named objects from the scene. When asked whether they had looked directly at a given object, participants’ responses were primarily determined by whether or not the object had been named, and not by whether it had been fixated. In Experiment 3, participants executed saccades towards single targets and then viewed a replay of either the eye movement they had just executed or that of someone else. Participants were at chance to identify their own saccade, even when it contained under- and overshoot corrections. The consistent inability to report on one's own eye movements across experiments suggests that awareness of eye movements is extremely impoverished or altogether absent. This is surprising given that information about prior eye movements is clearly used during visual search, motor error correction, and learning. }
}

@article{Frischen2007,
  author    = {Frischen, Alexandra and Bayliss, Andrew P. and Tipper, Steven P.},
  title     = {Gaze cueing of attention: Visual attention, social cognition, and individual differences},
  journaltitle   = {Psychological Bulletin},
  volume    = {133},
  number    = {4},
  pages     = {694--724},
  date      = {2007},
  doi       = {10.1037/0033-2909.133.4.694},
  url       = {https://doi.org/10.1037/0033-2909.133.4.694}
}

@article{Lindeberg2012,
  author = {Lindeberg, T. },
  title   = {{S}cale {I}nvariant {F}eature {T}ransform},
  journaltitle = {Scholarpedia},
  volume  = {7},
  number  = {5},
  doi     = {10.4249/scholarpedia.10491},
  urldate = {2025-05-02},
  date    = {2012-05},
}

@inproceedings{Rublee2011,
  author = {Rublee, Ethan and Rabaud, Vincent and Konolige, Kurt and Bradski, Gary},
  date = {2011},
  month = {11},
  pages = {2564--2571},
  title = {ORB: an efficient alternative to SIFT or SURF},
  booktitle = {Proceedings of the IEEE International Conference on Computer Vision},
  doi = {10.1109/iccv.2011.6126544},
}

@online{ReyOtero2015,
  title={An analysis of the factors affecting keypoint stability in scale-space}, 
  author={Ives Rey-Otero and Jean-Michel Morel and Mauricio Delbracio},
  date={2015},
  eprint={1511.08478},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1511.08478},
  urldate={2025-05-02},
}


@online{Girshick2014,
  title={Rich feature hierarchies for accurate object detection and semantic segmentation}, 
  author={Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik},
  date={2014},
  eprint={1311.2524},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1311.2524}, 
  urldate={2025-05-02},
}

@online{Redmon2016,
  title={You Only Look Once: Unified, Real-Time Object Detection}, 
  author={Joseph Redmon and Santosh Divvala and Ross Girshick and Ali Farhadi},
  date={2016},
  eprint={1506.02640},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1506.02640},
  urldate={2025-05-02},
}

@online{He2018,
  title={Mask R-CNN}, 
  author={Kaiming He and Georgia Gkioxari and Piotr Dollár and Ross Girshick},
  date={2018},
  eprint={1703.06870},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1703.06870},
  urldate={2025-05-02},
}

@online{Khanam2024,
  title={YOLOv11: An Overview of the Key Architectural Enhancements}, 
  author={Rahima Khanam and Muhammad Hussain},
  date={2024},
  eprint={2410.17725},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2410.17725},
  urldate={2025-05-02},
}

@article{Huang2024,
  author = {Huang, Huadong and Wang, Binyu and Xiao, Jiannan and Zhu, Tianyu},
  date = {2024},
  month = {02},
  pages = {80--88},
  title = {Improved small-object detection using YOLOv8: A comparative study},
  volume = {41},
  journaltitle = {Applied and Computational Engineering},
  doi = {10.54254/2755-2721/41/20230714}
}

@article{Hafiz2020,
  title={A survey on instance segmentation: state of the art},
  volume={9},
  ISSN={2192-662X},
  url={http://dx.doi.org/10.1007/s13735-020-00195-x},
  DOI={10.1007/s13735-020-00195-x},
  number={3},
  journaltitle={International Journal of Multimedia Information Retrieval},
  publisher={Springer Science and Business Media LLC},
  author={Hafiz, Abdul Mueed and Bhat, Ghulam Mohiuddin},
  date={2020},
  month=jul, 
  pages={171--189} 
}

@thesis{Cederin2023, 
  series={UPTEC IT}, 
  title={Automatic object detection and tracking for eye-tracking analysis}, 
  url={https://urn.kb.se/resolve?urn=urn:nbn:se:uu:diva-504416}, 
  author={Cederin, Liv and Bremberg, Ulrika}, 
  date={2023}, 
  collection={UPTEC IT},
  institution = {Uppsala University},
  type = {Bachelor's thesis},
}

@online{Carion2020,
  title={End-to-End Object Detection with Transformers}, 
  author={Nicolas Carion and Francisco Massa and Gabriel Synnaeve and Nicolas Usunier and Alexander Kirillov and Sergey Zagoruyko},
  date={2020},
  eprint={2005.12872},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2005.12872},
  urldate={2025-05-02},
}

@online{Oquab2024,
  title={DINOv2: Learning Robust Visual Features without Supervision}, 
  author={Maxime Oquab and Timothée Darcet and Théo Moutakanni and Huy Vo and Marc Szafraniec and Vasil Khalidov and Pierre Fernandez and Daniel Haziza and Francisco Massa and Alaaeldin El-Nouby and Mahmoud Assran and Nicolas Ballas and Wojciech Galuba and Russell Howes and Po-Yao Huang and Shang-Wen Li and Ishan Misra and Michael Rabbat and Vasu Sharma and Gabriel Synnaeve and Hu Xu and Hervé Jegou and Julien Mairal and Patrick Labatut and Armand Joulin and Piotr Bojanowski},
  date={2024},
  eprint={2304.07193},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2304.07193}, 
  urldate={2025-05-02},
}

@online{Kirillov2019,
  title={Panoptic Segmentation}, 
  author={Alexander Kirillov and Kaiming He and Ross Girshick and Carsten Rother and Piotr Dollár},
  date={2019},
  eprint={1801.00868},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/1801.00868},
  urldate={2025-05-02},
}

@online{Zhang2022,
  title={DINO: DETR with Improved DeNoising Anchor Boxes for End-to-End Object Detection}, 
  author={Hao Zhang and Feng Li and Shilong Liu and Lei Zhang and Hang Su and Jun Zhu and Lionel M. Ni and Heung-Yeung Shum},
  date={2022},
  eprint={2203.03605},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2203.03605},
  urldate={2025-05-02},
}

@online{Liu2023,
  title={Grounding DINO: Marrying DINO with Grounded Pre-Training for Open-Set Object Detection}, 
  author={Shilong Liu and Zhaoyang Zeng and Tianhe Ren and Feng Li and Hao Zhang and Jie Yang and Qing Jiang and Chunyuan Li and Jianwei Yang and Hang Su and Jun Zhu and Lei Zhang},
  date={2023},
  eprint={2303.05499},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2303.05499},
  urldate={2025-05-02},
}


@Article{Son2024,
  AUTHOR = {Son, Jinhwan and Jung, Heechul},
  TITLE = {Teacher–Student Model Using Grounding DINO and You Only Look Once for Multi-Sensor-Based Object Detection},
  journaltitle = {Applied Sciences},
  VOLUME = {14},
  date = {2024},
  NUMBER = {6},
  ARTICLE-NUMBER = {2232},
  URL = {https://www.mdpi.com/2076-3417/14/6/2232},
  ISSN = {2076-3417},
  ABSTRACT = {Object detection is a crucial research topic in the fields of computer vision and artificial intelligence, involving the identification and classification of objects within images. Recent advancements in deep learning technologies, such as YOLO (You Only Look Once), Faster-R-CNN, and SSDs (Single Shot Detectors), have demonstrated high performance in object detection. This study utilizes the YOLOv8 model for real-time object detection in environments requiring fast inference speeds, specifically in CCTV and automotive dashcam scenarios. Experiments were conducted using the ‘Multi-Image Identical Situation and Object Identification Data’ provided by AI Hub, consisting of multi-image datasets captured in identical situations using CCTV, dashcams, and smartphones. Object detection experiments were performed on three types of multi-image datasets captured in identical situations. Despite the utility of YOLO, there is a need for performance improvement in the AI Hub dataset. Grounding DINO, a zero-shot object detector with a high mAP performance, is employed. While efficient auto-labeling is possible with Grounding DINO, its processing speed is slower than YOLO, making it unsuitable for real-time object detection scenarios. This study conducts object detection experiments using publicly available labels and utilizes Grounding DINO as a teacher model for auto-labeling. The generated labels are then used to train YOLO as a student model, and performance is compared and analyzed. Experimental results demonstrate that using auto-generated labels for object detection does not lead to degradation in performance. The combination of auto-labeling and manual labeling significantly enhances performance. Additionally, an analysis of datasets containing data from various devices, including CCTV, dashcams, and smartphones, reveals the impact of different device types on the recognition accuracy for distinct devices. Through Grounding DINO, this study proves the efficacy of auto-labeling technology in contributing to efficiency and performance enhancement in the field of object detection, presenting practical applicability.},
  DOI = {10.3390/app14062232}
}

@online{Kirillov2023,
  title={Segment Anything}, 
  author={Alexander Kirillov and Eric Mintun and Nikhila Ravi and Hanzi Mao and Chloe Rolland and Laura Gustafson and Tete Xiao and Spencer Whitehead and Alexander C. Berg and Wan-Yen Lo and Piotr Dollár and Ross Girshick},
  date={2023},
  eprint={2304.02643},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2304.02643}, 
  urldate={2025-05-02},
}

@online{Ravi2024,
  title={SAM 2: Segment Anything in Images and Videos}, 
  author={Nikhila Ravi and Valentin Gabeur and Yuan-Ting Hu and Ronghang Hu and Chaitanya Ryali and Tengyu Ma and Haitham Khedr and Roman Rädle and Chloe Rolland and Laura Gustafson and Eric Mintun and Junting Pan and Kalyan Vasudev Alwala and Nicolas Carion and Chao-Yuan Wu and Ross Girshick and Piotr Dollár and Christoph Feichtenhofer},
  date={2024},
  eprint={2408.00714},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2408.00714},
  urldate={2025-05-02},
}

@online{Zhao2023,
  title={Fast Segment Anything},
  author={Xu Zhao and Wenchao Ding and Yongqi An and Yinglong Du and Tao Yu and Min Li and Ming Tang and Jinqiao Wang},
  date={2023},
  eprint={2306.12156},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2306.12156},
  urldate={2025-05-02},
}

@online{Radford2021,
  title={Learning Transferable Visual Models From Natural Language Supervision}, 
  author={Alec Radford and Jong Wook Kim and Chris Hallacy and Aditya Ramesh and Gabriel Goh and Sandhini Agarwal and Girish Sastry and Amanda Askell and Pamela Mishkin and Jack Clark and Gretchen Krueger and Ilya Sutskever},
  date={2021},
  eprint={2103.00020},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2103.00020}, 
  urldate={2025-05-02},
}

@online{Wang2023,
  title={GazeSAM: What You See is What You Segment}, 
  author={Bin Wang and Armstrong Aboah and Zheyuan Zhang and Ulas Bagci},
  date={2023},
  eprint={2304.13844},
  archivePrefix={arXiv},
  primaryClass={cs.CV},
  url={https://arxiv.org/abs/2304.13844},
  urldate={2025-05-02},
}


@online{Tobii2025a,
  author       = {{Tobii AB}},
  title        = {Tobii Pro Glasses 3},
  date         = {2025},
  url          = {https://www.tobii.com/products/eye-trackers/wearables/tobii-pro-glasses-3},
  urldate      = {2025-05-02},
  note         = {Productpagina van Tobii Pro Glasses 3}
}

@online{Tobii2025b,
  author       = {{Tobii AB}},
  title        = {Understanding Tobii Pro Lab Eye Tracking Metrics},
  date         = {2025},
  url          = {https://connect.tobii.com/s/article/understanding-tobii-pro-lab-eye-tracking-metrics},
  urldate      = {2025-05-02},
  note         = {Uitleg over eyetracking-metrics in Tobii Pro Lab}
}

@online{Tobii2025c,
  author       = {{Tobii AB}},
  title        = {Visualizations for Tobii Pro Lab},
  date         = {2025},
  url          = {https://connect.tobii.com/s/article/Visualizations-for-Tobii-Pro-Lab},
  urldate      = {2025-05-02},
  note         = {Overzicht van visualisatiemogelijkheden in Tobii Pro Lab}
}

@online{Hu2024facebookresearch,
  title = {facebookresearch/sam2},
  url = {https://github.com/facebookresearch/sam2},
  author = {Hu, Ronghang and Niels, Rogge and Khedr, Haitham and Arun and CharlesCNorton and Ryali, Chay and Rädle, Roman and Dymchenko, Sergii and Danwand and Garcia, Diego and jhj0517},
  date = {2024-12-25},
  urldate = {2025-02-20},
}

@online{Hu2025ilianbronchart,
  title = {ilianbronchart/sam2},
  url = {https://github.com/ilianbronchart/sam2},
  author = {Hu, Ronghang and Niels, Rogge and Khedr, Haitham and Arun and CharlesCNorton and Bronchart, Ilian and Ryali, Chay and Rädle, Roman and Dymchenko, Sergii and Danwand and Garcia, Diego and jhj0517},
  date = {2025-03-16},
  urldate = {2025-03-16},
}

@online{heyoeyo2024comment,
  title = {Comment on "Are there any method for reducing gpu memory overhead?"},
  url = {https://github.com/facebookresearch/sam2/issues/196},
  author = {heyoeyo},
  date = {2024-08-13},
  urldate = {2025-02-20},
  note = {Comment on GitHub issue},
}

@online{Tobii2023,
  title = {Tobii Pro Glasses 3 Developer Guide},
  url = {https://go.tobii.com/tobii-pro-glasses-3-developer-guide},
  author = {{Tobii AB}},
  date = {2023-04},
  urldate = {2025-05-13},
}

@incollection{Remington2012,
  title = {Chapter 4 - Retina},
  editor = {Lee Ann Remington},
  booktitle = {Clinical Anatomy and Physiology of the Visual System (Third Edition)},
  publisher = {Butterworth-Heinemann},
  edition = {Third Edition},
  address = {Saint Louis},
  pages = {61--92},
  year = {2012},
  isbn = {978-1-4377-1926-0},
  doi = {10.1016/b978-1-4377-1926-0.10004-9},
  url = {https://www.sciencedirect.com/science/article/pii/B9781437719260100049},
  author = {Lee Ann Remington}
}

@Comment{jabref-meta: databaseType:biblatex;}
