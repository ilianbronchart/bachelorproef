{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c7465f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.api.controllers.generate_embeddings as generate_embeddings\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from sqlalchemy.orm import Session\n",
    "from src.config import GAZE_FOVEA_FOV, TOBII_FOV_X\n",
    "from src.db import engine\n",
    "from src.db.models import Recording, SimRoomClass\n",
    "from src.api.controllers.gaze_segmentation import (\n",
    "    get_gaze_points,\n",
    "    match_frames_to_gaze,\n",
    "    parse_gazedata_file,\n",
    "    mask_was_viewed\n",
    ")\n",
    "from src.utils import cv2_video_fps, cv2_video_frame_count, cv2_video_resolution\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from ultralytics import FastSAM\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from typing import Any\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "986aff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_metadata.json\") as file:\n",
    "    experiment_metadata = json.load(file)\n",
    "    trial_recordings_metadata = experiment_metadata[\"trial_recordings_metadata\"]\n",
    "    trial_recording_uuids = list(trial_recordings_metadata.keys())\n",
    "    labeling_same_background_uuid = experiment_metadata[\"labeling_same_background_uuid\"]\n",
    "    labeling_diff_background_uuid = experiment_metadata[\"labeling_diff_background_uuid\"]\n",
    "\n",
    "with Session(engine) as session:\n",
    "    trial_recordings = (\n",
    "        session.query(Recording).filter(Recording.uuid.in_(trial_recording_uuids)).all()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55c25d3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recording_uuid</th>\n",
       "      <th>frame_idx</th>\n",
       "      <th>class_id</th>\n",
       "      <th>mask_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>67b71a70-da64-467a-9fb6-91bc29265fd1</td>\n",
       "      <td>223</td>\n",
       "      <td>1</td>\n",
       "      <td>5473</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>67b71a70-da64-467a-9fb6-91bc29265fd1</td>\n",
       "      <td>42</td>\n",
       "      <td>1</td>\n",
       "      <td>1449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67b71a70-da64-467a-9fb6-91bc29265fd1</td>\n",
       "      <td>86</td>\n",
       "      <td>1</td>\n",
       "      <td>1405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67b71a70-da64-467a-9fb6-91bc29265fd1</td>\n",
       "      <td>242</td>\n",
       "      <td>1</td>\n",
       "      <td>8207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>67b71a70-da64-467a-9fb6-91bc29265fd1</td>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>1479</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         recording_uuid  frame_idx  class_id  mask_area\n",
       "0  67b71a70-da64-467a-9fb6-91bc29265fd1        223         1       5473\n",
       "1  67b71a70-da64-467a-9fb6-91bc29265fd1         42         1       1449\n",
       "2  67b71a70-da64-467a-9fb6-91bc29265fd1         86         1       1405\n",
       "3  67b71a70-da64-467a-9fb6-91bc29265fd1        242         1       8207\n",
       "4  67b71a70-da64-467a-9fb6-91bc29265fd1         21         1       1479"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GROUND_TRUTH_PATH = Path(\"data/ground_truth.csv\")\n",
    "ground_truth_df = pd.read_csv(GROUND_TRUTH_PATH)\n",
    "ground_truth_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8a8cf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def naive_frame_predictions(candidate_df: pd.DataFrame, confidence_threshold: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each frame and object, selects the candidate class with the minimum min_distance.\n",
    "    Then, for each object id (i.e. track), performs majority voting across frames using only confident predictions.\n",
    "    If an object never has a confident prediction (min_distance <= confidence_threshold), its prediction is -1.\n",
    "    Finally, returns a DataFrame with separate rows for each combination of frame_idx, object_id, predicted class, and the corresponding min_distance.\n",
    "    \n",
    "    Parameters:\n",
    "        candidate_df (pd.DataFrame): DataFrame with columns:\n",
    "            \"frame_idx\", \"object_id\", \"class_id\", \"min_distance\", (and optionally others)\n",
    "        confidence_threshold (float): The maximum min_distance for a prediction to be considered confident.\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with columns:\n",
    "            \"frame_idx\", \"object_id\", \"predicted_class\", \"min_distance\"\n",
    "    \"\"\"\n",
    "    # Step 1: For each (frame_idx, object_id) pair, select the candidate with the lowest min_distance.\n",
    "    frame_level_preds = candidate_df.loc[\n",
    "        candidate_df.groupby([\"frame_idx\", \"object_id\"])[\"min_distance\"].idxmin()\n",
    "    ].copy()\n",
    "    \n",
    "    # Mark each prediction as confident if its min_distance is below or equal to the threshold.\n",
    "    frame_level_preds[\"confident\"] = frame_level_preds[\"min_distance\"] <= confidence_threshold\n",
    "    \n",
    "    # Step 2: For each object_id (across frames), perform majority voting using only confident predictions.\n",
    "    def majority_vote_or_unknown(group: pd.DataFrame):\n",
    "        confident_votes = group[group[\"confident\"]]\n",
    "        if confident_votes.empty:\n",
    "            return -1\n",
    "        else:\n",
    "            # mode()[0] returns one candidate in case of tie.\n",
    "            return confident_votes[\"class_id\"].mode()[0]\n",
    "    \n",
    "    object_predictions = frame_level_preds.groupby(\"object_id\").apply(majority_vote_or_unknown).reset_index()\n",
    "    object_predictions.columns = [\"object_id\", \"final_predicted_class\"]\n",
    "    \n",
    "    # Step 3: Merge the object-level predictions back into the frame-level DataFrame.\n",
    "    # This retains the min_distance value for each frame's candidate.\n",
    "    frame_predictions_with_obj = frame_level_preds.merge(\n",
    "        object_predictions, on=\"object_id\", how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Select and rename the columns for the final output.\n",
    "    final_df = frame_predictions_with_obj[[\"frame_idx\", \"object_id\", \"final_predicted_class\", \"min_distance\"]].copy()\n",
    "    final_df.rename(columns={\"final_predicted_class\": \"predicted_class\"}, inplace=True)\n",
    "    \n",
    "    # Optionally, sort for readability.\n",
    "    final_df.sort_values([\"frame_idx\", \"object_id\"], inplace=True)\n",
    "    \n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c87f9585",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_predictions_to_ground_truth(predictions_df: pd.DataFrame, \n",
    "                                          gt_df: pd.DataFrame, \n",
    "                                          ignore_unknown: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compares predictions_df to gt_df on a frame-level basis and computes extended metrics.\n",
    "    \n",
    "    Parameters:\n",
    "        predictions_df (pd.DataFrame): DataFrame containing columns:\n",
    "            \"frame_idx\", \"object_id\", \"predicted_class\", \"min_distance\", etc.\n",
    "        gt_df (pd.DataFrame): Ground truth DataFrame containing columns:\n",
    "            \"frame_idx\", \"class_id\". Ground truth does not contain unknown predictions.\n",
    "        ignore_unknown (bool): If True, unknown predictions (i.e., predicted_class == -1)\n",
    "            are removed before computing metrics.\n",
    "    \n",
    "    Returns:\n",
    "        merged_df (pd.DataFrame): A DataFrame merging ground truth with predictions. It includes:\n",
    "            - frame_idx, class_id (ground truth)\n",
    "            - predicted_class_set (set of predicted classes for that frame)\n",
    "            - correct (True if ground truth is in predicted_class_set, False otherwise)\n",
    "            - TP, FP, FN for each frame\n",
    "            - per-frame precision (purity) and recall\n",
    "        metrics (dict): A dictionary containing overall metrics: frame-level accuracy, \n",
    "            micro precision, micro recall, F1 score, and average purity.\n",
    "    \"\"\"\n",
    "    # Group predictions by frame_idx to form a set of predicted classes for each frame.\n",
    "    frame_preds = predictions_df.groupby(\"frame_idx\")[\"predicted_class\"].agg(lambda x: set(x)).reset_index()\n",
    "    frame_preds.rename(columns={\"predicted_class\": \"predicted_class_set\"}, inplace=True)\n",
    "    \n",
    "    # Merge with ground truth on frame_idx.\n",
    "    merged_df = gt_df.merge(frame_preds, on=\"frame_idx\", how=\"left\")\n",
    "    \n",
    "    # Define a function to compute per-frame metrics.\n",
    "    def compute_metrics(row):\n",
    "        # Start with the predicted set; if missing, use empty set.\n",
    "        pred_set = row.get(\"predicted_class_set\", set())\n",
    "        if not isinstance(pred_set, set):\n",
    "            pred_set = set()\n",
    "        if ignore_unknown:\n",
    "            pred_set = {p for p in pred_set if p != -1}\n",
    "        gt = row[\"class_id\"]\n",
    "        \n",
    "        # True positive: 1 if ground truth is in pred_set.\n",
    "        TP = 1 if gt in pred_set else 0\n",
    "        # False negative: 1 if ground truth is not predicted.\n",
    "        FN = 0 if TP == 1 else 1\n",
    "        # False positives: any extra predictions besides the ground truth.\n",
    "        FP = (len(pred_set) - 1) if TP == 1 else len(pred_set)\n",
    "        \n",
    "        # Per-frame precision (purity): defined only if there's at least one prediction.\n",
    "        if len(pred_set) > 0:\n",
    "            precision = TP / (TP + FP)\n",
    "        else:\n",
    "            precision = np.nan  # or 0, depending on your preference\n",
    "        # Per-frame recall: here it's either 1 (if predicted) or 0 (if not).\n",
    "        recall = TP  # since FN is 1 when TP==0 and denominator is always 1.\n",
    "        correct = True if TP == 1 else False\n",
    "        \n",
    "        return pd.Series({\n",
    "            \"predicted_class_set\": pred_set,\n",
    "            \"correct\": correct,\n",
    "            \"TP\": TP,\n",
    "            \"FP\": FP,\n",
    "            \"FN\": FN,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall\n",
    "        })\n",
    "    \n",
    "    # Apply the per-row metric computation.\n",
    "    metrics_df = merged_df.apply(compute_metrics, axis=1)\n",
    "    merged_df = pd.concat([merged_df, metrics_df], axis=1)\n",
    "    \n",
    "    # Compute overall (micro-average) metrics.\n",
    "    total_TP = merged_df[\"TP\"].sum()\n",
    "    total_FP = merged_df[\"FP\"].sum()\n",
    "    total_FN = merged_df[\"FN\"].sum()\n",
    "    \n",
    "    micro_precision = total_TP / (total_TP + total_FP) if (total_TP + total_FP) > 0 else np.nan\n",
    "    micro_recall = total_TP / (total_TP + total_FN) if (total_TP + total_FN) > 0 else np.nan\n",
    "    micro_f1 = (2 * micro_precision * micro_recall / (micro_precision + micro_recall)\n",
    "                if (micro_precision + micro_recall) > 0 else np.nan)\n",
    "    \n",
    "    # Frame-level accuracy: proportion of frames correctly predicted.\n",
    "    frame_accuracy = merged_df[\"correct\"].mean()\n",
    "    \n",
    "    # Average purity (average per-frame precision, ignoring frames with no predictions).\n",
    "    avg_purity = merged_df[\"precision\"].dropna().mean()\n",
    "    \n",
    "    metrics = {\n",
    "        \"frame_accuracy\": frame_accuracy,\n",
    "        \"micro_precision\": micro_precision,\n",
    "        \"micro_recall\": micro_recall,\n",
    "        \"micro_f1\": micro_f1,\n",
    "        \"average_purity\": avg_purity\n",
    "    }\n",
    "        \n",
    "    return merged_df, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "399115cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for grounding_dataset_k=100_samples=400:\n",
      "frame_accuracy: 0.7994791666666666\n",
      "micro_precision: 0.9109792284866469\n",
      "micro_recall: 0.7994791666666666\n",
      "micro_f1: 0.8515950069348127\n",
      "average_purity: 0.9341692789968652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1142994/3815977101.py:36: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  object_predictions = frame_level_preds.groupby(\"object_id\").apply(majority_vote_or_unknown).reset_index()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "target_trial_recording = trial_recordings[0]\n",
    "GROUNDING_DATASETS_PATH = Path(\"data/grounding_datasets\")\n",
    "FINAL_PREDICTIONS_PATH = Path(\"data/final_predictions\")\n",
    "\n",
    "pattern = r\"grounding_dataset_k=(\\d+)_samples=(\\d+)\"\n",
    "for grounding_dataset_path in (GROUNDING_DATASETS_PATH / target_trial_recording.uuid).iterdir():\n",
    "    match = re.search(pattern, grounding_dataset_path.stem)\n",
    "\n",
    "    if match:\n",
    "        k = int(match.group(1))\n",
    "        num_samples = int(match.group(2))\n",
    "    else:\n",
    "        raise ValueError(\"Filename does not match the expected pattern.\")\n",
    "    \n",
    "    grounding_df = pd.read_csv(grounding_dataset_path)\n",
    "    predictions_df = naive_frame_predictions(grounding_df, confidence_threshold=0.5)\n",
    "\n",
    "    predictions_path = FINAL_PREDICTIONS_PATH / target_trial_recording.uuid\n",
    "    if predictions_path.exists():\n",
    "        shutil.rmtree(predictions_path)\n",
    "    predictions_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    predictions_df.to_csv(predictions_path / f\"predictions_k={k}_samples={num_samples}.csv\", index=False)\n",
    "\n",
    "    gt_df = ground_truth_df[\n",
    "        ground_truth_df[\"recording_uuid\"] == target_trial_recording.uuid\n",
    "    ].copy()\n",
    "\n",
    "    merged_df, metrics = compare_predictions_to_ground_truth(\n",
    "        predictions_df, gt_df, ignore_unknown=True\n",
    "    )\n",
    "\n",
    "    print(f\"Metrics for {grounding_dataset_path.stem}:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35b14d12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2fe01600-c057-40ee-8434-4e9e0688ca2d'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_trial_recording.uuid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
