{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a111f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 23:00:58.684474: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-06 23:00:58.697273: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743973258.707583  291001 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743973258.710848  291001 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 23:00:58.725218: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.api.controllers.generate_embeddings as generate_embeddings\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from sqlalchemy.orm import Session\n",
    "from src.config import GAZE_FOVEA_FOV, TOBII_FOV_X\n",
    "from src.db import engine\n",
    "from src.db.models import Recording, SimRoomClass\n",
    "from src.api.controllers.gaze_segmentation import (\n",
    "    get_gaze_points,\n",
    "    match_frames_to_gaze,\n",
    "    parse_gazedata_file,\n",
    "    mask_was_viewed\n",
    ")\n",
    "from src.utils import cv2_video_fps, cv2_video_frame_count, cv2_video_resolution\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from ultralytics import FastSAM\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e84946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_metadata.json\") as file:\n",
    "    experiment_metadata = json.load(file)\n",
    "    trial_recordings_metadata = experiment_metadata[\"trial_recordings_metadata\"]\n",
    "    trial_recording_uuids = list(trial_recordings_metadata.keys())\n",
    "    labeling_same_background_uuid = experiment_metadata[\"labeling_same_background_uuid\"]\n",
    "    labeling_diff_background_uuid = experiment_metadata[\"labeling_diff_background_uuid\"]\n",
    "\n",
    "with Session(engine) as session:\n",
    "    trial_recordings = (\n",
    "        session.query(Recording).filter(Recording.uuid.in_(trial_recording_uuids)).all()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5231553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2 = generate_embeddings.load_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97f7806",
   "metadata": {},
   "outputs": [],
   "source": [
    "GAZE_SEGMENTATION_RESULTS_PATH = Path(\"data/gaze_segmentation_results\")\n",
    "\n",
    "def process_recording(recording: Recording):\n",
    "    recording_uuid = recording.uuid\n",
    "    gaze_segmentation_results_path = GAZE_SEGMENTATION_RESULTS_PATH / recording_uuid\n",
    "    \n",
    "    gaze_segmentation_results = list(gaze_segmentation_results_path.iterdir())\n",
    "    gaze_segmentation_results.sort(key=lambda x: int(x.stem))\n",
    "    gaze_segmentation_results = [\n",
    "        np.load(result, allow_pickle=True) for result in gaze_segmentation_results\n",
    "    ]\n",
    "\n",
    "    grounding_stats = pd.DataFrame(\n",
    "        columns=[\n",
    "            \"frame_idx\",\n",
    "            \"object_id\",\n",
    "            \"class_id\",\n",
    "            \"avg_distance\",\n",
    "            \"min_distance\",\n",
    "            \"max_distance\",\n",
    "            \"var_distance\",\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    result_rows = []\n",
    "    for result in gaze_segmentation_results:\n",
    "        frame_idx = result[\"frame_idx\"]\n",
    "        rois = result[\"rois\"]\n",
    "        object_ids = result[\"object_ids\"]\n",
    "\n",
    "        # Get embeddings (assuming one batch is returned)\n",
    "        embeddings, _, _ = list(generate_embeddings.get_embeddings(dinov2, rois))[0]\n",
    "        per_roi_distances, per_roi_class_ids = generate_embeddings.search_index(\n",
    "            index, embeddings, k=50\n",
    "        )\n",
    "\n",
    "        for i, roi in enumerate(rois):  # iterate over each ROI\n",
    "            object_id = object_ids[i]\n",
    "            distances = per_roi_distances[i]\n",
    "            class_ids = per_roi_class_ids[i]\n",
    "\n",
    "            # Group distances by class using defaultdict for conciseness\n",
    "            class_to_dists = defaultdict(list)\n",
    "            for cid, d in zip(class_ids, distances, strict=False):\n",
    "                class_to_dists[cid].append(d)\n",
    "\n",
    "            # For each class, compute statistics and add a row\n",
    "            for cid, dists in class_to_dists.items():\n",
    "                rows.append({\n",
    "                    \"frame_idx\": frame_idx,\n",
    "                    \"object_id\": object_id,\n",
    "                    \"class_id\": cid,\n",
    "                    \"avg_distance\": np.mean(dists),\n",
    "                    \"min_distance\": np.min(dists),\n",
    "                    \"max_distance\": np.max(dists),\n",
    "                    \"var_distance\": np.var(dists),\n",
    "                })\n",
    "\n",
    "    grounding_stats = pd.DataFrame(rows)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
