{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a111f936",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 00:08:14.914218: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-09 00:08:12.505071: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1744150092.568406  163761 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1744150092.585723  163761 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-09 00:08:12.736616: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.48, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    }
   ],
   "source": [
    "from controlled_experiment.settings import FULLY_LABELED_RECORDINGS\n",
    "\n",
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "from collections import defaultdict\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import src.api.controllers.generate_embeddings as generate_embeddings\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from sqlalchemy.orm import Session\n",
    "from src.config import GAZE_FOVEA_FOV, TOBII_FOV_X\n",
    "from src.db import engine\n",
    "from src.db.models import Recording, SimRoomClass\n",
    "from src.api.controllers.gaze_segmentation import (\n",
    "    get_gaze_points,\n",
    "    match_frames_to_gaze,\n",
    "    parse_gazedata_file,\n",
    "    mask_was_viewed\n",
    ")\n",
    "from src.utils import cv2_video_fps, cv2_video_frame_count, cv2_video_resolution\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from ultralytics import FastSAM\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9e84946b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_metadata.json\") as file:\n",
    "    experiment_metadata = json.load(file)\n",
    "    trial_recordings_metadata = experiment_metadata[\"trial_recordings_metadata\"]\n",
    "    trial_recording_uuids = list(trial_recordings_metadata.keys())\n",
    "    labeling_same_background_uuid = experiment_metadata[\"labeling_same_background_uuid\"]\n",
    "    labeling_diff_background_uuid = experiment_metadata[\"labeling_diff_background_uuid\"]\n",
    "\n",
    "with Session(engine) as session:\n",
    "    trial_recordings = (\n",
    "        session.query(Recording).filter(Recording.uuid.in_(trial_recording_uuids)).all()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5231553a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dinov2 = generate_embeddings.load_model()\n",
    "GAZE_SEGMENTATION_RESULTS_PATH = Path(\"data/gaze_segmentation_results\")\n",
    "SAME_BACKGROUND_VECTOR_INDEXES_PATH = Path(\"data/vector_indexes/same_background\")\n",
    "DIFF_BACKGROUND_VECTOR_INDEXES_PATH = Path(\"data/vector_indexes/diff_background\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37331195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grounding_dataset(dataset_path: Path, gaze_segmentation_results: list[Any], index: faiss.IndexIDMap, k: int):\n",
    "    \"\"\"\n",
    "    Create a grounding dataset CSV file with raw candidate distances for each object in each frame.\n",
    "\n",
    "    For each gaze segmentation result:\n",
    "      - Retrieves the frame index, regions of interest (rois), and associated object IDs.\n",
    "      - Computes embeddings for each ROI and searches the provided index to get k candidate matches.\n",
    "      - For each ROI, iterates over each candidate by pairing the candidate’s class ID with its raw distance.\n",
    "      - Writes each candidate as an individual row in the output DataFrame with the following columns:\n",
    "          \"frame_idx\", \"object_id\", \"class_id\", and \"distance\".\n",
    "    \n",
    "    Parameters:\n",
    "        dataset_path (Path): The file path where the CSV dataset will be saved.\n",
    "        gaze_segmentation_results (list[Any]): List containing gaze segmentation results; each element\n",
    "                                                 must have \"frame_idx\", \"rois\", and \"object_ids\".\n",
    "        index (faiss.IndexIDMap): A FAISS index (wrapped in an IndexIDMap) used to retrieve candidates.\n",
    "        k (int): The number of nearest neighbors (candidates) to retrieve.\n",
    "    \n",
    "    Returns:\n",
    "        None. The resulting DataFrame is saved to a CSV file at dataset_path.\n",
    "    \"\"\"\n",
    "    result_rows = []\n",
    "    for result in gaze_segmentation_results:\n",
    "        frame_idx = result[\"frame_idx\"]\n",
    "        rois = result[\"rois\"]\n",
    "        object_ids = result[\"object_ids\"]\n",
    "\n",
    "        # Get embeddings (assuming one batch is returned)\n",
    "        embeddings, _, _ = list(generate_embeddings.get_embeddings(dinov2, rois))[0]\n",
    "        per_roi_distances, per_roi_class_ids = generate_embeddings.search_index(\n",
    "            index, embeddings, k=k\n",
    "        )\n",
    "\n",
    "        for i, roi in enumerate(rois):  # iterate over each ROI in the frame\n",
    "            object_id = object_ids[i]\n",
    "            distances = per_roi_distances[i]\n",
    "            class_ids = per_roi_class_ids[i]\n",
    "\n",
    "            # Write each candidate (class id and corresponding raw distance) as a separate row.\n",
    "            for cid, d in zip(class_ids, distances, strict=False):\n",
    "                result_rows.append({\n",
    "                    \"frame_idx\": frame_idx,\n",
    "                    \"object_id\": object_id,\n",
    "                    \"class_id\": cid,\n",
    "                    \"distance\": d,\n",
    "                })\n",
    "\n",
    "    pd.DataFrame(result_rows).to_csv(dataset_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f97f7806",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trial recordings:   0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trial recordings:  14%|█▍        | 2/14 [02:15<13:30, 67.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating grounding dataset for sample_count=500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing trial recordings: 100%|██████████| 14/14 [05:14<00:00, 22.45s/it]\n"
     ]
    }
   ],
   "source": [
    "GROUNDING_DATASETS_PATH = Path(\"data/grounding_datasets\")\n",
    "\n",
    "for trial_recording in tqdm(trial_recordings, desc=\"Processing trial recordings\"):\n",
    "    if trial_recording.uuid not in FULLY_LABELED_RECORDINGS:\n",
    "        continue\n",
    "\n",
    "    # Load gaze segmentation results for this recording\n",
    "    gaze_segmentation_results_path = GAZE_SEGMENTATION_RESULTS_PATH / trial_recording.uuid\n",
    "    gaze_segmentation_results = list(gaze_segmentation_results_path.iterdir())\n",
    "    gaze_segmentation_results.sort(key=lambda x: int(x.stem))\n",
    "    gaze_segmentation_results = [\n",
    "        np.load(result, allow_pickle=True) for result in gaze_segmentation_results\n",
    "    ]\n",
    "\n",
    "    # Create the grounding datasets directory for this recording\n",
    "    grounding_datasets_path = GROUNDING_DATASETS_PATH / trial_recording.uuid\n",
    "    if grounding_datasets_path.exists():\n",
    "        shutil.rmtree(grounding_datasets_path)\n",
    "    grounding_datasets_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create grounding datasets for each index and for each k value\n",
    "    vector_index_paths = list(SAME_BACKGROUND_VECTOR_INDEXES_PATH.iterdir())\n",
    "    for vector_index_path in tqdm(vector_index_paths, desc=\"Processing vector indexes\", leave=False):\n",
    "        sample_count = int(vector_index_path.name.split(\"_\")[0])\n",
    "        index = faiss.read_index(str(vector_index_path))\n",
    "\n",
    "        print(f\"Creating grounding dataset for sample_count={sample_count}\")\n",
    "        grounding_dataset_path = grounding_datasets_path / f\"grounding_dataset_{sample_count}_samples.csv\"\n",
    "        create_grounding_dataset(\n",
    "            dataset_path=grounding_dataset_path,\n",
    "            gaze_segmentation_results=gaze_segmentation_results,\n",
    "            index=index,\n",
    "            k=sample_count,\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
