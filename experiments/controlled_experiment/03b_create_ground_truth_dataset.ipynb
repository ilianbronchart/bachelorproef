{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9cd7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from src.api.controllers.calibration_recording_controller import (\n",
    "    AnnotatedClassResponse,\n",
    "    get_annotated_classes,\n",
    "    get_calibration_recording_by_uuid,\n",
    "    get_gaze_data_path,\n",
    "    get_recording_path,\n",
    ")\n",
    "from src.api.controllers.gaze_segmentation import (\n",
    "    get_gaze_point_per_frame,\n",
    "    mask_was_viewed,\n",
    ")\n",
    "from src.config import TOBII_GLASSES_FPS, VIEWED_RADIUS\n",
    "from src.logic.glasses.domain import GazePoint\n",
    "from src.utils import (\n",
    "    cv2_video_fps,\n",
    "    cv2_video_frame_count,\n",
    "    cv2_video_resolution,\n",
    "    draw_annotation_on_frame,\n",
    "    extract_frames_to_dir,\n",
    ")\n",
    "from tqdm import tqdm\n",
    "\n",
    "from controlled_experiment.settings import FULLY_LABELED_RECORDINGS, TRIAL_RECORDING_UUIDS, LABELING_VALIDATION_VIDEOS_PATH, GROUND_TRUTH_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0d202",
   "metadata": {},
   "source": [
    "# Create the ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b4fb15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mask', 'box', 'roi', 'class_id', 'frame_idx']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.load(\n",
    "        \"/home/zilian/projects/bachelorproef/experiments/controlled_experiment/data/labeling_results/2/1/0.npz\"\n",
    "    ).files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58422dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viewed_annotations_per_frame(\n",
    "    annotated_classes: list[AnnotatedClassResponse],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    video_resolution: tuple[int, int],\n",
    "):\n",
    "    # Gather all annotation paths for each annotated frame\n",
    "    annotations_per_frame: dict[int, list[Path]] = {}\n",
    "    for anno_class in annotated_classes:\n",
    "        for annotation_path in anno_class.annotation_paths:\n",
    "            frame_idx = int(annotation_path.stem)\n",
    "\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            mask = annotation_file[\"mask\"]\n",
    "            x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "\n",
    "            # Put the mask in a tensor of the same size as the video frame\n",
    "            mask_full = np.zeros(video_resolution, dtype=np.uint8)\n",
    "            mask_full[y1:y2, x1:x2] = mask\n",
    "            mask_full_torch = torch.from_numpy(mask_full)\n",
    "\n",
    "            gaze_point = gaze_point_per_frame.get(frame_idx)\n",
    "            if gaze_point is None:\n",
    "                continue\n",
    "\n",
    "            if mask_was_viewed(mask_full_torch, gaze_point.position):\n",
    "                if frame_idx not in annotations_per_frame:\n",
    "                    annotations_per_frame[frame_idx] = []\n",
    "\n",
    "                annotations_per_frame[frame_idx].append(annotation_path)\n",
    "\n",
    "    return annotations_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4acf39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_validation_video_frames(\n",
    "    frames: list[Path],\n",
    "    annotations_per_frame: dict[int, list[Path]],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    annotated_classes: list[AnnotatedClassResponse],\n",
    "):\n",
    "    class_id_to_annotated_class = {\n",
    "        anno_class.id: anno_class for anno_class in annotated_classes\n",
    "    }\n",
    "\n",
    "    # Iterate over frames and draw the annotations on them if they exist\n",
    "    for frame in tqdm(frames, desc=\"Drawing annotations on frames\"):\n",
    "        frame_idx = int(frame.stem)\n",
    "        frame_img = cv2.imread(str(frame))\n",
    "\n",
    "        if annotations_per_frame.get(frame_idx) is not None:\n",
    "            for annotation_path in annotations_per_frame[frame_idx]:\n",
    "                annotation_file = np.load(annotation_path)\n",
    "                class_id = int(annotation_file[\"class_id\"])\n",
    "                x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "                mask = annotation_file[\"mask\"]\n",
    "\n",
    "                # Squeeze mask if it has an extra dimension\n",
    "                if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "                    mask = mask[0]\n",
    "                if mask.dtype != bool:\n",
    "                    mask = mask.astype(bool)\n",
    "\n",
    "                class_color_hex = class_id_to_annotated_class[class_id].color\n",
    "                class_name = class_id_to_annotated_class[class_id].class_name\n",
    "                box = (x1, y1, x2, y2)\n",
    "\n",
    "                # Annotate the frame using the reusable function\n",
    "                frame_img = draw_annotation_on_frame(\n",
    "                    frame_img, mask, box, class_color_hex, class_name\n",
    "                )\n",
    "\n",
    "        # Draw the gaze point on the frame\n",
    "        gaze_point = gaze_point_per_frame.get(frame_idx)\n",
    "        if gaze_point is not None:\n",
    "            gaze_x, gaze_y = gaze_point.position\n",
    "            cv2.circle(\n",
    "                frame_img,\n",
    "                (int(gaze_x), int(gaze_y)),\n",
    "                radius=VIEWED_RADIUS,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=2,\n",
    "            )\n",
    "\n",
    "        # Save the modified image back to its original location\n",
    "        cv2.imwrite(str(frame), frame_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564f9814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Getting viewed annotations for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Building ground truth for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [01:49<23:37, 109.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Getting viewed annotations for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Building ground truth for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [03:38<21:48, 109.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Getting viewed annotations for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Building ground truth for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [05:02<00:00, 21.58s/it]\n"
     ]
    }
   ],
   "source": [
    "if not LABELING_VALIDATION_VIDEOS_PATH.exists():\n",
    "    os.makedirs(LABELING_VALIDATION_VIDEOS_PATH)\n",
    "else:\n",
    "    for file in LABELING_VALIDATION_VIDEOS_PATH.glob(\"*.mp4\"):\n",
    "        # os.remove(file)\n",
    "        pass\n",
    "\n",
    "if GROUND_TRUTH_PATH.exists():\n",
    "    GROUND_TRUTH_PATH.unlink()\n",
    "\n",
    "CREATE_VALIDATION_VIDEO = False\n",
    "gt_rows = []\n",
    "for recording_uuid in tqdm(TRIAL_RECORDING_UUIDS):\n",
    "    if recording_uuid not in FULLY_LABELED_RECORDINGS:\n",
    "        continue\n",
    "\n",
    "    calibration_recording = get_calibration_recording_by_uuid(recording_uuid)\n",
    "    annotated_classes = get_annotated_classes(calibration_recording.id)\n",
    "\n",
    "    # Get statistics of the video\n",
    "    trial_recording_path = get_recording_path(calibration_recording.id)\n",
    "    trial_video_resolution = cv2_video_resolution(trial_recording_path)\n",
    "    trial_video_fps = cv2_video_fps(trial_recording_path)\n",
    "    trial_video_frame_count = cv2_video_frame_count(trial_recording_path)\n",
    "\n",
    "    # Load and preprocess gaze points\n",
    "    print(f\"Loading gaze data for {recording_uuid}\")\n",
    "    gaze_data_path = get_gaze_data_path(calibration_recording.id)\n",
    "    gaze_point_per_frame = get_gaze_point_per_frame(\n",
    "        gaze_data_path=gaze_data_path,\n",
    "        resolution=trial_video_resolution,\n",
    "        frame_count=trial_video_frame_count,\n",
    "        fps=trial_video_fps,\n",
    "    )\n",
    "\n",
    "    # Get all annotations that were viewed\n",
    "    print(f\"Getting viewed annotations for {recording_uuid}\")\n",
    "    annotations_per_frame = get_viewed_annotations_per_frame(\n",
    "        annotated_classes=annotated_classes,\n",
    "        gaze_point_per_frame=gaze_point_per_frame,\n",
    "        video_resolution=trial_video_resolution,\n",
    "    )\n",
    "\n",
    "    # Build the ground truth DataFrame\n",
    "    # TODO: Might be interesting to add blur metric per frame to the ground truth dataset\n",
    "    print(f\"Building ground truth for {recording_uuid}\")\n",
    "    for frame_idx, annotation_paths in annotations_per_frame.items():\n",
    "        for annotation_path in annotation_paths:\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            class_id = int(annotation_file[\"class_id\"])\n",
    "            mask_area = np.sum(annotation_file[\"mask\"])\n",
    "            x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "            roi = annotation_file[\"roi\"]\n",
    "\n",
    "            laplacian_variance = cv2.Laplacian(roi, cv2.CV_64F).var()\n",
    "\n",
    "            gt_rows.append({\n",
    "                \"recording_uuid\": recording_uuid,\n",
    "                \"frame_idx\": frame_idx,\n",
    "                \"class_id\": class_id,\n",
    "                \"mask_area\": mask_area,\n",
    "                \"laplacian_variance\": laplacian_variance,\n",
    "                \"x1\": x1,\n",
    "                \"y1\": y1,\n",
    "                \"x2\": x2,\n",
    "                \"y2\": y2,\n",
    "            })\n",
    "\n",
    "    if CREATE_VALIDATION_VIDEO:\n",
    "        # Extract frames from the video and save them to a temporary directory\n",
    "        print(f\"Extracting frames for {recording_uuid}\")\n",
    "        tmp_frames_dir = tempfile.TemporaryDirectory()\n",
    "        tmp_frames_path = Path(tmp_frames_dir.name)\n",
    "        extract_frames_to_dir(\n",
    "            video_path=trial_recording_path,\n",
    "            frames_path=tmp_frames_path,\n",
    "            print_output=False,\n",
    "        )\n",
    "        frames = sorted(list(tmp_frames_path.glob(\"*.jpg\")), key=lambda x: int(x.stem))\n",
    "\n",
    "        print(f\"Drawing annotations for {recording_uuid}\")\n",
    "        draw_validation_video_frames(\n",
    "            frames=frames,\n",
    "            annotations_per_frame=annotations_per_frame,\n",
    "            gaze_point_per_frame=gaze_point_per_frame,\n",
    "            annotated_classes=annotated_classes,\n",
    "        )\n",
    "\n",
    "        print(f\"Creating video for {recording_uuid}\")\n",
    "        cmd = f'ffmpeg -hwaccel cuda -y -pattern_type glob -framerate {TOBII_GLASSES_FPS} -i \"{tmp_frames_path!s}/*.jpg\" -c:v libx264 -pix_fmt yuv420p \"{LABELING_VALIDATION_VIDEOS_PATH}/{recording_uuid}.mp4\"'\n",
    "        subprocess.run(\n",
    "            cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "\n",
    "ground_truth_df = pd.DataFrame(gt_rows)\n",
    "ground_truth_df.to_csv(\n",
    "    GROUND_TRUTH_PATH,\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
