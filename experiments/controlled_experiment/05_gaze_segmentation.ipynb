{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import shutil\n",
    "import traceback\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms.functional as F\n",
    "from sqlalchemy.orm import Session\n",
    "from src.api.controllers.gaze_segmentation import (\n",
    "    get_gaze_points,\n",
    "    mask_was_viewed,\n",
    "    match_frames_to_gaze,\n",
    "    parse_gazedata_file,\n",
    ")\n",
    "from src.config import CHECKPOINTS_PATH, GAZE_FOVEA_FOV, TOBII_FOV_X\n",
    "from src.db import engine\n",
    "from src.db.models import Recording\n",
    "from src.utils import cv2_video_fps, cv2_video_frame_count, cv2_video_resolution\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from torchvision.transforms import InterpolationMode\n",
    "from tqdm import tqdm\n",
    "from ultralytics import FastSAM\n",
    "\n",
    "from controlled_experiment.settings import FULLY_LABELED_RECORDINGS, GAZE_SEGMENTATION_RESULTS_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_metadata.json\") as file:\n",
    "    experiment_metadata = json.load(file)\n",
    "    trial_recordings_metadata = experiment_metadata[\"trial_recordings_metadata\"]\n",
    "    trial_recording_uuids = list(trial_recordings_metadata.keys())\n",
    "    labeling_same_background_uuid = experiment_metadata[\"labeling_same_background_uuid\"]\n",
    "    labeling_diff_background_uuid = experiment_metadata[\"labeling_diff_background_uuid\"]\n",
    "\n",
    "with Session(engine) as session:\n",
    "    trial_recordings = (\n",
    "        session.query(Recording).filter(Recording.uuid.in_(trial_recording_uuids)).all()\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmenting and Tracking based on Gaze Data, and grounding based on previously built Vector Index\n",
    "\n",
    "There's a few considerations that might be interesting in an experimental context:\n",
    "1. Selection of `k` in top-k results from the database?\n",
    "2. Segmentation quality (IOU?, Confidence?)\n",
    "3. Adding padding to the bounding boxes?\n",
    "4. Indexing, search parameters? (which ones exist)\n",
    "5. Merging of same-frame ROIs or not?\n",
    "6. Importance of metrics (average, min, max, variance, ?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeSegmentationJob:\n",
    "    def __init__(\n",
    "        self,\n",
    "        video_path: Path,\n",
    "        gaze_data_path: Path,\n",
    "        results_path: Path,\n",
    "        fovea_fov: float = GAZE_FOVEA_FOV,\n",
    "        fov_x: float = TOBII_FOV_X,\n",
    "        checkpoint_path: str = \"checkpoints/FastSAM-x.pt\",\n",
    "        output_video_path: Path | None = None,\n",
    "    ):\n",
    "        self.video_path = video_path\n",
    "        self.gaze_data_path = gaze_data_path\n",
    "        self.fovea_fov = fovea_fov\n",
    "        self.fov_x = fov_x\n",
    "\n",
    "        # Set up the results directory.\n",
    "        self.results_path = results_path\n",
    "        if self.results_path.exists():\n",
    "            shutil.rmtree(self.results_path, ignore_errors=True)\n",
    "            self.results_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Load the FastSAM model.\n",
    "        self.model = FastSAM(checkpoint_path)\n",
    "\n",
    "        # Video properties.\n",
    "        self.resolution = cv2_video_resolution(self.video_path)\n",
    "        self.aspect_ratio = self.resolution[1] / self.resolution[0]  # W / H\n",
    "        self.fps = cv2_video_fps(self.video_path)\n",
    "        self.viewed_radius = int((self.fovea_fov / self.fov_x) * self.resolution[1])\n",
    "        self.frame_count = cv2_video_frame_count(self.video_path)\n",
    "\n",
    "        # Set up the output video.\n",
    "        if output_video_path is not None:\n",
    "            self.video_result = cv2.VideoWriter(\n",
    "                str(output_video_path),\n",
    "                cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "                self.fps,\n",
    "                (self.resolution[1], self.resolution[0]),\n",
    "            )\n",
    "        else:\n",
    "            self.video_result = None\n",
    "\n",
    "        # Parse gaze data.\n",
    "        self.gaze_data = parse_gazedata_file(self.gaze_data_path)\n",
    "        self.gaze_points = get_gaze_points(self.gaze_data, self.resolution)\n",
    "\n",
    "        # Map frame indexes to gaze points.\n",
    "        self.frame_gaze_mapping = match_frames_to_gaze(\n",
    "            self.frame_count, self.gaze_points, self.fps\n",
    "        )\n",
    "\n",
    "    def get_gaze_position(self, frame_idx: int) -> tuple[int, int] | None:\n",
    "        \"\"\"\n",
    "        Get the gaze position for a frame index.\n",
    "        \"\"\"\n",
    "        gaze_points = self.frame_gaze_mapping[frame_idx]\n",
    "        if len(gaze_points) == 0:\n",
    "            return None\n",
    "        return gaze_points[0].position\n",
    "\n",
    "    def mask_too_large(self, mask: torch.Tensor) -> bool:\n",
    "        \"\"\"\n",
    "        Check if the mask area is less than or equal to 30% of the frame area.\n",
    "\n",
    "        Args:\n",
    "            mask: A tensor containing a single mask of shape (H, W)\n",
    "\n",
    "        Returns:\n",
    "            bool: True if the mask's area is less than or equal to 30% of the frame area, False otherwise.\n",
    "        \"\"\"\n",
    "        height, width = mask.shape\n",
    "        frame_area = height * width\n",
    "        max_mask_area = 0.1 * frame_area\n",
    "\n",
    "        mask_area = mask.sum()\n",
    "        return mask_area >= max_mask_area\n",
    "\n",
    "    def run(self):\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            for frame_idx, results in enumerate(\n",
    "                self.model.track(\n",
    "                    source=str(self.video_path), imgsz=640, stream=True, verbose=False\n",
    "                )\n",
    "            ):\n",
    "                try:\n",
    "                    gaze_position = self.get_gaze_position(frame_idx)\n",
    "                    if gaze_position is None:\n",
    "                        continue\n",
    "\n",
    "                    boxes = []\n",
    "                    rois = []\n",
    "                    masks = []\n",
    "                    object_ids = []\n",
    "                    confidences = []\n",
    "                    for result in results:\n",
    "                        confidences.append(float(result.boxes[0].conf))\n",
    "\n",
    "                        mask = F.resize(\n",
    "                            result.masks[0].data,\n",
    "                            self.resolution,\n",
    "                            interpolation=InterpolationMode.NEAREST,\n",
    "                        ).squeeze()\n",
    "\n",
    "                        if not self.mask_too_large(mask) and mask_was_viewed(\n",
    "                            mask, gaze_position\n",
    "                        ):\n",
    "                            box = masks_to_boxes(mask.unsqueeze(0)).int().cpu().numpy()[0]\n",
    "                            x1, y1, x2, y2 = box\n",
    "                            roi = results[0].orig_img[y1:y2, x1:x2, :]\n",
    "                            roi = cv2.cvtColor(roi, cv2.COLOR_BGR2RGB)\n",
    "                            boxes.append(box)\n",
    "                            masks.append(mask.cpu().numpy().astype(np.uint8))\n",
    "\n",
    "                            rois.append(roi)\n",
    "                            object_ids.append(int(result.boxes.id[0]))\n",
    "\n",
    "                    if len(boxes) > 0:\n",
    "                        # Offload saving with thread pool (asynchronously)\n",
    "                        rois_array = np.empty(len(rois), dtype=object)\n",
    "                        for i, roi in enumerate(rois):\n",
    "                            rois_array[i] = roi\n",
    "\n",
    "                        masks_array = np.empty(len(masks), dtype=object)\n",
    "                        for i, mask in enumerate(masks):\n",
    "                            masks_array[i] = mask\n",
    "\n",
    "                        executor.submit(\n",
    "                            np.savez_compressed,\n",
    "                            self.results_path / f\"{frame_idx}.npz\",\n",
    "                            boxes=boxes,\n",
    "                            rois=rois_array,\n",
    "                            masks=masks_array,\n",
    "                            object_ids=object_ids,\n",
    "                            frame_idx=frame_idx,\n",
    "                            gaze_position=gaze_position,\n",
    "                            confidences=confidences,\n",
    "                        )\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "                    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing recordings: 100%|██████████| 14/14 [08:05<00:00, 34.70s/it]\n"
     ]
    }
   ],
   "source": [
    "if GAZE_SEGMENTATION_RESULTS_PATH.exists():\n",
    "    shutil.rmtree(GAZE_SEGMENTATION_RESULTS_PATH, ignore_errors=True)\n",
    "GAZE_SEGMENTATION_RESULTS_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def process_recording(recording: Recording):\n",
    "    \"\"\"\n",
    "    Process a recording for gaze segmentation.\n",
    "    \"\"\"\n",
    "    recording_uuid = recording.uuid\n",
    "    video_path = Path(recording.video_path)\n",
    "    gaze_data_path = Path(recording.gaze_data_path)\n",
    "    results_path = GAZE_SEGMENTATION_RESULTS_PATH / recording_uuid\n",
    "\n",
    "    if results_path.exists():\n",
    "        shutil.rmtree(results_path, ignore_errors=True)\n",
    "    results_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    job = GazeSegmentationJob(\n",
    "        video_path=video_path,\n",
    "        gaze_data_path=gaze_data_path,\n",
    "        results_path=results_path,\n",
    "        fovea_fov=GAZE_FOVEA_FOV,\n",
    "        fov_x=TOBII_FOV_X,\n",
    "        checkpoint_path=CHECKPOINTS_PATH / \"FastSAM-x.pt\",\n",
    "    )\n",
    "    job.run()\n",
    "\n",
    "\n",
    "for recording in tqdm(trial_recordings, desc=\"Processing recordings\"):\n",
    "    if recording.uuid in FULLY_LABELED_RECORDINGS:\n",
    "        process_recording(recording)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
