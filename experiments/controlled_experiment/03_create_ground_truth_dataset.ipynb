{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cd7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sqlalchemy.orm import Session\n",
    "from src.api.controllers.calibration_recording_controller import (\n",
    "    AnnotatedClassResponse,\n",
    "    get_annotated_classes,\n",
    "    get_recording_path,\n",
    "    get_gaze_data_path\n",
    ")\n",
    "from src.config import TOBII_GLASSES_FPS\n",
    "from src.db import engine\n",
    "from src.db.models import CalibrationRecording\n",
    "from src.utils import (\n",
    "    cv2_video_resolution,\n",
    "    draw_annotation_on_frame,\n",
    "    extract_frames_to_dir,\n",
    "    cv2_video_fps,\n",
    "    cv2_video_frame_count\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from src.api.controllers.gaze_segmentation import mask_was_viewed, parse_gazedata_file, get_gaze_points, match_frames_to_gaze, get_gaze_point_per_frame\n",
    "from src.logic.glasses.domain import GazePoint\n",
    "import pandas as pd\n",
    "from src.config import VIEWED_RADIUS\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22a0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_metadata.json\") as file:\n",
    "    experiment_metadata = json.load(file)\n",
    "    trial_recordings_metadata = experiment_metadata[\"trial_recordings_metadata\"]\n",
    "    trial_recording_uuids = list(trial_recordings_metadata.keys())\n",
    "    labeling_same_background_uuid = experiment_metadata[\"labeling_same_background_uuid\"]\n",
    "    labeling_diff_background_uuid = experiment_metadata[\"labeling_diff_background_uuid\"]\n",
    "\n",
    "with Session(engine) as session:\n",
    "    calibration_recordings = session.query(CalibrationRecording).all()\n",
    "\n",
    "    trial_recordings = {\n",
    "        cr.recording_uuid: cr\n",
    "        for cr in calibration_recordings\n",
    "        if cr.recording_uuid in trial_recording_uuids\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37577c44",
   "metadata": {},
   "source": [
    "# Validate Labeling Data\n",
    "\n",
    "Here, we will validate the labeling data so that we can be sure that the data is correct and that the labels are correct. We will also check for any missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1772e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording_uuid, trial_metadata in trial_recordings_metadata.items():\n",
    "    calibration_recording = trial_recordings[recording_uuid]\n",
    "    annotated_classes = get_annotated_classes(calibration_recording.id)\n",
    "    class_name_to_annotation_paths = {\n",
    "        anno_class.class_name: anno_class.annotation_paths\n",
    "        for anno_class in annotated_classes\n",
    "    }\n",
    "    trial_objects_metadata = trial_metadata[\"objects\"]\n",
    "\n",
    "    assert len(trial_objects_metadata) == 5, \"Number of objects in trial is not 5\"\n",
    "    assert len(annotated_classes) == 5, \"Number of annotated classes is not 5\"\n",
    "\n",
    "    for object_metadata in trial_objects_metadata:\n",
    "        class_name, _, _ = object_metadata\n",
    "\n",
    "        assert class_name in class_name_to_annotation_paths, (\n",
    "            \"Original object class name not in annotated classes\"\n",
    "        )\n",
    "\n",
    "        annotation_paths = class_name_to_annotation_paths[class_name]\n",
    "        assert len(annotation_paths) > 0, f\"Annotation paths for {class_name} are empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0d202",
   "metadata": {},
   "source": [
    "# Create the ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4fb15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mask', 'box', 'roi', 'class_id', 'frame_idx']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.load(\n",
    "        \"/home/zilian/projects/bachelorproef/experiments/controlled_experiment/data/labeling_results/2/1/0.npz\"\n",
    "    ).files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58422dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viewed_annotations_per_frame(\n",
    "    annotated_classes: list[AnnotatedClassResponse],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    video_resolution: tuple[int, int],\n",
    "):\n",
    "    # Gather all annotation paths for each annotated frame\n",
    "    annotations_per_frame: dict[int, list[Path]] = {}\n",
    "    for anno_class in annotated_classes:\n",
    "        for annotation_path in anno_class.annotation_paths:\n",
    "            frame_idx = int(annotation_path.stem)\n",
    "\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            mask = annotation_file[\"mask\"]\n",
    "            x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "\n",
    "            # Put the mask in a tensor of the same size as the video frame\n",
    "            mask_full = np.zeros(video_resolution, dtype=np.uint8)\n",
    "            mask_full[y1:y2, x1:x2] = mask\n",
    "            mask_full_torch = torch.from_numpy(mask_full)\n",
    "\n",
    "            gaze_point = gaze_point_per_frame.get(frame_idx, None)\n",
    "            if gaze_point is None:\n",
    "                continue\n",
    "\n",
    "            if mask_was_viewed(mask_full_torch, gaze_point.position):\n",
    "                if frame_idx not in annotations_per_frame:\n",
    "                    annotations_per_frame[frame_idx] = []\n",
    "\n",
    "                annotations_per_frame[frame_idx].append(annotation_path)\n",
    "    \n",
    "    return annotations_per_frame\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acf39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_validation_video_frames(\n",
    "    frames: list[Path],\n",
    "    annotations_per_frame: dict[int, list[Path]],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    annotated_classes : list[AnnotatedClassResponse],\n",
    "):\n",
    "    class_id_to_annotated_class = {\n",
    "        anno_class.id: anno_class for anno_class in annotated_classes\n",
    "    }\n",
    "\n",
    "    # Iterate over frames and draw the annotations on them if they exist\n",
    "    for frame in tqdm(frames, desc=\"Drawing annotations on frames\"):\n",
    "        frame_idx = int(frame.stem) \n",
    "        frame_img = cv2.imread(str(frame))\n",
    "        \n",
    "        if annotations_per_frame.get(frame_idx) is not None:\n",
    "            for annotation_path in annotations_per_frame[frame_idx]:\n",
    "                annotation_file = np.load(annotation_path)\n",
    "                class_id = int(annotation_file[\"class_id\"])\n",
    "                x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "                mask = annotation_file[\"mask\"]\n",
    "\n",
    "                # Squeeze mask if it has an extra dimension\n",
    "                if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "                    mask = mask[0]\n",
    "                if mask.dtype != bool:\n",
    "                    mask = mask.astype(bool)\n",
    "\n",
    "                class_color_hex = class_id_to_annotated_class[class_id].color\n",
    "                class_name = class_id_to_annotated_class[class_id].class_name\n",
    "                box = (x1, y1, x2, y2)\n",
    "\n",
    "                # Annotate the frame using the reusable function\n",
    "                frame_img = draw_annotation_on_frame(\n",
    "                    frame_img, mask, box, class_color_hex, class_name\n",
    "                )\n",
    "        \n",
    "        # Draw the gaze point on the frame\n",
    "        gaze_point = gaze_point_per_frame.get(frame_idx, None)\n",
    "        if gaze_point is not None:\n",
    "            gaze_x, gaze_y = gaze_point.position\n",
    "            cv2.circle(\n",
    "                frame_img,\n",
    "                (int(gaze_x), int(gaze_y)),\n",
    "                radius=VIEWED_RADIUS,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=2,\n",
    "            )\n",
    "\n",
    "        # Save the modified image back to its original location\n",
    "        cv2.imwrite(str(frame), frame_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564f9814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Getting viewed annotations for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Building ground truth for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Extracting frames for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Drawing annotations for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 2064/2064 [00:14<00:00, 144.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 1/14 [01:02<13:30, 62.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Getting viewed annotations for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Building ground truth for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Extracting frames for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n",
      "Drawing annotations for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1365/1365 [00:09<00:00, 142.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 32f02db7-adc0-4556-a2da-ed2ba60a58c9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 2/14 [01:43<10:01, 50.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Getting viewed annotations for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Building ground truth for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Extracting frames for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n",
      "Drawing annotations for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1557/1557 [00:11<00:00, 136.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for b8eeecc0-06b1-47f7-acb5-89aab3c1724d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██▏       | 3/14 [02:29<08:49, 48.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n",
      "Getting viewed annotations for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n",
      "Building ground truth for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n",
      "Extracting frames for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n",
      "Drawing annotations for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1500/1500 [00:10<00:00, 138.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for d50c5f3b-2822-4462-9880-5a8f0dd46bfb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|██▊       | 4/14 [03:18<08:05, 48.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n",
      "Getting viewed annotations for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n",
      "Building ground truth for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n",
      "Extracting frames for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n",
      "Drawing annotations for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1229/1229 [00:08<00:00, 141.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 9fa3e3b8-ed94-4b06-ba49-e66e3997d710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 5/14 [03:54<06:34, 43.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 98128cdc-ffeb-40cb-9528-573e25028e87\n",
      "Getting viewed annotations for 98128cdc-ffeb-40cb-9528-573e25028e87\n",
      "Building ground truth for 98128cdc-ffeb-40cb-9528-573e25028e87\n",
      "Extracting frames for 98128cdc-ffeb-40cb-9528-573e25028e87\n",
      "Drawing annotations for 98128cdc-ffeb-40cb-9528-573e25028e87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1543/1543 [00:08<00:00, 172.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 98128cdc-ffeb-40cb-9528-573e25028e87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 6/14 [04:39<05:54, 44.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n",
      "Getting viewed annotations for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n",
      "Building ground truth for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n",
      "Extracting frames for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n",
      "Drawing annotations for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1270/1270 [00:08<00:00, 143.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 89b60530-e0e4-4f5d-9ee6-af85c8d99ff4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 7/14 [05:17<04:56, 42.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n",
      "Getting viewed annotations for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n",
      "Building ground truth for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n",
      "Extracting frames for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n",
      "Drawing annotations for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 2041/2041 [00:13<00:00, 154.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 2fe01600-c057-40ee-8434-4e9e0688ca2d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 8/14 [06:21<04:54, 49.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n",
      "Getting viewed annotations for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n",
      "Building ground truth for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n",
      "Extracting frames for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n",
      "Drawing annotations for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1554/1554 [00:11<00:00, 135.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 67823ccd-a1f0-4cde-b954-3b9e5fe160c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 9/14 [07:09<04:03, 48.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for b214c60b-7521-495b-a699-e223da0c77c1\n",
      "Getting viewed annotations for b214c60b-7521-495b-a699-e223da0c77c1\n",
      "Building ground truth for b214c60b-7521-495b-a699-e223da0c77c1\n",
      "Extracting frames for b214c60b-7521-495b-a699-e223da0c77c1\n",
      "Drawing annotations for b214c60b-7521-495b-a699-e223da0c77c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1440/1440 [00:10<00:00, 139.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for b214c60b-7521-495b-a699-e223da0c77c1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████▏  | 10/14 [07:51<03:06, 46.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n",
      "Getting viewed annotations for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n",
      "Building ground truth for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n",
      "Extracting frames for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n",
      "Drawing annotations for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1364/1364 [00:09<00:00, 142.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for b8f453aa-5a12-4cbb-a0ec-20eb503f8797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▊  | 11/14 [08:27<02:09, 43.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 7ae61789-7a26-4c31-abef-4ab49a34abfd\n",
      "Getting viewed annotations for 7ae61789-7a26-4c31-abef-4ab49a34abfd\n",
      "Building ground truth for 7ae61789-7a26-4c31-abef-4ab49a34abfd\n",
      "Extracting frames for 7ae61789-7a26-4c31-abef-4ab49a34abfd\n",
      "Drawing annotations for 7ae61789-7a26-4c31-abef-4ab49a34abfd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1358/1358 [00:09<00:00, 143.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 7ae61789-7a26-4c31-abef-4ab49a34abfd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 12/14 [09:07<01:24, 42.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 6f3e2ccf-51f6-4377-8b84-63a3c16928a8\n",
      "Getting viewed annotations for 6f3e2ccf-51f6-4377-8b84-63a3c16928a8\n",
      "Building ground truth for 6f3e2ccf-51f6-4377-8b84-63a3c16928a8\n",
      "Extracting frames for 6f3e2ccf-51f6-4377-8b84-63a3c16928a8\n",
      "Drawing annotations for 6f3e2ccf-51f6-4377-8b84-63a3c16928a8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1458/1458 [00:08<00:00, 167.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 6f3e2ccf-51f6-4377-8b84-63a3c16928a8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 13/14 [09:51<00:43, 43.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 5235be94-da01-43b5-8827-92a51d32ce30\n",
      "Getting viewed annotations for 5235be94-da01-43b5-8827-92a51d32ce30\n",
      "Building ground truth for 5235be94-da01-43b5-8827-92a51d32ce30\n",
      "Extracting frames for 5235be94-da01-43b5-8827-92a51d32ce30\n",
      "Drawing annotations for 5235be94-da01-43b5-8827-92a51d32ce30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 1368/1368 [00:10<00:00, 132.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 5235be94-da01-43b5-8827-92a51d32ce30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [10:31<00:00, 45.14s/it]\n"
     ]
    }
   ],
   "source": [
    "LABELING_VALIDATION_VIDEOS_PATH = Path(\"data/labeling_validation_videos\")\n",
    "if not LABELING_VALIDATION_VIDEOS_PATH.exists():\n",
    "    os.makedirs(LABELING_VALIDATION_VIDEOS_PATH)\n",
    "else:\n",
    "    for file in LABELING_VALIDATION_VIDEOS_PATH.glob(\"*.mp4\"):\n",
    "        os.remove(file)\n",
    "\n",
    "CREATE_VALIDATION_VIDEO = True\n",
    "\n",
    "GROUND_TRUTH_PATH = Path(\"data/ground_truth.csv\")\n",
    "if GROUND_TRUTH_PATH.exists():\n",
    "    GROUND_TRUTH_PATH.unlink()\n",
    "ground_truth_df = pd.DataFrame(\n",
    "    columns=[\"recording_uuid\", \"frame_idx\", \"class_id\", \"mask_area\"]\n",
    ")\n",
    "\n",
    "for recording_uuid, trial_metadata in tqdm(trial_recordings_metadata.items()):\n",
    "    calibration_recording = trial_recordings[recording_uuid]\n",
    "    annotated_classes = get_annotated_classes(calibration_recording.id)\n",
    "\n",
    "    # Get statistics of the video\n",
    "    trial_recording_path = get_recording_path(calibration_recording.id)\n",
    "    trial_video_resolution = cv2_video_resolution(trial_recording_path)\n",
    "    trial_video_fps = cv2_video_fps(trial_recording_path)\n",
    "    trial_video_frame_count = cv2_video_frame_count(trial_recording_path)\n",
    "\n",
    "    # Load and preprocess gaze points\n",
    "    print(f\"Loading gaze data for {recording_uuid}\")\n",
    "    gaze_data_path = get_gaze_data_path(calibration_recording.id)\n",
    "    gaze_point_per_frame = get_gaze_point_per_frame(\n",
    "        gaze_data_path=gaze_data_path,\n",
    "        resolution=trial_video_resolution,\n",
    "        frame_count=trial_video_frame_count,\n",
    "        fps=trial_video_fps,\n",
    "    )\n",
    "\n",
    "    # Get all annotations that were viewed\n",
    "    print(f\"Getting viewed annotations for {recording_uuid}\")\n",
    "    annotations_per_frame = get_viewed_annotations_per_frame(\n",
    "        annotated_classes=annotated_classes,\n",
    "        gaze_point_per_frame=gaze_point_per_frame,\n",
    "        video_resolution=trial_video_resolution,\n",
    "    )\n",
    "\n",
    "    # Build the ground truth DataFrame\n",
    "    # TODO: Might be interesting to add blur metric per frame to the ground truth dataset\n",
    "    print(f\"Building ground truth for {recording_uuid}\")\n",
    "    for frame_idx, annotation_paths in annotations_per_frame.items():\n",
    "        for annotation_path in annotation_paths:\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            class_id = int(annotation_file[\"class_id\"])\n",
    "            mask_area = np.sum(annotation_file[\"mask\"])\n",
    "\n",
    "            ground_truth_df = pd.concat(\n",
    "                [\n",
    "                    ground_truth_df,\n",
    "                    pd.DataFrame(\n",
    "                        {\n",
    "                            \"recording_uuid\": [recording_uuid],\n",
    "                            \"frame_idx\": [frame_idx],\n",
    "                            \"class_id\": [class_id],\n",
    "                            \"mask_area\": [mask_area],\n",
    "                        }\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    if CREATE_VALIDATION_VIDEO:\n",
    "        # Extract frames from the video and save them to a temporary directory\n",
    "        print(f\"Extracting frames for {recording_uuid}\")\n",
    "        tmp_frames_dir = tempfile.TemporaryDirectory()\n",
    "        tmp_frames_path = Path(tmp_frames_dir.name)\n",
    "        extract_frames_to_dir(\n",
    "            video_path=trial_recording_path, frames_path=tmp_frames_path, print_output=False\n",
    "        )\n",
    "        frames = sorted(list(tmp_frames_path.glob(\"*.jpg\")), key=lambda x: int(x.stem))\n",
    "\n",
    "        print(f\"Drawing annotations for {recording_uuid}\")\n",
    "        draw_validation_video_frames(\n",
    "            frames=frames,\n",
    "            annotations_per_frame=annotations_per_frame,\n",
    "            gaze_point_per_frame=gaze_point_per_frame,\n",
    "            annotated_classes=annotated_classes,\n",
    "        )\n",
    "\n",
    "        print(f\"Creating video for {recording_uuid}\")\n",
    "        cmd = f'ffmpeg -hwaccel cuda -y -pattern_type glob -framerate {TOBII_GLASSES_FPS} -i \"{tmp_frames_path!s}/*.jpg\" -c:v libx264 -pix_fmt yuv420p \"{LABELING_VALIDATION_VIDEOS_PATH}/{recording_uuid}.mp4\"'\n",
    "        subprocess.run(cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "ground_truth_df.to_csv(\n",
    "    GROUND_TRUTH_PATH,\n",
    "    index=False,\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
