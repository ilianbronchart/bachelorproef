{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9cd7602",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import subprocess\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sqlalchemy.orm import Session\n",
    "from src.api.controllers.calibration_recording_controller import (\n",
    "    AnnotatedClassResponse,\n",
    "    get_annotated_classes,\n",
    "    get_gaze_data_path,\n",
    "    get_recording_path,\n",
    ")\n",
    "from src.api.controllers.gaze_segmentation import (\n",
    "    get_gaze_point_per_frame,\n",
    "    mask_was_viewed,\n",
    ")\n",
    "from src.config import TOBII_GLASSES_FPS, VIEWED_RADIUS\n",
    "from src.db import engine\n",
    "from src.db.models import CalibrationRecording\n",
    "from src.logic.glasses.domain import GazePoint\n",
    "from src.utils import (\n",
    "    cv2_video_fps,\n",
    "    cv2_video_frame_count,\n",
    "    cv2_video_resolution,\n",
    "    draw_annotation_on_frame,\n",
    "    extract_frames_to_dir,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a22a0be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"experiment_metadata.json\") as file:\n",
    "    experiment_metadata = json.load(file)\n",
    "    trial_recordings_metadata = experiment_metadata[\"trial_recordings_metadata\"]\n",
    "    trial_recording_uuids = list(trial_recordings_metadata.keys())\n",
    "    labeling_same_background_uuid = experiment_metadata[\"labeling_same_background_uuid\"]\n",
    "    labeling_diff_background_uuid = experiment_metadata[\"labeling_diff_background_uuid\"]\n",
    "\n",
    "with Session(engine) as session:\n",
    "    calibration_recordings = session.query(CalibrationRecording).all()\n",
    "\n",
    "    trial_recordings = {\n",
    "        cr.recording_uuid: cr\n",
    "        for cr in calibration_recordings\n",
    "        if cr.recording_uuid in trial_recording_uuids\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37577c44",
   "metadata": {},
   "source": [
    "# Validate Labeling Data\n",
    "\n",
    "Here, we will validate the labeling data so that we can be sure that the data is correct and that the labels are correct. We will also check for any missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1772e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for recording_uuid, trial_metadata in trial_recordings_metadata.items():\n",
    "    calibration_recording = trial_recordings[recording_uuid]\n",
    "    annotated_classes = get_annotated_classes(calibration_recording.id)\n",
    "    class_name_to_annotation_paths = {\n",
    "        anno_class.class_name: anno_class.annotation_paths\n",
    "        for anno_class in annotated_classes\n",
    "    }\n",
    "    trial_objects_metadata = trial_metadata[\"objects\"]\n",
    "\n",
    "    assert len(trial_objects_metadata) == 5, \"Number of objects in trial is not 5\"\n",
    "    assert len(annotated_classes) == 5, \"Number of annotated classes is not 5\"\n",
    "\n",
    "    for object_metadata in trial_objects_metadata:\n",
    "        class_name, _, _ = object_metadata\n",
    "\n",
    "        assert class_name in class_name_to_annotation_paths, (\n",
    "            \"Original object class name not in annotated classes\"\n",
    "        )\n",
    "\n",
    "        annotation_paths = class_name_to_annotation_paths[class_name]\n",
    "        assert len(annotation_paths) > 0, f\"Annotation paths for {class_name} are empty\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b0d202",
   "metadata": {},
   "source": [
    "# Create the ground truth dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b4fb15b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['mask', 'box', 'roi', 'class_id', 'frame_idx']\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    np.load(\n",
    "        \"/home/zilian/projects/bachelorproef/experiments/controlled_experiment/data/labeling_results/2/1/0.npz\"\n",
    "    ).files\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58422dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_viewed_annotations_per_frame(\n",
    "    annotated_classes: list[AnnotatedClassResponse],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    video_resolution: tuple[int, int],\n",
    "):\n",
    "    # Gather all annotation paths for each annotated frame\n",
    "    annotations_per_frame: dict[int, list[Path]] = {}\n",
    "    for anno_class in annotated_classes:\n",
    "        for annotation_path in anno_class.annotation_paths:\n",
    "            frame_idx = int(annotation_path.stem)\n",
    "\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            mask = annotation_file[\"mask\"]\n",
    "            x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "\n",
    "            # Put the mask in a tensor of the same size as the video frame\n",
    "            mask_full = np.zeros(video_resolution, dtype=np.uint8)\n",
    "            mask_full[y1:y2, x1:x2] = mask\n",
    "            mask_full_torch = torch.from_numpy(mask_full)\n",
    "\n",
    "            gaze_point = gaze_point_per_frame.get(frame_idx)\n",
    "            if gaze_point is None:\n",
    "                continue\n",
    "\n",
    "            if mask_was_viewed(mask_full_torch, gaze_point.position):\n",
    "                if frame_idx not in annotations_per_frame:\n",
    "                    annotations_per_frame[frame_idx] = []\n",
    "\n",
    "                annotations_per_frame[frame_idx].append(annotation_path)\n",
    "\n",
    "    return annotations_per_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4acf39ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_validation_video_frames(\n",
    "    frames: list[Path],\n",
    "    annotations_per_frame: dict[int, list[Path]],\n",
    "    gaze_point_per_frame: dict[int, GazePoint],\n",
    "    annotated_classes: list[AnnotatedClassResponse],\n",
    "):\n",
    "    class_id_to_annotated_class = {\n",
    "        anno_class.id: anno_class for anno_class in annotated_classes\n",
    "    }\n",
    "\n",
    "    # Iterate over frames and draw the annotations on them if they exist\n",
    "    for frame in tqdm(frames, desc=\"Drawing annotations on frames\"):\n",
    "        frame_idx = int(frame.stem)\n",
    "        frame_img = cv2.imread(str(frame))\n",
    "\n",
    "        if annotations_per_frame.get(frame_idx) is not None:\n",
    "            for annotation_path in annotations_per_frame[frame_idx]:\n",
    "                annotation_file = np.load(annotation_path)\n",
    "                class_id = int(annotation_file[\"class_id\"])\n",
    "                x1, y1, x2, y2 = annotation_file[\"box\"]\n",
    "                mask = annotation_file[\"mask\"]\n",
    "\n",
    "                # Squeeze mask if it has an extra dimension\n",
    "                if mask.ndim == 3 and mask.shape[0] == 1:\n",
    "                    mask = mask[0]\n",
    "                if mask.dtype != bool:\n",
    "                    mask = mask.astype(bool)\n",
    "\n",
    "                class_color_hex = class_id_to_annotated_class[class_id].color\n",
    "                class_name = class_id_to_annotated_class[class_id].class_name\n",
    "                box = (x1, y1, x2, y2)\n",
    "\n",
    "                # Annotate the frame using the reusable function\n",
    "                frame_img = draw_annotation_on_frame(\n",
    "                    frame_img, mask, box, class_color_hex, class_name\n",
    "                )\n",
    "\n",
    "        # Draw the gaze point on the frame\n",
    "        gaze_point = gaze_point_per_frame.get(frame_idx)\n",
    "        if gaze_point is not None:\n",
    "            gaze_x, gaze_y = gaze_point.position\n",
    "            cv2.circle(\n",
    "                frame_img,\n",
    "                (int(gaze_x), int(gaze_y)),\n",
    "                radius=VIEWED_RADIUS,\n",
    "                color=(0, 0, 255),\n",
    "                thickness=2,\n",
    "            )\n",
    "\n",
    "        # Save the modified image back to its original location\n",
    "        cv2.imwrite(str(frame), frame_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "564f9814",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/14 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gaze data for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Getting viewed annotations for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Building ground truth for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Extracting frames for 67b71a70-da64-467a-9fb6-91bc29265fd1\n",
      "Drawing annotations for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Drawing annotations on frames: 100%|██████████| 2064/2064 [00:14<00:00, 146.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating video for 67b71a70-da64-467a-9fb6-91bc29265fd1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:21<00:00,  5.83s/it]\n",
      "100%|██████████| 14/14 [01:21<00:00,  5.83s/it]\n"
     ]
    }
   ],
   "source": [
    "LABELING_VALIDATION_VIDEOS_PATH = Path(\"data/labeling_validation_videos\")\n",
    "if not LABELING_VALIDATION_VIDEOS_PATH.exists():\n",
    "    os.makedirs(LABELING_VALIDATION_VIDEOS_PATH)\n",
    "else:\n",
    "    for file in LABELING_VALIDATION_VIDEOS_PATH.glob(\"*.mp4\"):\n",
    "        os.remove(file)\n",
    "\n",
    "CREATE_VALIDATION_VIDEO = True\n",
    "\n",
    "GROUND_TRUTH_PATH = Path(\"data/ground_truth.csv\")\n",
    "if GROUND_TRUTH_PATH.exists():\n",
    "    GROUND_TRUTH_PATH.unlink()\n",
    "ground_truth_df = pd.DataFrame(\n",
    "    columns=[\"recording_uuid\", \"frame_idx\", \"class_id\", \"mask_area\"]\n",
    ")\n",
    "\n",
    "for recording_uuid, trial_metadata in tqdm(trial_recordings_metadata.items()):\n",
    "    if recording_uuid != \"67b71a70-da64-467a-9fb6-91bc29265fd1\":\n",
    "        continue\n",
    "\n",
    "    calibration_recording = trial_recordings[recording_uuid]\n",
    "    annotated_classes = get_annotated_classes(calibration_recording.id)\n",
    "\n",
    "    # Get statistics of the video\n",
    "    trial_recording_path = get_recording_path(calibration_recording.id)\n",
    "    trial_video_resolution = cv2_video_resolution(trial_recording_path)\n",
    "    trial_video_fps = cv2_video_fps(trial_recording_path)\n",
    "    trial_video_frame_count = cv2_video_frame_count(trial_recording_path)\n",
    "\n",
    "    # Load and preprocess gaze points\n",
    "    print(f\"Loading gaze data for {recording_uuid}\")\n",
    "    gaze_data_path = get_gaze_data_path(calibration_recording.id)\n",
    "    gaze_point_per_frame = get_gaze_point_per_frame(\n",
    "        gaze_data_path=gaze_data_path,\n",
    "        resolution=trial_video_resolution,\n",
    "        frame_count=trial_video_frame_count,\n",
    "        fps=trial_video_fps,\n",
    "    )\n",
    "\n",
    "    # Get all annotations that were viewed\n",
    "    print(f\"Getting viewed annotations for {recording_uuid}\")\n",
    "    annotations_per_frame = get_viewed_annotations_per_frame(\n",
    "        annotated_classes=annotated_classes,\n",
    "        gaze_point_per_frame=gaze_point_per_frame,\n",
    "        video_resolution=trial_video_resolution,\n",
    "    )\n",
    "\n",
    "    # Build the ground truth DataFrame\n",
    "    # TODO: Might be interesting to add blur metric per frame to the ground truth dataset\n",
    "    print(f\"Building ground truth for {recording_uuid}\")\n",
    "    for frame_idx, annotation_paths in annotations_per_frame.items():\n",
    "        for annotation_path in annotation_paths:\n",
    "            annotation_file = np.load(annotation_path)\n",
    "            class_id = int(annotation_file[\"class_id\"])\n",
    "            mask_area = np.sum(annotation_file[\"mask\"])\n",
    "\n",
    "            ground_truth_df = pd.concat([\n",
    "                ground_truth_df,\n",
    "                pd.DataFrame({\n",
    "                    \"recording_uuid\": [recording_uuid],\n",
    "                    \"frame_idx\": [frame_idx],\n",
    "                    \"class_id\": [class_id],\n",
    "                    \"mask_area\": [mask_area],\n",
    "                }),\n",
    "            ])\n",
    "\n",
    "    if CREATE_VALIDATION_VIDEO:\n",
    "        # Extract frames from the video and save them to a temporary directory\n",
    "        print(f\"Extracting frames for {recording_uuid}\")\n",
    "        tmp_frames_dir = tempfile.TemporaryDirectory()\n",
    "        tmp_frames_path = Path(tmp_frames_dir.name)\n",
    "        extract_frames_to_dir(\n",
    "            video_path=trial_recording_path,\n",
    "            frames_path=tmp_frames_path,\n",
    "            print_output=False,\n",
    "        )\n",
    "        frames = sorted(list(tmp_frames_path.glob(\"*.jpg\")), key=lambda x: int(x.stem))\n",
    "\n",
    "        print(f\"Drawing annotations for {recording_uuid}\")\n",
    "        draw_validation_video_frames(\n",
    "            frames=frames,\n",
    "            annotations_per_frame=annotations_per_frame,\n",
    "            gaze_point_per_frame=gaze_point_per_frame,\n",
    "            annotated_classes=annotated_classes,\n",
    "        )\n",
    "\n",
    "        print(f\"Creating video for {recording_uuid}\")\n",
    "        cmd = f'ffmpeg -hwaccel cuda -y -pattern_type glob -framerate {TOBII_GLASSES_FPS} -i \"{tmp_frames_path!s}/*.jpg\" -c:v libx264 -pix_fmt yuv420p \"{LABELING_VALIDATION_VIDEOS_PATH}/{recording_uuid}.mp4\"'\n",
    "        subprocess.run(\n",
    "            cmd, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL\n",
    "        )\n",
    "\n",
    "ground_truth_df.to_csv(\n",
    "    GROUND_TRUTH_PATH,\n",
    "    index=False,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
