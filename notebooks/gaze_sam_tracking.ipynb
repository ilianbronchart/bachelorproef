{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zilian/projects/bachelorproef\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zilian/projects/bachelorproef/.venv/lib/python3.10/site-packages/IPython/core/magics/osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import cv2\n",
    "import torch\n",
    "from src.utils import cv2_video_fps, cv2_video_resolution\n",
    "from src.logic.glasses.gaze import parse_gazedata_file, match_frames_to_gaze, get_gaze_points\n",
    "from ultralytics import FastSAM\n",
    "from pathlib import Path\n",
    "from torchvision.ops import masks_to_boxes\n",
    "from ultralytics.engine.results import Results\n",
    "import torchvision.transforms.functional as F\n",
    "from tqdm import tqdm\n",
    "import concurrent.futures\n",
    "import numpy as np\n",
    "import tempfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GazeSAMJob:\n",
    "    def __init__(self,\n",
    "                 video_path: Path,\n",
    "                 gaze_data_path: Path,\n",
    "                 tmp_frames_path: Path,\n",
    "                 gaze_sam_results_path: Path,\n",
    "                 batch_size: int = 50,\n",
    "                 crop_size: int = 512,\n",
    "                 fovea_fov: float = 1,\n",
    "                 fov_x: float = 95,\n",
    "                 checkpoint_path: str = \"checkpoints/FastSAM-x.pt\"):\n",
    "        self.video_path = video_path\n",
    "        self.gaze_data_path = gaze_data_path\n",
    "        self.tmp_frames_path = tmp_frames_path\n",
    "        self.gaze_sam_results_path = gaze_sam_results_path\n",
    "        self.batch_size = batch_size\n",
    "        self.crop_size = crop_size\n",
    "        self.half_crop = crop_size // 2\n",
    "        self.fovea_fov = fovea_fov\n",
    "        self.fov_x = fov_x\n",
    "\n",
    "        # Load the FastSAM model.\n",
    "        self.model = FastSAM(checkpoint_path)\n",
    "\n",
    "        # Video properties.\n",
    "        self.resolution = cv2_video_resolution(self.video_path)\n",
    "        self.fps = cv2_video_fps(self.video_path)\n",
    "        self.viewed_radius = int((self.fovea_fov / self.fov_x) * self.resolution[1])\n",
    "\n",
    "        # Parse gaze data.\n",
    "        self.gaze_data = parse_gazedata_file(self.gaze_data_path)\n",
    "        self.gaze_points = get_gaze_points(self.gaze_data, self.resolution)\n",
    "\n",
    "        # Get all frame file paths (assuming names are frame indices).\n",
    "        self.frames = sorted(\n",
    "            [frame for frame in self.tmp_frames_path.iterdir() if frame.suffix.lower() == \".jpg\"],\n",
    "            key=lambda x: int(x.stem)\n",
    "        )\n",
    "        # Map frame indexes to gaze points.\n",
    "        self.frame_gaze_mapping = match_frames_to_gaze(len(self.frames), self.gaze_points, self.fps)\n",
    "        # Batch the frame file paths.\n",
    "        self.frame_batches = [self.frames[i:i + self.batch_size]\n",
    "                              for i in range(0, len(self.frames), self.batch_size)]\n",
    "\n",
    "        # Timing statistics.\n",
    "        self.total_batch_load_time = 0.0\n",
    "        self.total_inference_time = 0.0\n",
    "        self.total_postprocess_time = 0.0\n",
    "\n",
    "    def get_gaze_position(self, frame_idx: int) -> tuple[int, int] | None:\n",
    "        \"\"\"\n",
    "        Get the gaze position for a frame index.\n",
    "        \"\"\"\n",
    "        gaze_points = self.frame_gaze_mapping[frame_idx]\n",
    "        if len(gaze_points) == 0:\n",
    "            return None\n",
    "        return gaze_points[0].position\n",
    "        \n",
    "\n",
    "    def load_image(self, frame_path: Path, gaze_point: tuple[int, int]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Load an image from disk, crop around the gaze point, and normalize.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not frame_path.stem.isdigit():\n",
    "                raise ValueError(f\"Frame name should be the frame index: {frame_path.stem}\")\n",
    "\n",
    "            img = cv2.imread(str(frame_path))\n",
    "            if img is None:\n",
    "                raise ValueError(f\"Failed to load image: {frame_path}\")\n",
    "\n",
    "            # Convert image to a CUDA tensor in CHW format.\n",
    "            img = torch.from_numpy(img).to(\"cuda\").permute(2, 0, 1)\n",
    "            cx, cy = gaze_point\n",
    "            img_crop = F.crop(img, cy - self.half_crop, cx - self.half_crop, self.crop_size, self.crop_size)\n",
    "            return img_crop.float() / 255.0\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading frame {frame_path}: {e}\")\n",
    "    \n",
    "    def filter_large_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Filter out masks with area greater than 30% of the frame area\n",
    "\n",
    "        Args:\n",
    "            masks: tensor containing masks of shape (N, H, W)\n",
    "        \"\"\"\n",
    "        if len(masks) == 0:\n",
    "            return masks\n",
    "\n",
    "        _, height, width = masks.shape\n",
    "        frame_area = height * width\n",
    "        max_mask_area = 0.3 * frame_area\n",
    "\n",
    "        mask_areas = masks.sum(dim=(1, 2))\n",
    "        filtered_masks = masks[mask_areas <= max_mask_area]\n",
    "        return filtered_masks\n",
    "\n",
    "    def filter_viewed_masks(self, masks: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Filter out masks that are not within the viewed radius of the gaze point.\n",
    "        \"\"\"\n",
    "        if len(masks) == 0:\n",
    "            return masks\n",
    "\n",
    "        sample_mask = masks[0]\n",
    "        height, width = sample_mask.shape\n",
    "        device = sample_mask.device\n",
    "\n",
    "        # Create a circular mask centered at the gaze point.\n",
    "        y = torch.arange(0, height, device=device).view(-1, 1).repeat(1, width)\n",
    "        x = torch.arange(0, width, device=device).view(1, -1).repeat(height, 1)\n",
    "        dist_sq = (x - self.half_crop) ** 2 + (y - self.half_crop) ** 2\n",
    "        circular_mask = (dist_sq <= self.viewed_radius**2).float().unsqueeze(0)  # (1, H, W)\n",
    "\n",
    "        # Apply the circular mask.\n",
    "        masked_masks = masks * circular_mask\n",
    "        mask_areas = masked_masks.sum(dim=(1, 2))\n",
    "        return masks[mask_areas > 0]\n",
    "\n",
    "    def postprocess_result(self, frame_idx: int, masks: torch.Tensor, gaze_point: tuple[int, int]) -> None:\n",
    "        \"\"\"\n",
    "        Postprocess inference results by filtering masks and saving them.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            filtered_masks = self.filter_large_masks(masks)\n",
    "            viewed_masks = self.filter_viewed_masks(filtered_masks)\n",
    "\n",
    "            boxes = masks_to_boxes(viewed_masks).int().cpu().numpy()\n",
    "\n",
    "            cropped_masks = []\n",
    "            for i, mask in enumerate(viewed_masks):\n",
    "                x1, y1, x2, y2 = boxes[i]\n",
    "                mask_np = mask.detach().cpu().numpy()\n",
    "                cropped_masks.append(mask_np[y1:y2, x1:x2])\n",
    "\n",
    "            cx, cy = gaze_point\n",
    "            crop_left, crop_top = max(cx - self.half_crop, 0), max(cy - self.half_crop, 0)\n",
    "            frame_boxes = boxes + np.array([crop_left, crop_top, crop_left, crop_top])\n",
    "\n",
    "            # Create an object array explicitly for the cropped masks.\n",
    "            cropped_masks_obj = np.empty(len(cropped_masks), dtype=object)\n",
    "            for i, cm in enumerate(cropped_masks):\n",
    "                cropped_masks_obj[i] = cm\n",
    "\n",
    "            self.gaze_sam_results_path.mkdir(parents=True, exist_ok=True)\n",
    "            np.savez_compressed(\n",
    "                self.gaze_sam_results_path / f\"{frame_idx}.npz\",\n",
    "                boxes=frame_boxes,\n",
    "                masks=cropped_masks_obj\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing frame {frame_idx}: {e}\")\n",
    "\n",
    "    def process_batches(self) -> None:\n",
    "        \"\"\"\n",
    "        For each batch of frames: load images, run inference, and postprocess results.\n",
    "        \"\"\"\n",
    "        for batch in tqdm(self.frame_batches, desc=\"Batches\"):\n",
    "            # Load images concurrently.\n",
    "            start_time = time.time()\n",
    "            batch_frame_indexes = []\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = []\n",
    "                for image in batch:\n",
    "                    if not image.stem.isdigit():\n",
    "                        raise ValueError(f\"Frame name should be the frame index: {image.stem}\")\n",
    "                    \n",
    "                    frame_idx = int(image.stem)\n",
    "                    gaze_position = self.get_gaze_position(frame_idx)\n",
    "\n",
    "                    if gaze_position is not None:\n",
    "                        futures.append(executor.submit(self.load_image, image, gaze_position))\n",
    "                        batch_frame_indexes.append(frame_idx)\n",
    "\n",
    "                batch_tensor = torch.stack([future.result() for future in futures])\n",
    "            self.total_batch_load_time += time.time() - start_time\n",
    "\n",
    "            # Run inference on the batch and measure GPU time.\n",
    "            start_event = torch.cuda.Event(enable_timing=True)\n",
    "            end_event = torch.cuda.Event(enable_timing=True)\n",
    "            start_event.record()\n",
    "            results: list[Results] = self.model.predict(\n",
    "                source=batch_tensor,\n",
    "                retina_masks=True,\n",
    "                device=\"cuda\",\n",
    "                verbose=False,\n",
    "                imgsz=self.crop_size,\n",
    "            )\n",
    "            end_event.record()\n",
    "            torch.cuda.synchronize()  # Ensure GPU operations have finished.\n",
    "            self.total_inference_time += start_event.elapsed_time(end_event) / 1000.0\n",
    "\n",
    "            # Postprocess results concurrently.\n",
    "            start = time.time()\n",
    "            with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "                futures = []\n",
    "                for i, result in enumerate(results):\n",
    "                    if result.masks is None or len(result.masks) == 0:\n",
    "                        continue\n",
    "                    frame_idx = batch_frame_indexes[i]\n",
    "                    gaze_point = self.get_gaze_position(frame_idx)\n",
    "                    futures.append(\n",
    "                        executor.submit(self.postprocess_result, frame_idx, result.masks.data, gaze_point)\n",
    "                    )\n",
    "                concurrent.futures.wait(futures)\n",
    "            self.total_postprocess_time += time.time() - start\n",
    "\n",
    "        print(f\"Total batch load time: {self.total_batch_load_time:.2f} seconds\")\n",
    "        print(f\"Total inference time: {self.total_inference_time:.2f} seconds\")\n",
    "        print(f\"Total postprocess time: {self.total_postprocess_time:.2f} seconds\")\n",
    "\n",
    "    def process_result_frame(self, original_frame: Path, frame_result: Path, result_frames_dir: Path, gaze_point: tuple[int, int] | None = None) -> None:\n",
    "        \"\"\"\n",
    "        Overlay each mask as a red translucent region (alpha 0.3) and draw its bounding box\n",
    "        on the original frame. The masks are drawn at the bounding box location since they\n",
    "        were saved cropped to their bounding box. Also overlay the gaze point as a circle if provided.\n",
    "        \"\"\"\n",
    "        frame = cv2.imread(str(original_frame))\n",
    "        if frame is None:\n",
    "            raise ValueError(f\"Failed to load frame: {original_frame}\")\n",
    "\n",
    "        data = np.load(frame_result, allow_pickle=True)\n",
    "        boxes = data[\"boxes\"]  # Expected shape: [N, 4] with (x1, y1, x2, y2)\n",
    "        masks = data[\"masks\"]  # A list or array of cropped mask arrays.\n",
    "\n",
    "        alpha = 0.3  # Opacity for the red overlay.\n",
    "        for i, box in enumerate(boxes):\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            roi = frame[y1:y2, x1:x2]\n",
    "            mask = masks[i]\n",
    "\n",
    "            # Ensure the mask is binary (0 or 255).\n",
    "            if mask.dtype != np.uint8:\n",
    "                mask = (mask > 0.5).astype(np.uint8) * 255\n",
    "\n",
    "            # Resize the mask to match ROI if necessary.\n",
    "            if mask.shape != roi.shape[:2]:\n",
    "                mask = cv2.resize(mask, (roi.shape[1], roi.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "\n",
    "            red_overlay = np.full_like(roi, (0, 0, 255))  # Red in BGR.\n",
    "            mask_bool = mask.astype(bool)\n",
    "            roi[mask_bool] = cv2.addWeighted(roi[mask_bool], 1 - alpha, red_overlay[mask_bool], alpha, 0)\n",
    "\n",
    "            # Draw the bounding box.\n",
    "            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "\n",
    "        # If a gaze point is provided, draw a circle on the frame.\n",
    "        if gaze_point is not None:\n",
    "            cv2.circle(frame, gaze_point, self.viewed_radius, (255, 0, 0), 2)\n",
    "\n",
    "        result_frames_dir.mkdir(parents=True, exist_ok=True)\n",
    "        cv2.imwrite(str(result_frames_dir / f\"{original_frame.stem}.jpg\"), frame)\n",
    "\n",
    "\n",
    "    def create_video_from_results(self, output_video: Path) -> None:\n",
    "        \"\"\"\n",
    "        Process saved .npz result frames, overlay masks and gaze point on the original frames,\n",
    "        and run ffmpeg to create a video.\n",
    "        \"\"\"\n",
    "        with tempfile.TemporaryDirectory() as tmpdir, concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "            futures = []\n",
    "            for batch in self.frame_batches:\n",
    "                for frame in batch:\n",
    "                    frame_idx = int(frame.stem)\n",
    "                    frame_result = self.gaze_sam_results_path / f\"{frame_idx}.npz\"\n",
    "                    if frame_result.exists():\n",
    "                        gaze_point = self.get_gaze_position(frame_idx)\n",
    "                        futures.append(\n",
    "                            executor.submit(self.process_result_frame, frame, frame_result, Path(tmpdir), gaze_point)\n",
    "                        )\n",
    "            concurrent.futures.wait(futures)\n",
    "            # Use ffmpeg to create a video (frames are read in glob order, so naming matters).\n",
    "            cmd = f'ffmpeg -hwaccel cuda -y -pattern_type glob -framerate {self.fps} -i \"{tmpdir}/*.jpg\" -c:v libx264 -pix_fmt yuv420p \"{output_video}\"'\n",
    "            os.system(cmd)\n",
    "\n",
    "    def run(self, output_video: Path) -> None:\n",
    "        \"\"\"\n",
    "        Execute the complete pipeline: process batches and then create the output video.\n",
    "        \"\"\"\n",
    "        self.process_batches()\n",
    "        self.create_video_from_results(output_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = Path(\"data/recordings/39f5164f-873d-4d6b-be6b-e1d5db79c02a.mp4\")\n",
    "GAZE_DATA_PATH = Path(\"data/recordings/39f5164f-873d-4d6b-be6b-e1d5db79c02a.tsv\")\n",
    "TMP_FRAMES_PATH = Path(\"notebooks/tmp/frames\")\n",
    "GAZE_SAM_RESULTS_PATH = Path(\"notebooks/tmp/gaze_sam_results\")\n",
    "\n",
    "gaze_sam_job = GazeSAMJob(\n",
    "    video_path=VIDEO_PATH,\n",
    "    gaze_data_path=GAZE_DATA_PATH,\n",
    "    tmp_frames_path=TMP_FRAMES_PATH,\n",
    "    gaze_sam_results_path=GAZE_SAM_RESULTS_PATH\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 51/51 [00:29<00:00,  1.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total batch load time: 2.87 seconds\n",
      "Total inference time: 19.71 seconds\n",
      "Total postprocess time: 7.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "gaze_sam_job.process_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n",
      "  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n",
      "  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n",
      "  libavutil      56. 70.100 / 56. 70.100\n",
      "  libavcodec     58.134.100 / 58.134.100\n",
      "  libavformat    58. 76.100 / 58. 76.100\n",
      "  libavdevice    58. 13.100 / 58. 13.100\n",
      "  libavfilter     7.110.100 /  7.110.100\n",
      "  libswscale      5.  9.100 /  5.  9.100\n",
      "  libswresample   3.  9.100 /  3.  9.100\n",
      "  libpostproc    55.  9.100 / 55.  9.100\n",
      "Input #0, image2, from '/tmp/tmpf7ou6ogj/*.jpg':\n",
      "  Duration: 00:01:40.62, start: 0.000000, bitrate: N/A\n",
      "  Stream #0:0: Video: mjpeg (Baseline), yuvj420p(pc, bt470bg/unknown/unknown), 1920x1080 [SAR 1:1 DAR 16:9], 24.95 fps, 24.95 tbr, 24.95 tbn, 24.95 tbc\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mjpeg (native) -> h264 (libx264))\n",
      "Press [q] to stop, [?] for help\n",
      "[libx264 @ 0x559202988900] using SAR=1/1\n",
      "[libx264 @ 0x559202988900] using cpu capabilities: MMX2 SSE2Fast SSSE3 SSE4.2 AVX FMA3 BMI2 AVX2 AVX512\n",
      "[libx264 @ 0x559202988900] profile High, level 4.0, 4:2:0, 8-bit\n",
      "[libx264 @ 0x559202988900] 264 - core 163 r3060 5db6aa6 - H.264/MPEG-4 AVC codec - Copyleft 2003-2021 - http://www.videolan.org/x264.html - options: cabac=1 ref=3 deblock=1:0:0 analyse=0x3:0x113 me=hex subme=7 psy=1 psy_rd=1.00:0.00 mixed_ref=1 me_range=16 chroma_me=1 trellis=1 8x8dct=1 cqm=0 deadzone=21,11 fast_pskip=1 chroma_qp_offset=-2 threads=24 lookahead_threads=4 sliced_threads=0 nr=0 decimate=1 interlaced=0 bluray_compat=0 constrained_intra=0 bframes=3 b_pyramid=2 b_adapt=1 b_bias=0 direct=1 weightb=1 open_gop=0 weightp=2 keyint=250 keyint_min=24 scenecut=40 intra_refresh=0 rc_lookahead=40 rc=crf mbtree=1 crf=23.0 qcomp=0.60 qpmin=0 qpmax=69 qpstep=4 ip_ratio=1.40 aq=1:1.00\n",
      "Output #0, mp4, to 'notebooks/tmp/output.mp4':\n",
      "  Metadata:\n",
      "    encoder         : Lavf58.76.100\n",
      "  Stream #0:0: Video: h264 (avc1 / 0x31637661), yuv420p(tv, bt470bg/unknown/unknown, progressive), 1920x1080 [SAR 1:1 DAR 16:9], q=2-31, 24.95 fps, 858669.00 tbn\n",
      "    Metadata:\n",
      "      encoder         : Lavc58.134.100 libx264\n",
      "    Side data:\n",
      "      cpb: bitrate max/min/avg: 0/0/0 buffer size: 0 vbv_delay: N/A\n",
      "frame= 2511 fps=124 q=-1.0 Lsize=   63086kB time=00:01:40.50 bitrate=5142.2kbits/s speed=4.97x    \n",
      "video:63057kB audio:0kB subtitle:0kB other streams:0kB global headers:0kB muxing overhead: 0.045048%\n",
      "[libx264 @ 0x559202988900] frame I:22    Avg QP:19.93  size: 56303\n",
      "[libx264 @ 0x559202988900] frame P:947   Avg QP:22.70  size: 39710\n",
      "[libx264 @ 0x559202988900] frame B:1542  Avg QP:24.00  size: 16684\n",
      "[libx264 @ 0x559202988900] consecutive B-frames: 14.5%  6.9% 11.8% 66.7%\n",
      "[libx264 @ 0x559202988900] mb I  I16..4: 21.1% 74.6%  4.3%\n",
      "[libx264 @ 0x559202988900] mb P  I16..4: 14.4% 38.8%  1.8%  P16..4: 25.5%  6.1%  1.8%  0.0%  0.0%    skip:11.6%\n",
      "[libx264 @ 0x559202988900] mb B  I16..4:  2.8%  6.3%  0.2%  B16..8: 40.8%  5.2%  0.7%  direct: 2.5%  skip:41.5%  L0:55.0% L1:39.2% BI: 5.8%\n",
      "[libx264 @ 0x559202988900] 8x8 transform intra:70.0% inter:78.5%\n",
      "[libx264 @ 0x559202988900] coded y,uvDC,uvAC intra: 38.6% 49.6% 3.6% inter: 13.8% 16.5% 0.6%\n",
      "[libx264 @ 0x559202988900] i16 v,h,dc,p: 31% 29% 13% 27%\n",
      "[libx264 @ 0x559202988900] i8 v,h,dc,ddl,ddr,vr,hd,vl,hu: 33% 26% 31%  2%  1%  2%  2%  1%  2%\n",
      "[libx264 @ 0x559202988900] i4 v,h,dc,ddl,ddr,vr,hd,vl,hu: 26% 31% 13%  4%  6%  5%  8%  4%  5%\n",
      "[libx264 @ 0x559202988900] i8c dc,h,v,p: 53% 21% 22%  4%\n",
      "[libx264 @ 0x559202988900] Weighted P-Frames: Y:3.8% UV:0.7%\n",
      "[libx264 @ 0x559202988900] ref P L0: 72.5% 13.2% 10.7%  3.4%  0.1%\n",
      "[libx264 @ 0x559202988900] ref B L0: 93.3%  5.6%  1.1%\n",
      "[libx264 @ 0x559202988900] ref B L1: 97.4%  2.6%\n",
      "[libx264 @ 0x559202988900] kb/s:5133.67\n"
     ]
    }
   ],
   "source": [
    "RESULT_VIDEO_PATH = Path(\"notebooks/tmp/output.mp4\")\n",
    "RESULT_VIDEO_PATH.unlink(missing_ok=True)\n",
    "\n",
    "gaze_sam_job.create_video_from_results(RESULT_VIDEO_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract_frames_to_dir(VIDEO_PATH, TMP_FRAMES_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
