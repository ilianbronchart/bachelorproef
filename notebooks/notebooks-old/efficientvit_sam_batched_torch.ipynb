{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from efficientvit.models.efficientvit.sam import EfficientViTSamPredictor, SamPad\n",
    "from efficientvit.sam_model_zoo import create_efficientvit_sam_model\n",
    "from src.core.utils import cv2_itervideo\n",
    "from src.api.services import gaze_service\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = Path(\"../data/recordings/af47ccce-c344-49d9-9916-5729e2ddc021.mp4\")\n",
    "GAZE_DATA_PATH = Path(\"../data/recordings/af47ccce-c344-49d9-9916-5729e2ddc021.tsv\")\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "frames = cv2_itervideo(VIDEO_PATH, device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = (frames.shape[3], frames.shape[2])\n",
    "gaze_data = gaze_service.parse_gazedata_file(GAZE_DATA_PATH)\n",
    "gaze_points = gaze_service.get_gaze_points(gaze_data, resolution)\n",
    "frame_gaze_mapping = gaze_service.match_frames_to_gaze(\n",
    "    num_frames=frames.shape[0],\n",
    "    gaze_points=gaze_points,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_efficientvit_sam_model(\n",
    "    name=\"efficientvit-sam-xl1\", pretrained=True, weight_url=\"../checkpoints/efficientvit_sam_xl1.pt\"\n",
    ")\n",
    "model = model.cuda().eval()\n",
    "model_predictor = EfficientViTSamPredictor(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_batch(\n",
    "    frames: torch.Tensor,\n",
    "    crop_points: list[tuple[int, int] | None],\n",
    "    crop_size: int,\n",
    "    start_index: int,\n",
    "    end_index: int,\n",
    ") -> tuple[torch.Tensor, list[tuple[int, int]]]:\n",
    "    \"\"\"\n",
    "    Preprocesses a batch of frames by cropping them around the given crop points\n",
    "\n",
    "    Args:\n",
    "        - frames: batch of frames to preprocess. Shape: (B, H, W, C)\n",
    "        - crop_points: list of crop points for each frame. If a frame does not have a crop point, the value is None\n",
    "        - crop_size: size of the crop window\n",
    "\n",
    "    Returns:\n",
    "        - preprocessed_batch: preprocessed_batch batch of frames. Shape: (B, C, H, W)\n",
    "        - new_centers: list of local crop points for each cropped frame.\n",
    "        - frame_indexes: list of original frame indices that were processed\n",
    "    \"\"\"\n",
    "    HALF_CROP = crop_size // 2\n",
    "    W, H = frames.shape[2], frames.shape[1]\n",
    "\n",
    "    # create an empty tensor to store the cropped frames\n",
    "    crop_points = crop_points[start_index:end_index]\n",
    "    frame_count = sum([1 for point in crop_points if point is not None])\n",
    "    preprocessed_batch = torch.zeros((frame_count, frames.shape[3], crop_size, crop_size), device=frames.device)\n",
    "    new_centers = []\n",
    "    valid_frame_idx = 0\n",
    "\n",
    "    for frame_idx in range(start_index, end_index):\n",
    "        crop_point = crop_points[frame_idx]\n",
    "        if crop_point is not None:\n",
    "            # Gaze coordinates\n",
    "            frame = frames[frame_idx].cpu().numpy()\n",
    "            cx = crop_point[0]\n",
    "            cy = crop_point[1]\n",
    "\n",
    "            # Compute bounding-box edges with clamping\n",
    "            left = max(cx - HALF_CROP, 0)\n",
    "            right = min(cx + HALF_CROP, W)\n",
    "            top = max(cy - HALF_CROP, 0)\n",
    "            bottom = min(cy + HALF_CROP, H)\n",
    "\n",
    "            # Crop the region\n",
    "            cropped_frame = frame[top:bottom, left:right]\n",
    "            local_cx = cx - left\n",
    "            local_cy = cy - top\n",
    "\n",
    "            # Apply preprocessing steps\n",
    "            # TODO create const or UI setting for kernel size?\n",
    "            preprocessed_frame = cv2.GaussianBlur(cropped_frame, (7, 7), 0)\n",
    "\n",
    "            # Apply necessary transforms (See efficientvit.models.efficientvit.sam.EfficientViTSam.transform)\n",
    "            tf = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=[123.675 / 255, 116.28 / 255, 103.53 / 255],\n",
    "                    std=[58.395 / 255, 57.12 / 255, 57.375 / 255],\n",
    "                ),\n",
    "                SamPad(size=crop_size),\n",
    "            ])\n",
    "\n",
    "            # Store results\n",
    "            temp_tensor = tf(preprocessed_frame).cuda()\n",
    "            preprocessed_batch[valid_frame_idx] = temp_tensor\n",
    "            torch.cuda.synchronize()\n",
    "            del temp_tensor\n",
    "\n",
    "            # Store the new crop point and frame index\n",
    "            new_centers.append((local_cx, local_cy))\n",
    "\n",
    "            valid_frame_idx += 1\n",
    "            del frame\n",
    "\n",
    "    return preprocessed_batch, new_centers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([43, 3, 1024, 1024])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "BATCH_NUM = 0\n",
    "\n",
    "\n",
    "crop_points = [gaze_points[0].position if len(gaze_points) > 0 else None for gaze_points in frame_gaze_mapping]\n",
    "start_index = BATCH_NUM * BATCH_SIZE\n",
    "end_index = (BATCH_NUM + 1) * BATCH_SIZE\n",
    "\n",
    "crop_points_batch = crop_points[start_index:end_index]\n",
    "crop_points_batch = [[point] for point in crop_points_batch if point is not None]\n",
    "\n",
    "\n",
    "preprocessed_batch, new_centers = preprocess_batch(frames, crop_points, 1024, start_index, end_index)\n",
    "preprocessed_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predictor.set_image_batch(preprocessed_batch)\n",
    "del preprocessed_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 1, 2])\n",
      "torch.Size([43, 1])\n"
     ]
    }
   ],
   "source": [
    "print(torch.tensor(crop_points_batch).shape)\n",
    "print(torch.tensor([[1]] * len(crop_points_batch)).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n",
      "torch.Size([43, 3, 1024, 1024])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(crop_points_batch)):\n",
    "    masks, iou_predictions, low_res_masks = model_predictor.predict_torch(\n",
    "        point_coords=torch.tensor(crop_points_batch).cuda(),\n",
    "        point_labels=torch.tensor([[1]] * len(crop_points_batch)).cuda(),\n",
    "        image_index=i,\n",
    "    )\n",
    "\n",
    "    print(masks.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
