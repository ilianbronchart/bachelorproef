{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from src.api.models.gaze import GazePoint\n",
    "from src.api.services import gaze_service\n",
    "from src.logic.models.efficientvit_sam import EfficientVitSAMCheckpoint, EfficientVitSAMModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = EfficientVitSAMModel(EfficientVitSAMCheckpoint.EFFICIENTVIT_SAM_XL0, Path(\"../checkpoints\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = Path(\"../data/recordings/af47ccce-c344-49d9-9916-5729e2ddc021.mp4\")\n",
    "GAZE_DATA_PATH = Path(\"../data/recordings/af47ccce-c344-49d9-9916-5729e2ddc021.tsv\")\n",
    "\n",
    "resolution = cv2_video_resolution(VIDEO_PATH)\n",
    "frame_count = cv2_video_frame_count(VIDEO_PATH)\n",
    "fps = cv2_video_fps(VIDEO_PATH)\n",
    "gaze_data = gaze_service.parse_gazedata_file(GAZE_DATA_PATH)\n",
    "gaze_points = gaze_service.get_gaze_points(gaze_data, resolution)\n",
    "frame_gaze_mapping = gaze_service.match_frames_to_gaze(num_frames=frame_count, gaze_points=gaze_points, fps=fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay_masks(frame, masks, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Overlay masks on the original frame\n",
    "    Args:\n",
    "        frame: Original frame (ndarray)\n",
    "        masks: Binary masks with shape (3, H, W)\n",
    "        alpha: Transparency of overlay (0-1)\n",
    "        colors: List of RGB colors for each mask\n",
    "    Returns:\n",
    "        frame: Frame with overlaid masks\n",
    "    \"\"\"\n",
    "\n",
    "    colors = [(255, 0, 0), (0, 255, 0), (0, 0, 255)]\n",
    "\n",
    "    # Overlay each mask\n",
    "    for i, mask in enumerate(masks):\n",
    "        # Create colored mask\n",
    "        colored_mask = np.zeros_like(frame)\n",
    "        colored_mask[mask] = colors[i]\n",
    "\n",
    "        # Overlay with alpha blending\n",
    "        frame = cv2.addWeighted(frame, 1, colored_mask, alpha, 0)\n",
    "\n",
    "    return frame.astype(np.uint8)\n",
    "\n",
    "\n",
    "def overlay_gaze_points(frame, gaze_points: list[GazePoint]):\n",
    "    \"\"\"\n",
    "    Overlay gaze points on the original frame\n",
    "    Args:\n",
    "        frame: Original frame (ndarray)\n",
    "        gaze_points: List of gaze points (x, y) in pixel coordinates\n",
    "    Returns:\n",
    "        frame_with_gazepoints: Frame with overlaid gaze points\n",
    "    \"\"\"\n",
    "    for gaze_point in gaze_points:\n",
    "        cv2.circle(frame, gaze_point.position, 15, (255, 0, 0), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = Path(\"./output\")\n",
    "OUTPUT_PATH.mkdir(exist_ok=True)\n",
    "for file in OUTPUT_PATH.iterdir():\n",
    "    file.unlink()\n",
    "\n",
    "frame_sample_points = [gaze_points[0].position if len(gaze_points) > 0 else [] for gaze_points in frame_gaze_mapping]\n",
    "frame_gaze_depths = [gaze_points[0].depth if len(gaze_points) > 0 else [] for gaze_points in frame_gaze_mapping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 100 is out of bounds for dimension 0 with size 50",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m     frames\u001b[38;5;241m.\u001b[39mappend(frame)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(frames) \u001b[38;5;241m==\u001b[39m BATCH_SIZE \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ret:\n\u001b[0;32m---> 20\u001b[0m     frame_masks \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_batch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_sample_points\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframe_gaze_depths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m:\u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresolution\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresolution\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, frame \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(frames):\n\u001b[1;32m     28\u001b[0m         frame \u001b[38;5;241m=\u001b[39m overlay_masks(frame, frame_masks[i])\n",
      "File \u001b[0;32m~/projects/bachelorproef/src/logic/models/efficientvit_sam.py:159\u001b[0m, in \u001b[0;36mEfficientVitSAMModel.predict_batch\u001b[0;34m(self, frames, frame_sample_points, frame_gaze_depths, resolution, mask_threshold)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;66;03m# Post-process the masks\u001b[39;00m\n\u001b[1;32m    158\u001b[0m masks \u001b[38;5;241m=\u001b[39m masks \u001b[38;5;241m>\u001b[39m mask_threshold \u001b[38;5;66;03m# TODO: Tune this threshold\u001b[39;00m\n\u001b[0;32m--> 159\u001b[0m best_mask \u001b[38;5;241m=\u001b[39m \u001b[43mmasks\u001b[49m\u001b[43m[\u001b[49m\u001b[43miou_predictions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    160\u001b[0m merged_mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlogical_or\u001b[38;5;241m.\u001b[39mreduce(masks)\n\u001b[1;32m    161\u001b[0m masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_masks(\n\u001b[1;32m    162\u001b[0m     [best_mask, merged_mask], \n\u001b[1;32m    163\u001b[0m     frame_gaze_depths[frame_idx], \n\u001b[1;32m    164\u001b[0m )\n",
      "\u001b[0;31mIndexError\u001b[0m: index 100 is out of bounds for dimension 0 with size 50"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 50\n",
    "batch_start = 0\n",
    "batch_end = BATCH_SIZE\n",
    "\n",
    "cap = cv2.VideoCapture(VIDEO_PATH)\n",
    "if not cap.isOpened():\n",
    "    raise ValueError(f\"Video file not found: {VIDEO_PATH}\")\n",
    "\n",
    "if len(frame_sample_points) != frame_count:\n",
    "    raise ValueError(\n",
    "        f\"Number of sample point batches ({len(frame_sample_points)}) does not match the number of frames ({frame_count})\"\n",
    "    )\n",
    "\n",
    "frames = []\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        frames.append(frame)\n",
    "\n",
    "    if len(frames) == BATCH_SIZE or not ret:\n",
    "        frame_masks = model.predict_batch(\n",
    "            frames,\n",
    "            frame_sample_points[batch_start:batch_end],\n",
    "            frame_gaze_depths[batch_start:batch_end],\n",
    "            resolution=resolution,\n",
    "        )\n",
    "\n",
    "        for i, frame in enumerate(frames):\n",
    "            frame = overlay_masks(frame, frame_masks[i])\n",
    "            overlay_gaze_points(frame, frame_gaze_mapping[batch_start + i])\n",
    "            cv2.imwrite(str(OUTPUT_PATH / f\"{batch_start + i:04d}.png\"), frame)\n",
    "\n",
    "        del frames\n",
    "        frames = []\n",
    "        batch_start += BATCH_SIZE\n",
    "        batch_end += BATCH_SIZE\n",
    "\n",
    "    if not ret:\n",
    "        # End of video\n",
    "        break\n",
    "\n",
    "cap.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
