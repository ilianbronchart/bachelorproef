{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import FastSAM\n",
    "from src.core.utils import cv2_loadvideo\n",
    "from pathlib import Path\n",
    "from src.core.utils import cv2_video_resolution, cv2_video_fps, cv2_video_frame_count\n",
    "from src.logic.glasses.gaze import parse_gazedata_file, get_gaze_points, match_frames_to_gaze\n",
    "import cv2\n",
    "from ultralytics.engine.results import Results\n",
    "from typing import List\n",
    "import torch\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=FastSAM('../checkpoints/FastSAM-s.engine')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VIDEO_PATH = Path(\"../data/recordings/af47ccce-c344-49d9-9916-5729e2ddc021.mp4\")\n",
    "GAZE_DATA_PATH = Path(\"../data/recordings/af47ccce-c344-49d9-9916-5729e2ddc021.tsv\")\n",
    "\n",
    "resolution = cv2_video_resolution(VIDEO_PATH)\n",
    "frame_count = cv2_video_frame_count(VIDEO_PATH)\n",
    "fps = cv2_video_fps(VIDEO_PATH)\n",
    "gaze_data = parse_gazedata_file(GAZE_DATA_PATH)\n",
    "gaze_points = get_gaze_points(gaze_data, resolution)\n",
    "frame_gaze_mapping = match_frames_to_gaze(frame_count, gaze_points, fps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_masks(masks: List[torch.Tensor], gaze_depth: float) -> List[torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Filter masks based on mask area and gaze depth\n",
    "    Args:\n",
    "        masks: List of binary masks with shape (H, W)\n",
    "        gaze_depth: Depth of the gaze point in millimeters\n",
    "    Returns:\n",
    "        filtered_masks: List of filtered binary masks\n",
    "    \"\"\"\n",
    "    \n",
    "    filtered_masks = []\n",
    "    for mask in masks:\n",
    "        # Calculate the area of the mask\n",
    "        mask_area = torch.sum(mask)\n",
    "        total_area = mask.shape[0] * mask.shape[1]\n",
    "\n",
    "        # maybe filtering by percentage as well is good to account for when gaze depth is not accurate\n",
    "        mask_area_pct = mask_area / total_area\n",
    "        \n",
    "        # Calculate the area of the mask relative to the depth\n",
    "        mask_area_relative = mask_area / (gaze_depth ** 2)\n",
    "\n",
    "        # count number of connected components\n",
    "        num_labels, _ = cv2.connectedComponents(mask.cpu().numpy().astype(np.uint8))\n",
    "        foreground_components = num_labels - 1\n",
    "\n",
    "        # Filter masks based on area\n",
    "        if mask_area_pct < 0.1 and foreground_components < 3:\n",
    "            filtered_masks.append(mask)\n",
    "    \n",
    "    return filtered_masks\n",
    "\n",
    "\n",
    "def overlay_gaze_points(frame, gaze_points: List[tuple[int, int]]):\n",
    "    \"\"\"\n",
    "    Overlay gaze points on the original frame\n",
    "    Args:\n",
    "        frame: Original frame (ndarray)\n",
    "        gaze_points: List of gaze points (x, y) in pixel coordinates\n",
    "    Returns:\n",
    "        frame_with_gazepoints: Frame with overlaid gaze points\n",
    "    \"\"\"\n",
    "    for gaze_point in gaze_points:\n",
    "        cv2.circle(frame, gaze_point, 15, (255, 0, 0), 2)\n",
    "\n",
    "\n",
    "def overlay(\n",
    "    image: np.ndarray, \n",
    "    mask: np.ndarray, \n",
    "    color: tuple[int, int, int] = (255, 0, 0), \n",
    "    alpha: float = 0.5, \n",
    "    resize=None\n",
    "):\n",
    "    \"\"\"Combines image and its segmentation mask into a single image.\n",
    "    https://www.kaggle.com/code/purplejester/showing-samples-with-segmentation-mask-overlay\n",
    "\n",
    "    Params:\n",
    "        image: Training image. np.ndarray,\n",
    "        mask: Segmentation mask. np.ndarray,\n",
    "        color: Color for segmentation mask rendering.  tuple[int, int, int] = (255, 0, 0)\n",
    "        alpha: Segmentation mask's transparency. float = 0.5,\n",
    "        resize: If provided, both image and its mask are resized before blending them together.\n",
    "        tuple[int, int] = (1024, 1024))\n",
    "\n",
    "    Returns:\n",
    "        image_combined: The combined image. np.ndarray\n",
    "\n",
    "    \"\"\"\n",
    "    color = color[::-1]\n",
    "    colored_mask = np.expand_dims(mask, 0).repeat(3, axis=0)\n",
    "    colored_mask = np.moveaxis(colored_mask, 0, -1)\n",
    "    masked = np.ma.MaskedArray(image, mask=colored_mask, fill_value=color)\n",
    "    image_overlay = masked.filled()\n",
    "\n",
    "    if resize is not None:\n",
    "        image = cv2.resize(image.transpose(1, 2, 0), resize)\n",
    "        image_overlay = cv2.resize(image_overlay.transpose(1, 2, 0), resize)\n",
    "\n",
    "    image_combined = cv2.addWeighted(image, 1 - alpha, image_overlay, alpha, 0)\n",
    "\n",
    "    return image_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ../checkpoints/FastSAM-s.engine for TensorRT inference...\n",
      "[01/29/2025-11:25:10] [TRT] [I] Loaded engine size: 27 MiB\n",
      "[01/29/2025-11:25:10] [TRT] [I] [MemUsageChange] TensorRT-managed allocation in IExecutionContext creation: CPU +0, GPU +21, now: CPU 0, GPU 45 (MiB)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 41\u001b[0m\n\u001b[1;32m     38\u001b[0m pad_y \u001b[38;5;241m=\u001b[39m CROP_SIZE \u001b[38;5;241m-\u001b[39m cropped_frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     39\u001b[0m padded_frame \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcopyMakeBorder(cropped_frame, \u001b[38;5;241m0\u001b[39m, pad_y, \u001b[38;5;241m0\u001b[39m, pad_x, cv2\u001b[38;5;241m.\u001b[39mBORDER_CONSTANT, value\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))\n\u001b[0;32m---> 41\u001b[0m result: Results \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadded_frame\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpoints\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcrop_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrop_y\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCROP_SIZE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43miou\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\n\u001b[1;32m     50\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result\u001b[38;5;241m.\u001b[39mmasks \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     overlay_gaze_points(original_frame, [gp\u001b[38;5;241m.\u001b[39mposition \u001b[38;5;28;01mfor\u001b[39;00m gp \u001b[38;5;129;01min\u001b[39;00m frame_gaze_points])\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:180\u001b[0m, in \u001b[0;36mModel.__call__\u001b[0;34m(self, source, stream, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    153\u001b[0m     source: Union[\u001b[38;5;28mstr\u001b[39m, Path, \u001b[38;5;28mint\u001b[39m, Image\u001b[38;5;241m.\u001b[39mImage, \u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m, np\u001b[38;5;241m.\u001b[39mndarray, torch\u001b[38;5;241m.\u001b[39mTensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    154\u001b[0m     stream: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    156\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    157\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;03m    Alias for the predict method, enabling the model instance to be callable for predictions.\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m        ...     print(f\"Detected {len(r)} objects in image\")\u001b[39;00m\n\u001b[1;32m    179\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 180\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/models/fastsam/model.py:50\u001b[0m, in \u001b[0;36mFastSAM.predict\u001b[0;34m(self, source, stream, bboxes, points, labels, texts, **kwargs)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mPerform segmentation prediction on image or video source.\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;124;03m    (list): Model predictions.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     49\u001b[0m prompts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(bboxes\u001b[38;5;241m=\u001b[39mbboxes, points\u001b[38;5;241m=\u001b[39mpoints, labels\u001b[38;5;241m=\u001b[39mlabels, texts\u001b[38;5;241m=\u001b[39mtexts)\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/engine/model.py:558\u001b[0m, in \u001b[0;36mModel.predict\u001b[0;34m(self, source, stream, predictor, **kwargs)\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m prompts \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mset_prompts\u001b[39m\u001b[38;5;124m\"\u001b[39m):  \u001b[38;5;66;03m# for SAM-type models\u001b[39;00m\n\u001b[1;32m    557\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mset_prompts(prompts)\n\u001b[0;32m--> 558\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredictor\u001b[38;5;241m.\u001b[39mpredict_cli(source\u001b[38;5;241m=\u001b[39msource) \u001b[38;5;28;01mif\u001b[39;00m is_cli \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:175\u001b[0m, in \u001b[0;36mBasePredictor.__call__\u001b[0;34m(self, source, model, stream, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream_inference(source, model, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream_inference\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/torch/utils/_contextlib.py:36\u001b[0m, in \u001b[0;36m_wrap_generator.<locals>.generator_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;66;03m# Issuing `None` to a generator fires it up\u001b[39;00m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m---> 36\u001b[0m         response \u001b[38;5;241m=\u001b[39m \u001b[43mgen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     39\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     40\u001b[0m             \u001b[38;5;66;03m# Forward the response to our caller and get its next request\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/engine/predictor.py:268\u001b[0m, in \u001b[0;36mBasePredictor.stream_inference\u001b[0;34m(self, source, model, *args, **kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# Postprocess\u001b[39;00m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m profilers[\u001b[38;5;241m2\u001b[39m]:\n\u001b[0;32m--> 268\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mim0s\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_callbacks(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_predict_postprocess_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m# Visualize, save, write results\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/models/fastsam/predict.py:35\u001b[0m, in \u001b[0;36mFastSAMPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[1;32m     33\u001b[0m labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m     34\u001b[0m texts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprompts\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtexts\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m---> 35\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[1;32m     37\u001b[0m     full_box \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\n\u001b[1;32m     38\u001b[0m         [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, result\u001b[38;5;241m.\u001b[39morig_shape[\u001b[38;5;241m1\u001b[39m], result\u001b[38;5;241m.\u001b[39morig_shape[\u001b[38;5;241m0\u001b[39m]], device\u001b[38;5;241m=\u001b[39mpreds[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32\n\u001b[1;32m     39\u001b[0m     )\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/models/yolo/segment/predict.py:32\u001b[0m, in \u001b[0;36mSegmentationPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# tuple if PyTorch model or array if exported\u001b[39;00m\n\u001b[1;32m     31\u001b[0m protos \u001b[38;5;241m=\u001b[39m preds[\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(preds[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m preds[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpostprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprotos\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/models/yolo/detect/predict.py:40\u001b[0m, in \u001b[0;36mDetectionPredictor.postprocess\u001b[0;34m(self, preds, img, orig_imgs, **kwargs)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(orig_imgs, \u001b[38;5;28mlist\u001b[39m):  \u001b[38;5;66;03m# input images are a torch.Tensor, not a list\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     orig_imgs \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_torch2numpy_batch(orig_imgs)\n\u001b[0;32m---> 40\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_results\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_imgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/models/yolo/segment/predict.py:47\u001b[0m, in \u001b[0;36mSegmentationPredictor.construct_results\u001b[0;34m(self, preds, img, orig_imgs, protos)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstruct_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs, protos):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Constructs a list of result objects from the predictions.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m        (list): List of result objects containing the original images, image paths, class names, bounding boxes, and masks.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m     48\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstruct_result(pred, img, orig_img, img_path, proto)\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pred, orig_img, img_path, proto \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, orig_imgs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m], protos)\n\u001b[1;32m     50\u001b[0m     ]\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/models/yolo/segment/predict.py:48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mconstruct_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, preds, img, orig_imgs, protos):\n\u001b[1;32m     35\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    Constructs a list of result objects from the predictions.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;124;03m        (list): List of result objects containing the original images, image paths, class names, bounding boxes, and masks.\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstruct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morig_img\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m pred, orig_img, img_path, proto \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(preds, orig_imgs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch[\u001b[38;5;241m0\u001b[39m], protos)\n\u001b[1;32m     50\u001b[0m     ]\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/models/yolo/segment/predict.py:72\u001b[0m, in \u001b[0;36mSegmentationPredictor.construct_result\u001b[0;34m(self, pred, img, orig_img, img_path, proto)\u001b[0m\n\u001b[1;32m     70\u001b[0m     masks \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mprocess_mask_native(proto, pred[:, \u001b[38;5;241m6\u001b[39m:], pred[:, :\u001b[38;5;241m4\u001b[39m], orig_img\u001b[38;5;241m.\u001b[39mshape[:\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# HWC\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m     masks \u001b[38;5;241m=\u001b[39m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproto\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m6\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpred\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupsample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# HWC\u001b[39;00m\n\u001b[1;32m     73\u001b[0m     pred[:, :\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mscale_boxes(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m:], pred[:, :\u001b[38;5;241m4\u001b[39m], orig_img\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Results(orig_img, path\u001b[38;5;241m=\u001b[39mimg_path, names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mnames, boxes\u001b[38;5;241m=\u001b[39mpred[:, :\u001b[38;5;241m6\u001b[39m], masks\u001b[38;5;241m=\u001b[39mmasks)\n",
      "File \u001b[0;32m~/projects/bachelorproef/.venv/lib/python3.10/site-packages/ultralytics/utils/ops.py:700\u001b[0m, in \u001b[0;36mprocess_mask\u001b[0;34m(protos, masks_in, bboxes, shape, upsample)\u001b[0m\n\u001b[1;32m    697\u001b[0m width_ratio \u001b[38;5;241m=\u001b[39m mw \u001b[38;5;241m/\u001b[39m iw\n\u001b[1;32m    698\u001b[0m height_ratio \u001b[38;5;241m=\u001b[39m mh \u001b[38;5;241m/\u001b[39m ih\n\u001b[0;32m--> 700\u001b[0m downsampled_bboxes \u001b[38;5;241m=\u001b[39m \u001b[43mbboxes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    701\u001b[0m downsampled_bboxes[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m width_ratio\n\u001b[1;32m    702\u001b[0m downsampled_bboxes[:, \u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m width_ratio\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "CROP_SIZE = 512\n",
    "HALF_CROP = CROP_SIZE // 2\n",
    "\n",
    "video_result = cv2.VideoWriter(\n",
    "    'filename.mp4',  \n",
    "    cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "    fps, \n",
    "    (resolution[1], resolution[0])\n",
    ") \n",
    "\n",
    "for frame_idx, frame in cv2_loadvideo(VIDEO_PATH):\n",
    "    original_frame = frame.copy()\n",
    "    frame_gaze_points = frame_gaze_mapping[frame_idx]\n",
    "\n",
    "    if len(frame_gaze_points) == 0:\n",
    "        video_result.write(original_frame) \n",
    "        continue\n",
    "\n",
    "    # Preprocess\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Crop the frame around the gaze point\n",
    "    gaze_point = frame_gaze_points[0]\n",
    "    cx, cy = gaze_point.position\n",
    "\n",
    "    # Compute bounding-box edges with clamping\n",
    "    left   = max(cx - HALF_CROP, 0)\n",
    "    right  = min(cx + HALF_CROP, resolution[1])\n",
    "    top    = max(cy - HALF_CROP, 0)\n",
    "    bottom = min(cy + HALF_CROP, resolution[0])\n",
    "    \n",
    "    crop_x = min(cx - left, HALF_CROP - 1)\n",
    "    crop_y = min(cy - top, HALF_CROP - 1)\n",
    "    cropped_frame = frame[top:bottom, left:right]\n",
    "\n",
    "    # Pad the bottom right corner to make the frame square\n",
    "    pad_x = CROP_SIZE - cropped_frame.shape[1]\n",
    "    pad_y = CROP_SIZE - cropped_frame.shape[0]\n",
    "    padded_frame = cv2.copyMakeBorder(cropped_frame, 0, pad_y, 0, pad_x, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
    "\n",
    "    result: Results = model(\n",
    "        source=padded_frame,\n",
    "        points=[(crop_x, crop_y)],\n",
    "        labels=[1],\n",
    "        device='cuda',\n",
    "        verbose=False,\n",
    "        imgsz=CROP_SIZE,\n",
    "        conf=0.5, \n",
    "        iou=0.9\n",
    "    )[0]\n",
    "\n",
    "    if result.masks is None:\n",
    "        overlay_gaze_points(original_frame, [gp.position for gp in frame_gaze_points])\n",
    "        video_result.write(original_frame) \n",
    "        continue\n",
    "    \n",
    "    # Post processing\n",
    "    masks = filter_masks(result.masks.data, gaze_point.depth)\n",
    "\n",
    "    # if len(masks) == 0:\n",
    "    #     overlay_gaze_points(original_frame, [gp.position for gp in frame_gaze_points])\n",
    "    #     video_result.write(original_frame) \n",
    "    #     continue\n",
    "\n",
    "    # mask = torch.stack(masks).sum(dim=0) > 0\n",
    "    # result.update(masks=torch.stack(masks))\n",
    "    plotted_result = result.plot(masks=True)\n",
    "    # full_mask = torch.zeros((resolution[0], resolution[1]), device='cuda')\n",
    "    # mask_thresh = mask > torch.tensor([0.5], device='cuda')\n",
    "    original_frame[top:bottom, left:right] = plotted_result[0:cropped_frame.shape[0], 0:cropped_frame.shape[1]]\n",
    "\n",
    "    # Rendering\n",
    "    # original_frame = overlay(original_frame, full_mask.cpu())\n",
    "    overlay_gaze_points(original_frame, [gp.position for gp in frame_gaze_points])\n",
    "\n",
    "    video_result.write(original_frame) \n",
    "\n",
    "video_result.release()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
